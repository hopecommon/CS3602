# 探索日志

本文档记录了迄今为止所有的探索步骤、失败尝试和发现。
旨在作为持久的项目日志，并作为论文中“负面结果/分析”部分的素材。

## 0) 范围和约束
- 硬件: NVIDIA A800 (Ampere).
- 模型: EleutherAI/pythia-2.8b.
- 目标: 在保持 **PPL (困惑度) 稳定**的同时提高 **TPOT (每个Token的时间) / 总时间**。
- StreamingLLM 已经到位；注意力/KV 开销已减少。
- 长文本 (PG19) 是主要评估对象；短文本 (wikitext) 用于健全性检查。

### Auto-cap（固定总预算）评测口径（强制）
为避免“总可见上下文长度变化”带来的混杂与复现争议，我们在所有核心实验中采用 **Auto-cap**：

> 固定一个总 KV 预算（也对应模型的最大上下文长度上限）`CAP_TOTAL`，并将各个“会占用 KV 长度/可见 token 数”的项都计入同一预算：
>
> \[
> S + W + \sigma + O + B \le C_{\text{cap}}
> \]
>
> 其中：`S=n_sink`（sink）、`W=window_size`（recent window）、`\sigma=cache_slack`（Slack）、`O=overlap`（重叠保留）、`B=refresh_budget`（刷新保留）。

**为什么每项都要算进去：**
1) 它们都会增加“attention 实际能看到的 KV 长度”或“KV cache 实际保存的长度”，从而改变显存、attention FLOPs/访存、以及（RoPE 模型中）位置分布；如果不计入 cap，就会出现“表面 window 相同但实际可见 token 更多”的不公平对比。  
2) Pythia-2.8B 的有效上下文边界在 2048 量级；一旦总长度越界，我们已多次观察到 **PPL 非线性恶化**。Auto-cap 用于把所有实验都锁定在同一上下文预算内，避免“越界导致的伪差异/复现失败”。

实现上，Auto-cap 等价于在给定 `CAP_TOTAL` 时自动推导：
`window_size = CAP_TOTAL - n_sink - cache_slack - overlap - refresh_budget`。

## 1) 早期系统级结论
- 在 StreamingLLM 之后，解码主要由 **MLP + LayerNorm/残差 + 框架/启动** 主导。
- 来自注意力侧技巧（推测性解码、Flash Attention 等）的进一步加速与流式处理冲突或不可靠。
- 现在大量时间花在“小算子”（残差/逐元素 + 内核启动）上，而不是注意力上。

## 2) 流式基线和窗口/Sink 扫描
我们进行了窗口/Sink 扫描以确定合理的操作点：

### Wikitext (4k tokens)
- **w2048** 最快，PPL 相对稳定。
- **w1024** 较慢，但 PPL 质量更高。
- 将 Sink 从 4 增加到 16/32 略微改善了 PPL，对 TPOT 影响极小。

### PG19 (20k tokens)
- 所有情况下的加速比都在 **6.2–7.3x** 左右。
- **w2048(sink + window)** 是最佳的 PPL/TPOT 权衡点。

**结果:** 一旦流式处理到位，仅减少窗口并不能改善 TPOT。
这表明修剪/复制开销或 MLP/启动开销占主导地位。

## 3) 量化尝试 (失败与部分成功)

### 3.1 TorchAO & INT8/INT4
最初目标: 通过量化 MLP/Linear 层来改善 TPOT。

#### 遇到的问题
- 环境中缺少 `torchao`。需要确保正确的 Python/venv。
- `int4wo` 需要 `fbgemm-gpu-genai >= 1.2.0`，但即使安装后模块路径 `fbgemm_gpu.experimental.genai` 仍然缺失；跳过了 int4wo。
- TorchAO 中的 INT8 weight-only **v1** 产生了 NaN：
  - 第一个非有限层: `gpt_neox.layers.2`。
  - `logits finite: False`, `fp32 loss finite: False`。

#### 修复
- 切换到 **IntxWeightOnlyConfig(version=2)**。
  - 这修复了 NaN (`logits finite: True`, `fp32 loss finite: True`)。

#### 结果
即使使用仅 MLP 的 INT8 v2：
- **没有 TPOT 增益**；通常比基线更慢。
- 质量稳定，但在 batch=1 解码中出现 **速度回退**。

**结论:** INT8 weight-only **没有** 加速 A800 上由 MLP 主导的解码。
这现在是报告中的一个负面结果。

## 4) Torch.compile / CUDA Graphs 问题
我们尝试了 `torch.compile(mode="reduce-overhead")` 以减少 Python 开销。

#### 错误
重复的 CUDA graphs 错误：
```
RuntimeError: accessing tensor output of CUDAGraphs that has been overwritten
```
这出现在 RoPE 应用路径中，与 CUDAGraphs 覆写有关。

#### 缓解
我们在编译路径中禁用了 cudagraphs 以避免不稳定性。

**结论:** 在此设置下，带有 cudagraphs 的 `torch.compile(reduce-overhead)` 是不稳定的。

## 5) 懒惰 / 周期性修剪 (compress_every)
我们引入了 **懒惰/周期性修剪**: 每 `R` 个 token 修剪一次 KV。

### 观察
- 更大的 R → 更快的 TPOT (修剪开销被摊销)。
- 加速比增加，但随着 R 增大 **PPL 下降**。
  - 例如: R=32 改善了 TPOT，PPL +~6%。
  - R=128 几乎没有额外的速度提升，但 **PPL 爆炸 (+34%)**。
- 明显的收益递减: 速度平台期 ~7.2–7.3x。

**结论:** 修剪开销是真实的；减少它有助于提高速度，
但周期性的“硬驱逐”会导致质量断崖。

## 6) 阶段 A 诊断 (NLL 尖峰)
我们实现了 `experiments/diagnose_prune_nll.py` 来记录：
- 每个 token 的 NLL
- 每个 token 的时间
- KV 长度
- 修剪事件
- 尖峰峰值/面积/宽度指标

### 发现
**Wikitext (w2048, s4, c32):**
- KV 长度漂移很小 (~31 tokens)。
- 存在 NLL 尖峰 (峰值/面积/宽度不可忽略)。

**PG19 (w2048, s4, c32):**
- 尖峰比 wikitext **更高且更宽**。
- KV 漂移仍然很小；主要问题是 **硬驱逐尖峰**。

**结论:** PPL 下降主要是由 **硬驱逐尖峰** 引起的，而不是窗口漂移。

## 6.5) Max\_Drop 的“频率陷阱”与修正方向
我们在 Auto-cap 下观察到：直接启用 `max_drop`（并配合 `cache_slack`）有时会 **拖慢 TPOT**，且对 PPL 改善有限甚至略差。
其根因通常不是 “max\_drop 思路错误”，而是 **实现语义导致 prune 事件更频繁**：

- 当 `cache_slack>0` 且 `max_drop>0` 时，如果一次 prune 后仍保持在接近硬上限（例如保留到 `H=C+\sigma`），那么下一次达到触发阈值所需的新 token 数会变少，等价于 **prune 周期缩短**，从而把 copy/重对齐开销重新放大。

**改进方向（后续计划）：** 将 Max\_Drop 做成“分阶段驱逐但不增加事件频率”的版本：
- 维持 Lazy 的触发节奏（`compress_every=K`），在一次 prune 后进入 cooldown（除非触碰硬上限才 emergency prune）；
- 选择安全的保留长度，确保在 cooldown 内不会提前触到硬上限（避免被迫再次 prune）。

## 6.6) Max\_Drop 的对照验证（Probe）
为避免“凭感觉/凭 PPL 最终值”判断，我们增加了专用 probe 来严格控制变量并输出 debug 信号：

- 脚本：`experiments/probes/verify_max_drop_effect.py`
- 设计：固定 `S`、Auto-cap(`CAP_TOTAL`)、`K`、`sigma`，仅切换 `delta`（`delta=0` vs `delta>0`）。
- 产物：每个 setting 保存 `*_summary.json`（TPOT、PPL proxy、spike 统计、prune 次数/分布）、`*_prune_events.json`（每次 prune 的 dropped/overflow/step）、以及 `*_per_token.csv`（逐 token NLL/时间/kv_len）。

**结论（当前实现与常用参数下）：**
- 在 Auto-cap 严格约束下，`max_drop` 很容易退化为“与 baseline 相同的 drop 行为”，或无法产生显著的 spike/PPL 改善；
- 这与我们在主消融中观察到的现象一致：启用 `max_drop` 往往会拖慢 TPOT，且质量改善不稳定/不明显。

## 6.7) 是否值得做 Soft Ring（ROI 探测）
我们曾设想：实现 soft ring（环形缓冲/减少 copy）可能让小 `K`（更平滑驱逐）也能保持接近大 `K` 的速度，从而提升质量；但这是否值得需要数据支撑。

我们新增了一个“prune 开销剖析”探测脚本，直接把每步时间拆为 forward vs wrapper.update（prune/重对齐开销）：

- 脚本：`experiments/probes/profile_prune_overhead.py`
- 输出：对多个 `K` 的 `total_ms_mean/forward_ms_mean/update_ms_mean`，并区分 prune step vs non-prune step 的 update 开销。

**发现（PG19, Auto-cap=2048, S=32, sigma=16, delta=0, decode_steps=2000）：**
- prune step 的 update 成本大约恒定（~5.6ms/次），但随着 `K` 增大被摊薄；
- `K>=16` 后 TPOT 基本饱和（与 `K=64` 的差距只有 ~1–2%），说明在当前最优点附近 soft ring 的“纯提速”空间很小；
- 因此 soft ring 更可能只在“我们明确需要更小 K 的质量收益”时才有意义。

## 6.8) K=16 vs K=64 的质量对照（spike 指标）
我们用 NLL spike 诊断脚本做了严格对照（同 Auto-cap、同 S/sigma/delta，仅变 `K`）：

- 脚本：`experiments/diagnose_prune_nll.py`（已支持 Auto-cap、`cache_slack/max_drop`、`max_decode_steps`）
- 汇总脚本：`experiments/probes/summarize_k_quality.py`（只读 `*_summary.json`，自动对比 K）

**结论（当前设定下）：**
- `K=16` 相比 `K=64` 的 spike 指标改善非常小（约 1% 量级，甚至 peak/area 的方向并不稳定），不足以支撑投入 soft ring 的工程成本；
- 因此我们暂时停止 soft ring 路线，将重点回到更稳定的策略（Lazy/Softlite）与更明确的质量-速度权衡。

## 6.9) Slack 是否有收益：受控对照验证计划
Slack 在我们的实现里理论上只有在 staged eviction（`delta>0`）时才会影响硬上限 `H=C+\sigma`。为避免混杂，我们新增了一个 slack 受控验证脚本：

- 脚本：`experiments/probes/verify_slack_effect.py`
- 设计：对每个 `sigma`，在 Auto-cap 下自动推导 window，并做 `delta=0`（control） vs `delta>0`（staged）的成对对照，输出 `summary.csv` 与逐点 compare。

该实验用于回答：Slack 在什么参数区间下能够显著降低 spike/PPL，同时不引入明显 TPOT 回退；如果效果微弱或不稳定，则将 Slack/Max\_Drop 调整为非默认组件或在论文中降级为消融/负结果。

### 6.9.1) 实验设计（严格控制变量）
我们使用 `experiments/probes/verify_slack_effect.py` 做受控对照：
- 固定 Auto-cap（`CAP_TOTAL=2048`），并对每个 `sigma` 自动推导 `window_size = CAP_TOTAL - sink - sigma - overlap - refresh_budget`；
- 固定 `sink`、`K`、数据段、decode 长度；
- 对每个 `sigma` 成对运行：
  - **control**：`delta=0`（禁用 Max\_Drop）
  - **staged**：`delta>0`（启用 Max\_Drop，观察 Slack 是否通过硬上限 `H=C+\sigma` 改变行为）
- 输出 `summary.csv` 和每点的 `*_compare.json`（包含 PPL/TPOT/spike/prune\_events 的差值）。

### 6.9.2) 结果摘要（PG19, Auto-cap=2048, S=32）
我们依次尝试了更“强迫 staged 生效”的设置：

1) `K=64, delta=32`：`results/probes/verify_slack/pg19_S32_cap2048_K64_delta32_summary.csv`  
2) `K=64, delta=8`：`results/probes/verify_slack/pg19_S32_cap2048_K64_delta8_summary.csv`  
3) `K=16, delta=8`：`results/probes/verify_slack/pg19_S32_cap2048_K16_delta8_summary.csv`  

**共同现象：**
- 在绝大多数 `sigma` 上，**control 与 staged 的指标几乎完全一致**：
  - `control_prunes == staged_prunes`（prune 事件数一致）
  - `ppl_delta ≈ 0`（或仅 1e-4～1e-3 级别波动）
  - `area_delta/peak_delta ≈ 0`（spike 指标差异极小且方向不稳定）
- 少量出现的非零差异通常是噪声级，且不构成稳定 Pareto 改善（例如 area 稍降但 PPL 反而略差）。

### 6.9.3) 结论与解释
在我们当前的 HF cache-side Start+Recent + RoPE 重对齐实现、以及严格 Auto-cap 评测口径下：
- Slack/Max\_Drop 的 staged eviction **很容易退化为与 control 等价的行为**（从 prune 次数与 spike 指标几乎不变可以直接观察到）；
- 因此 Slack 在当前实现路径中**收益不可重复或噪声级**，并不适合作为默认主结论组件。

### 6.9.4) 后续动作
- 论文写作上：保留 Slack/Max\_Drop 的形式化定义与动机，但将其定位为 **ablation/negative result**（在本实现与评测 regime 下收益不显著，且可能轻微回退）。  
- 工程上：若未来要“让 Slack 真正生效”，需要先让 staged eviction 在行为层面可观测（例如 drop 分布或保留长度在 prune 时确实发生变化），否则继续扫参意义不大。

## 7) 重叠和刷新 (阶段 B1 尝试)
我们添加了：
- `overlap`: 保留额外的最近 token。
- `refresh_budget`: 重新插入选定的旧 token (均匀策略)。

### 关键问题
`refresh` 是通过将驱逐的 token 以新位置重新插入到尾部来实现的。
这在 RoPE 下造成了位置不一致，并导致巨大的 PPL 下降。

### Wikitext
- 使用 `window=2048` 时，任何重叠/刷新都超过了上下文限制 → PPL 崩溃。
- 当总长度被限制在 ≤ 2048 时，重叠略有帮助，但刷新仍然有害。

### 自动上限实验
自动上限强制执行：
```
sink + window + cache_slack + overlap + refresh_budget <= 2048
```
并且用 `CAP_TOTAL` 统一控制所有实验的总预算（避免不同实验点“总可见 token 数”不同导致的混杂）。
这使得重叠实际上是一种重新分配，而不是真正的增加。
结果：
- **PPL 相同** 在不同的重叠值下 (预期之中)。
- TPOT 变化很小，接近测量噪声。

**结论:** 在自动上限下重叠没有效果；刷新 (尾部插入) 是负面的。

### 7.1) Boundary Bridge（失败案例：质量更差）
我们曾尝试一个更“轻量级、常数预算”的思路来缓解 eviction cliff：在每次 prune 的边界附近额外保留一小段“桥接 token”（`bridge_budget`），希望减少 RoPE 位置重对齐与上下文突变带来的 NLL spike，从而在不显著增加 TPOT 的前提下改善 PPL。

**实现直觉：**
- 在 Auto-cap（`CAP_TOTAL=2048`）下，`bridge_budget` 必须占用固定预算，因此本质上是在“最近窗口”中用少量 token 换取“边界连续性”。

**结果（快速验证后立刻止损）：**
- 在 `K=64` 的设定下，PPL 反而明显恶化（例如 PPL 增幅从约 `~2.8%` 上升到 `~6.5%`），因此我们直接暂停并回滚该改动。

**初步分析（为何可能天然负收益）：**
- 在总预算固定的前提下，边界桥接 token 并不是“额外信息”，而是挤占了最近窗口；而对自回归生成而言，最近 token 的边际价值通常更高；
- 若桥接 token 的选择与位置映射不严格对齐模型的 RoPE 语义（或带来更强的“伪近邻”效应），可能会放大分布偏移而不是缓解。

**结论：**
`bridge_budget` 在当前实现与评测 regime 下表现为明确的负结果，不再作为主线组件，仅保留为失败尝试记录。

## 8) 修正性解释
我们得出结论：
- 在 RoPE 下 **刷新 (尾部插入) 在结构上是错误的**。

## 9) 显存（Peak Memory）口径与“1.21×”现象说明

### 9.1 我们的主线评测：为何 Streaming 的 peak memory 约为 baseline 的 `~1.21×`
在我们的 paper 口径（`experiments/eval_utils.py::_compute_streaming_decode_perplexity`）里：
- **Baseline** 是 sliding-window **recompute**：每步 `use_cache=False`，不常驻 KV cache；
- **Streaming(Start+Recent/Lazy)** 是 `use_cache=True`，需要常驻 KV cache，并且在当前 Transformers 版本下默认使用 `DynamicCache` 机制。

因此在相同 cap（例如 2048）下，Streaming 路径相对于 baseline 必然多出一部分 **KV 常驻显存**；同时，`DynamicCache` 的 token-append 可能产生短暂的额外分配峰值，使得最终观测到 Streaming 的 peak memory 比 baseline 高（例如约 `1.21×`）是预期现象，而不是 staging buffers 或 lazy overflow 造成的。

我们用两组探针排除了常见误解：
- staging buffers 开关对 peak 几乎无影响：`results/probes/staging_buffers/wikitext_S32_W2016_K64_wiki_compare.json` 中 `staging_on/off` 的 `peak_memory_mb` 完全一致；
- strict prune（`R=1`）依然存在明显增量：`results/paper_experiments/sweeps/R/pg19_R1.json` 中 baseline `~5722MB` vs streaming `~6941MB`（差值约 `~1.22GB`）。

### 9.2 MIT 版本的两个显存数据来自哪里、为何不宜直接与我们比较
在 `results/mit_bench` 下我们记录了 MIT 仓库脚本的速度/显存基准（用于 sanity/参考，不进入主线结论）：
- `results/mit_bench/pg19_benchmark_recompute.json`：recompute baseline（`mode=recompute`，逐步 `use_cache=False`，不保存 KV）peak `5730MB`；
- `results/mit_bench/pg19_benchmark_streaming.json`：streaming（`mode=streaming`，逐步 `use_cache=True`，保存 KV，并执行 Start+Recent 裁剪）peak `7890MB`。

把它们按同一口径做差/做比值，得到：
- 绝对增量：`7890 - 5730 ≈ 2160MB`
- 相对比例：`7890 / 5730 ≈ 1.38×`

这两个数字“看起来很大”的关键是：`benchmark_streaming.py` 的 `recompute` 基线本身已经包含 **模型权重常驻显存**（Pythia-2.8B fp16 量级约 5–6GB），因此 `5730MB` 更接近“纯模型 + 运行时少量 buffer”的峰值，而不是“几乎为 0 的 baseline”。

而 `streaming` 模式在此基础上还会额外引入两类开销：
1) **KV 常驻**：Start+Recent 需要在每层保留 K/V（例如 cap=2048 时约为数百 MB 级别），这会让显存必然高于 recompute；
2) **Cache API 兼容转换带来的峰值放大**：在我们当前环境（Transformers 4.57 + HF Cache API）下，MIT 原始的 legacy cache selector（Start+Recent）与 HF 的 `DynamicCache` 不匹配，因此 `benchmark_streaming.py` 需要在每个阶段/每步将 `past_key_values` 在 `DynamicCache` ↔ legacy 之间来回转换。该过程会触发额外的 cache 重建/拼接（例如 `torch.cat` 追加），导致短时间内“新旧 cache 共存”，从而把 peak memory 拉高到 `~7890MB`。

这些数字来自 `mit-streaming-llm/examples/benchmark_streaming.py` 的 `torch.cuda.max_memory_allocated` 统计，并且在我们当前环境（Transformers 4.57 + HF Cache API）下，MIT 脚本为了兼容 Start+Recent 的 legacy cache selector，会在 `DynamicCache` 与 legacy cache 之间做转换，从而引入额外的 cache 重建/拼接开销，可能放大 peak memory。

此外，MIT 的另一个脚本 `mit-streaming-llm/examples/eval_long_ppl.py` 默认通过 `device_map="auto"` 加载模型（见 `mit-streaming-llm/streaming_llm/utils.py`），如果只读取单卡的 `max_memory_allocated()`，会出现“显存异常偏小”的误读；因此该脚本的显存字段在跨环境对比上更不可靠。

**结论：**当前 Transformer/Cache API 的差异导致 MIT 脚本的显存统计与我们的 paper 口径很难做到严格可比；我们把这些作为探索日志记录与环境差异说明，而不把它作为主线声称的证据链。
- **自动上限 + 重叠** 没有产生有意义的变化，因为总长度是固定的。

因此，阶段 B 应专注于 **平滑驱逐** 而不增加长度。

## 9) 阶段 B 转变: 从刷新到平滑驱逐
鉴于周期性驱逐带来的尖峰：
- 推荐的路径是 **消除硬驱逐断崖** 同时保持低修剪开销。
- 策略: **具有低复制开销的平滑 / 每步驱逐**。

候选方法：
1) 软环形缓冲区 / 仅包装重打包 (Soft ring buffer / wrap-only repack)
2) 静态缓存 (HF StaticCache)
3) 仅在开销无法降低时使用小 R 的懒惰修剪

## 10) 静态缓存尝试
我们添加了支持：
```
--cache-implementation static
```
在 `eval_streaming_llm.py` / `eval_utils.py` 中。

目标: 减少动态重新分配并稳定每步解码。

结果:
- **与 streaming 裁剪不兼容**。StaticCache 假设 KV 长度固定，
  而我们的 streaming wrapper 会裁剪/重写 KV，触发 CUDA
  `index_copy_(): index out of bounds` 断言。
- 已加入显式 guard，避免在 streaming pruning 下启用 static cache，
  并将此记录为负面结果。

## 11) 基线管理
因为基础运行很昂贵，我们现在：
- 计算 **单个固定基线** (wikitext + pg19)，为 1–3 次运行的平均值。
- 存储在 `results/baselines/*_baseline_avg.json`。
- 所有后续实验自动加载此基线 (无需重新运行)。

这保持了比较的稳定性，并避免了重复的基线成本。

## 12) 阶段 B2 矩阵 (当前状态)
我们为 PG19 准备了一个最小矩阵：
- 严格动态 (compress=1)
- 懒惰 c32 / c16 (动态)
- 严格静态
- 懒惰 c32 / c16 (静态)

注意: 严格动态预计会很慢 (每步修剪开销)。

待运行：
```
  python experiments/run_phase_b2_matrix.py \
    --dataset-name pg19 \
    --dataset-config pg19 \
    --max-eval-tokens 20000
```


## 13) Lazy prune: sink=32/64 (pg19, total=2048)

背景: 在 total=2048 前提下提高 sink 以稳定 PPL，并对比速度损失。

### sink=32 (window=2016)
- c16: speedup 6.91x, ΔPPL +2.29%
- c32: speedup 6.93x, ΔPPL +2.43%
- c64: speedup 7.03x, ΔPPL +2.82%

### sink=64 (window=1984)
- c16: speedup 6.86x, ΔPPL +2.33%
- c32: speedup 6.93x, ΔPPL +2.48%
- c64: speedup 6.74x, ΔPPL +2.75%

观察:
- sink 提升明显压低 PPL（均 <3%）。
- 速度略降；sink=32 + c64 是目前兼顾速度与质量的最优点（≈7.03x / 2.82%）。

## 14) Softlite sweep: cache_slack + max_drop (pg19, sink=32, total=2048)

设置: compress_every ∈ {32, 64}, cache_slack ∈ {0, 16, 32}, max_drop ∈ {16, 32}。

### 单因素: cache_slack (固定 max_drop=32)
- c32: slack=0 → 6.94x / +2.43%; slack=16 → 7.07x / +2.19%; slack=32 → 7.00x / +2.18%
- c64: slack=0 → 7.11x / +2.82%; slack=16 → 6.99x / +2.55%; slack=32 → 6.98x / +2.39%
结论: **增加 slack 一般会降低 PPL**，但对速度影响不稳定；在 c32 下 slack=16 同时改善速度和 PPL。

### 单因素: max_drop (固定 slack)
- slack=0: c32 的 max_drop 16 vs 32 基本一致 (≈6.91–6.94x / +2.43%)。
- slack=16: c32 的 max_drop=32 明显更快 (7.07x) 且 PPL 更低 (+2.19%)。
- slack=32: c32 的 max_drop=32 速度略高 (7.00x) 且 PPL 略高 (+2.18%)。
结论: **max_drop 只有在 slack>0 时才显著起作用**；否则等价于常规 lazy prune。

### 交互项: slack × max_drop
- slack>0 允许“分段小步”裁剪，max_drop 决定单次落幅。
- best 交互点出现在 **slack=16, max_drop=32, c32**：既提高速度又降低 PPL。
结论: **softlite 有效性来自 slack 与 max_drop 的耦合**，需要同时开启才能看到净收益。

## 15) 负面结果总结 (明确)
1) **INT8/INT4 量化**: 不稳定或更慢；在 batch=1 解码中无 TPOT 增益。
2) **Torch.compile + cudagraphs**: 不稳定；CUDAGraphs 覆写错误。
3) **通过尾部重新插入进行刷新**: PPL 崩溃 (位置不一致)。
4) **仅减少窗口**: 通常更慢；不能解决 MLP/启动瓶颈。
5) **自动上限重叠**: 对 PPL 无影响；微小的 TPOT 差异可能是噪声。

## 16) 关键正面结果
1) 懒惰修剪降低了 TPOT 并提高了加速比。
2) 阶段 A 诊断清楚地显示了尖峰机制。
3) 重叠 **仅** 当总长度保持在 2048 以内且重叠实际上增加了有效上下文 (不在自动上限下) 时才能稳定 PPL。

## 17) 算子融合尝试 (torch.compile / 外部库)
目标: 尝试现成 fused 路径以降低 MLP/LN/launch 开销。

环境探测:
- xformers: 未安装
- flash_attn: 未安装
- triton: 可用 (依赖 torch)

尝试:
- `torch.compile(mode="reduce-overhead")` + streaming decode
- 即使关闭 cudagraphs 选项, 仍触发 CUDAGraph 覆写错误:
  `RuntimeError: accessing tensor output of CUDAGraphs that has been overwritten`
  (RoPE 路径中 `apply_rotary_pos_emb` / `torch.cat` 触发)

结论:
- 当前环境下外部 fused 库不可用。
- `torch.compile(reduce-overhead)` 在 streaming decode 下不稳定, 作为负结果记录。

补充 profiling 证据:
- 通过 `experiments/profile_kernels.py` 对 streaming vs baseline 进行 kernel 统计。
- 两者 kernel 数量几乎一致, 说明 streaming 并未改变 LN/残差/MLP 的 kernel 密度。
- CPU 侧高频 op 主要是 `addmm / add / layer_norm / mul`。
- 由于 profiler 未显示 CUDA self time, 仍可作为“框架/小算子密集”的证据。

## 18) 关于 “MIT 官方 StreamingLLM” 与我们的实现差异（重要澄清）

我们在仓库中同时包含：
- `mit-streaming-llm/`：MIT 官方参考实现（包含对 GPT-NeoX/LLaMA 等的 **pos\_shift attention patch**）。
- `streaming_llm/`：我们在 HuggingFace 新 Cache API 上的实现（通过 **cache-side RoPE re-alignment** 实现位置一致性）。

关键差异：
1) MIT 官方实现并非“没有压缩/不裁剪”，而是 **每步都做 Start+Recent eviction**（相当于我们 `compress_every=1` 的 strict 模式），但它通过 patch attention（pos\_shift）把位置对齐融入 attention forward。
2) 我们的实现为保持模块化/可插拔，避免侵入式修改 attention kernel，因此采用在 KV cache 管理器中显式执行 RoPE shift/rerotate（`streaming_llm/rope_utils.py`），在 strict (`compress_every=1`) 下会放大 per-step copy/重排成本，因此需要 Lazy/Slack/Max\_Drop 来摊销/分段裁剪。

结论（写论文时的表述建议）：
- 我们的算法层面仍属于 StreamingLLM（Start+Recent + attention sinks）框架；但 **实现路径与 MIT repo 不同**（pos\_shift vs cache-side re-alignment），因此 “MIT 数字”不一定完全复现。
- 为避免“实现分支混杂”，我们在论文实验中加入了 `Ours-framework-only` 对照：保持 Start+Recent 语义但走我们的 wrapper 代码路径，用于证明主结果差异来自策略而非实现路径。

## 19) 当前共识方向
- **不要** 以当前形式追求刷新。
- **要** 追求没有硬断崖的平滑驱逐 (软环形或静态缓存)。
- **要** 使用固定基线以避免重新运行。

---

## 附录: 关键脚本和输出
- `experiments/diagnose_prune_nll.py`: 每个 token 的 NLL + 尖峰指标
- `experiments/run_phase_b1_sweep.py`: 重叠/刷新矩阵 (阶段 B1)
- `experiments/run_phase_b2_matrix.py`: 严格 vs 懒惰 vs 静态 (阶段 B2)
- `experiments/run_fixed_baseline.py`: 创建固定基线平均值
- 基线:
  - `results/baselines/wikitext_baseline_avg.json`
  - `results/baselines/pg19_baseline_avg.json`
