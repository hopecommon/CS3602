# 探索日志

本文档记录了迄今为止所有的探索步骤、失败尝试和发现。
旨在作为持久的项目日志，并作为论文中“负面结果/分析”部分的素材。

## 0) 范围和约束
- 硬件: NVIDIA A800 (Ampere).
- 模型: EleutherAI/pythia-2.8b.
- 目标: 在保持 **PPL (困惑度) 稳定**的同时提高 **TPOT (每个Token的时间) / 总时间**。
- StreamingLLM 已经到位；注意力/KV 开销已减少。
- 长文本 (PG19) 是主要评估对象；短文本 (wikitext) 用于健全性检查。

## 1) 早期系统级结论
- 在 StreamingLLM 之后，解码主要由 **MLP + LayerNorm/残差 + 框架/启动** 主导。
- 来自注意力侧技巧（推测性解码、Flash Attention 等）的进一步加速与流式处理冲突或不可靠。
- 现在大量时间花在“小算子”（残差/逐元素 + 内核启动）上，而不是注意力上。

## 2) 流式基线和窗口/Sink 扫描
我们进行了窗口/Sink 扫描以确定合理的操作点：

### Wikitext (4k tokens)
- **w2048** 最快，PPL 相对稳定。
- **w1024** 较慢，但 PPL 质量更高。
- 将 Sink 从 4 增加到 16/32 略微改善了 PPL，对 TPOT 影响极小。

### PG19 (20k tokens)
- 所有情况下的加速比都在 **6.2–7.3x** 左右。
- **w2048(sink + window)** 是最佳的 PPL/TPOT 权衡点。

**结果:** 一旦流式处理到位，仅减少窗口并不能改善 TPOT。
这表明修剪/复制开销或 MLP/启动开销占主导地位。

## 3) 量化尝试 (失败与部分成功)

### 3.1 TorchAO & INT8/INT4
最初目标: 通过量化 MLP/Linear 层来改善 TPOT。

#### 遇到的问题
- 环境中缺少 `torchao`。需要确保正确的 Python/venv。
- `int4wo` 需要 `fbgemm-gpu-genai >= 1.2.0`，但即使安装后模块路径 `fbgemm_gpu.experimental.genai` 仍然缺失；跳过了 int4wo。
- TorchAO 中的 INT8 weight-only **v1** 产生了 NaN：
  - 第一个非有限层: `gpt_neox.layers.2`。
  - `logits finite: False`, `fp32 loss finite: False`。

#### 修复
- 切换到 **IntxWeightOnlyConfig(version=2)**。
  - 这修复了 NaN (`logits finite: True`, `fp32 loss finite: True`)。

#### 结果
即使使用仅 MLP 的 INT8 v2：
- **没有 TPOT 增益**；通常比基线更慢。
- 质量稳定，但在 batch=1 解码中出现 **速度回退**。

**结论:** INT8 weight-only **没有** 加速 A800 上由 MLP 主导的解码。
这现在是报告中的一个负面结果。

## 4) Torch.compile / CUDA Graphs 问题
我们尝试了 `torch.compile(mode="reduce-overhead")` 以减少 Python 开销。

#### 错误
重复的 CUDA graphs 错误：
```
RuntimeError: accessing tensor output of CUDAGraphs that has been overwritten
```
这出现在 RoPE 应用路径中，与 CUDAGraphs 覆写有关。

#### 缓解
我们在编译路径中禁用了 cudagraphs 以避免不稳定性。

**结论:** 在此设置下，带有 cudagraphs 的 `torch.compile(reduce-overhead)` 是不稳定的。

## 5) 懒惰 / 周期性修剪 (compress_every)
我们引入了 **懒惰/周期性修剪**: 每 `R` 个 token 修剪一次 KV。

### 观察
- 更大的 R → 更快的 TPOT (修剪开销被摊销)。
- 加速比增加，但随着 R 增大 **PPL 下降**。
  - 例如: R=32 改善了 TPOT，PPL +~6%。
  - R=128 几乎没有额外的速度提升，但 **PPL 爆炸 (+34%)**。
- 明显的收益递减: 速度平台期 ~7.2–7.3x。

**结论:** 修剪开销是真实的；减少它有助于提高速度，
但周期性的“硬驱逐”会导致质量断崖。

## 6) 阶段 A 诊断 (NLL 尖峰)
我们实现了 `experiments/diagnose_prune_nll.py` 来记录：
- 每个 token 的 NLL
- 每个 token 的时间
- KV 长度
- 修剪事件
- 尖峰峰值/面积/宽度指标

### 发现
**Wikitext (w2048, s4, c32):**
- KV 长度漂移很小 (~31 tokens)。
- 存在 NLL 尖峰 (峰值/面积/宽度不可忽略)。

**PG19 (w2048, s4, c32):**
- 尖峰比 wikitext **更高且更宽**。
- KV 漂移仍然很小；主要问题是 **硬驱逐尖峰**。

**结论:** PPL 下降主要是由 **硬驱逐尖峰** 引起的，而不是窗口漂移。

## 7) 重叠和刷新 (阶段 B1 尝试)
我们添加了：
- `overlap`: 保留额外的最近 token。
- `refresh_budget`: 重新插入选定的旧 token (均匀策略)。

### 关键问题
`refresh` 是通过将驱逐的 token 以新位置重新插入到尾部来实现的。
这在 RoPE 下造成了位置不一致，并导致巨大的 PPL 下降。

### Wikitext
- 使用 `window=2048` 时，任何重叠/刷新都超过了上下文限制 → PPL 崩溃。
- 当总长度被限制在 ≤ 2048 时，重叠略有帮助，但刷新仍然有害。

### 自动上限实验
自动上限强制执行：
```
window + sink + overlap + refresh <= 2048
```
这使得重叠实际上是一种重新分配，而不是真正的增加。
结果：
- **PPL 相同** 在不同的重叠值下 (预期之中)。
- TPOT 变化很小，接近测量噪声。

**结论:** 在自动上限下重叠没有效果；刷新 (尾部插入) 是负面的。

## 8) 修正性解释
我们得出结论：
- 在 RoPE 下 **刷新 (尾部插入) 在结构上是错误的**。
- **自动上限 + 重叠** 没有产生有意义的变化，因为总长度是固定的。

因此，阶段 B 应专注于 **平滑驱逐** 而不增加长度。

## 9) 阶段 B 转变: 从刷新到平滑驱逐
鉴于周期性驱逐带来的尖峰：
- 推荐的路径是 **消除硬驱逐断崖** 同时保持低修剪开销。
- 策略: **具有低复制开销的平滑 / 每步驱逐**。

候选方法：
1) 软环形缓冲区 / 仅包装重打包 (Soft ring buffer / wrap-only repack)
2) 静态缓存 (HF StaticCache)
3) 仅在开销无法降低时使用小 R 的懒惰修剪

## 10) 静态缓存尝试
我们添加了支持：
```
--cache-implementation static
```
在 `eval_streaming_llm.py` / `eval_utils.py` 中。

目标: 减少动态重新分配并稳定每步解码。

结果: 等待全面评估；矩阵脚本已准备就绪。

## 11) 基线管理
因为基础运行很昂贵，我们现在：
- 计算 **单个固定基线** (wikitext + pg19)，为 1–3 次运行的平均值。
- 存储在 `results/baselines/*_baseline_avg.json`。
- 所有后续实验自动加载此基线 (无需重新运行)。

这保持了比较的稳定性，并避免了重复的基线成本。

## 12) 阶段 B2 矩阵 (当前状态)
我们为 PG19 准备了一个最小矩阵：
- 严格动态 (compress=1)
- 懒惰 c32 / c16 (动态)
- 严格静态
- 懒惰 c32 / c16 (静态)

注意: 严格动态预计会很慢 (每步修剪开销)。

待运行：
```
  python experiments/run_phase_b2_matrix.py \
    --dataset-name pg19 \
    --dataset-config pg19 \
    --max-eval-tokens 20000
```


## 13) 负面结果总结 (明确)
1) **INT8/INT4 量化**: 不稳定或更慢；在 batch=1 解码中无 TPOT 增益。
2) **Torch.compile + cudagraphs**: 不稳定；CUDAGraphs 覆写错误。
3) **通过尾部重新插入进行刷新**: PPL 崩溃 (位置不一致)。
4) **仅减少窗口**: 通常更慢；不能解决 MLP/启动瓶颈。
5) **自动上限重叠**: 对 PPL 无影响；微小的 TPOT 差异可能是噪声。

## 14) 关键正面结果
1) 懒惰修剪降低了 TPOT 并提高了加速比。
2) 阶段 A 诊断清楚地显示了尖峰机制。
3) 重叠 **仅** 当总长度保持在 2048 以内且重叠实际上增加了有效上下文 (不在自动上限下) 时才能稳定 PPL。

## 15) 当前共识方向
- **不要** 以当前形式追求刷新。
- **要** 追求没有硬断崖的平滑驱逐 (软环形或静态缓存)。
- **要** 使用固定基线以避免重新运行。

---

## 附录: 关键脚本和输出
- `experiments/diagnose_prune_nll.py`: 每个 token 的 NLL + 尖峰指标
- `experiments/run_phase_b1_sweep.py`: 重叠/刷新矩阵 (阶段 B1)
- `experiments/run_phase_b2_matrix.py`: 严格 vs 懒惰 vs 静态 (阶段 B2)
- `experiments/run_fixed_baseline.py`: 创建固定基线平均值
- 基线:
  - `results/baselines/wikitext_baseline_avg.json`
  - `results/baselines/pg19_baseline_avg.json`
