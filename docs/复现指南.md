# 复现指南

> **更新日期**: 2025-12-29  
> **基于**: [探索日志.md](./探索日志.md) 的最新实验结果  
> **硬件**: NVIDIA A800 (80GB)  
> **模型**: EleutherAI/pythia-2.8b

本指南提供可复现的命令行示例,用于重现论文中的核心实验结果。所有配置已经过验证,确保速度与质量的最佳平衡。

---

## 0) 固定基线（只跑一次,之后复用）

我们统一用 PyTorch 原生推理 + sliding window 作为基线。
只需要跑一次（wikitext/pg19 各 1 个）,以后不再重复跑。

```bash
python experiments/run_fixed_baseline.py \
  --n-sink 4 \
  --window-size 2044 \
  --runs 1
```

**生成文件**：
- `results/baselines/wikitext_baseline_avg.json`
- `results/baselines/pg19_baseline_avg.json`

之后所有实验会自动读取这个 baseline,避免重复计算。

---

## 1) MIT StreamingLLM（参考实现）

使用 `--streaming-mode mit` 复现 MIT 官方实现：

```bash
# PG19 (20k tokens)
python experiments/eval_streaming_llm.py \
  --dataset-name pg19 \
  --dataset-config pg19 \
  --max-eval-tokens 20000 \
  --n-sink 4 \
  --window-size 2044 \
  --compress-every 1 \
  --streaming-mode mit \
  --mode streaming

# WikiText (4k tokens)
python experiments/eval_streaming_llm.py \
  --dataset-name wikitext \
  --dataset-config wikitext-103-v1 \
  --max-eval-tokens 4096 \
  --n-sink 4 \
  --window-size 2044 \
  --compress-every 1 \
  --streaming-mode mit \
  --mode streaming
```

**预期结果**: 
- PG19: 加速比 ~6.2-7.3×, PPL 增幅 ~1-3%
- WikiText: 加速比 ~3-5×, PPL 增幅 ~2-4%

---

## 2) Our StreamingLLM（标准配置）

这是我们自己的 Streaming 实现（非优化版,compress_every=1）：

```bash
# PG19 (20k tokens)
python experiments/eval_streaming_llm.py \
  --dataset-name pg19 \
  --dataset-config pg19 \
  --max-eval-tokens 20000 \
  --n-sink 4 \
  --window-size 2044 \
  --compress-every 1 \
  --streaming-mode ours \
  --mode streaming

# WikiText (4k tokens)
python experiments/eval_streaming_llm.py \
  --dataset-name wikitext \
  --dataset-config wikitext-103-v1 \
  --max-eval-tokens 4096 \
  --n-sink 4 \
  --window-size 2044 \
  --compress-every 1 \
  --streaming-mode ours \
  --mode streaming
```

**预期结果**: 
- 与 MIT 实现性能相当
- 验证我们的实现正确性

---

## 3) Our Best (最优配置) ⭐

基于探索日志第13和14节的实验结果,**当前最优配置**为：

**配置选择理由**（详见 [探索日志.md §13-14](./探索日志.md#13-lazy-prune-sink3264-pg19-total2048)）：
- **sink=32**: 显著降低 PPL（相比 sink=4/16）,同时速度损失可接受
- **compress_every=64**: 在 sink=32 配置下达到最高加速比（7.03×）
- **cache_slack=16 + max_drop=32**: Softlite 最优交互点,同时提升速度和质量
- **Auto-cap(total)=2048**: 固定总 KV 预算，避免对比混杂与越界导致的 PPL 崩溃

> Auto-cap 口径：`sink + window + cache_slack (+ overlap) (+ refresh_budget) <= 2048`  
> 因此当启用 `cache_slack/overlap/refresh` 时，应同步下调 `window_size`；推荐直接用 `run_paper_experiments.sh`（自动推导 window）。

### 3.1) 最优配置 (推荐用于论文主结果)

```bash
# PG19 (20k tokens) - Softlite最优配置
python experiments/eval_streaming_llm.py \
  --dataset-name pg19 \
  --dataset-config pg19 \
  --max-eval-tokens 20000 \
  --n-sink 32 \
  --window-size 2000 \
  --compress-every 64 \
  --cache-slack 16 \
  --max-drop 32 \
  --streaming-mode ours \
  --mode streaming

# WikiText (4k tokens) - 同等配置
python experiments/eval_streaming_llm.py \
  --dataset-name wikitext \
  --dataset-config wikitext-103-v1 \
  --max-eval-tokens 4096 \
  --n-sink 32 \
  --window-size 2000 \
  --compress-every 64 \
  --cache-slack 16 \
  --max-drop 32 \
  --streaming-mode ours \
  --mode streaming
```

**预期结果** (PG19):
- **加速比**: ~7.03-7.11×（相比 baseline）
- **PPL 增幅**: ~+2.19-2.55%（可接受范围内）
- **关键创新**: Lazy Prune (compress_every) + Softlite (slack + max_drop) 组合

### 3.2) 备选配置 (质量优先)

如果追求更低的 PPL 增幅（~2.2%）,可使用 compress_every=32：

```bash
python experiments/eval_streaming_llm.py \
  --dataset-name pg19 \
  --dataset-config pg19 \
  --max-eval-tokens 20000 \
  --n-sink 32 \
  --window-size 2016 \
  --compress-every 32 \
  --cache-slack 16 \
  --max-drop 32 \
  --streaming-mode ours \
  --mode streaming
```

**预期结果** (PG19):
- **加速比**: ~6.94-7.07×
- **PPL 增幅**: ~+2.18-2.19%（最优质量点）

---

## 4) 消融实验（Ablation Studies）

### 4.1) Lazy Prune 消融

验证 `compress_every` 参数的影响：

```bash
# 测试不同的 compress_every 值
for c in 1 16 32 64 128; do
  python experiments/eval_streaming_llm.py \
    --dataset-name pg19 \
    --dataset-config pg19 \
    --max-eval-tokens 20000 \
    --n-sink 4 \
    --window-size 2044 \
    --compress-every $c \
    --streaming-mode ours \
    --mode streaming \
    --output-file results/ablation/pg19_c${c}.json
done
```

**观察重点**:
- compress_every ↑ → TPOT ↓（修剪开销摊销）
- compress_every 过大（≥128）→ PPL ↑↑（断崖式质量下降）

### 4.2) Sink Size 消融

验证 attention sink 数量的影响：

```bash
# 测试不同的 sink 值 (total=2048固定)
for s in 4 16 32 64; do
  w=$((2048 - s))  # 自动计算 window size
  python experiments/eval_streaming_llm.py \
    --dataset-name pg19 \
    --dataset-config pg19 \
    --max-eval-tokens 20000 \
    --n-sink $s \
    --window-size $w \
    --compress-every 32 \
    --streaming-mode ours \
    --mode streaming \
    --output-file results/ablation/pg19_sink${s}.json
done
```

**关键发现** (探索日志 §13):
- sink=32/64: PPL 增幅显著降低至 <3%
- sink=4: PPL 增幅较高（~5-6%）
- 速度损失可忽略（<5%）

### 4.3) Softlite 组件消融

验证 cache_slack 和 max_drop 的独立/组合效果：

```bash
# Baseline (无 Softlite)
python experiments/eval_streaming_llm.py \
  --dataset-name pg19 --dataset-config pg19 \
  --max-eval-tokens 20000 \
  --n-sink 32 --window-size 2016 \
  --compress-every 32 \
  --streaming-mode ours --mode streaming \
  --output-file results/ablation/pg19_no_softlite.json

# 仅 cache_slack
python experiments/eval_streaming_llm.py \
  --dataset-name pg19 --dataset-config pg19 \
  --max-eval-tokens 20000 \
  --n-sink 32 --window-size 2016 \
  --compress-every 32 --cache-slack 16 \
  --streaming-mode ours --mode streaming \
  --output-file results/ablation/pg19_slack_only.json

# 仅 max_drop（效果有限,需配合 slack）
python experiments/eval_streaming_llm.py \
  --dataset-name pg19 --dataset-config pg19 \
  --max-eval-tokens 20000 \
  --n-sink 32 --window-size 2016 \
  --compress-every 32 --max-drop 32 \
  --streaming-mode ours --mode streaming \
  --output-file results/ablation/pg19_maxdrop_only.json

# Softlite 完整版 (slack + max_drop)
python experiments/eval_streaming_llm.py \
  --dataset-name pg19 --dataset-config pg19 \
  --max-eval-tokens 20000 \
  --n-sink 32 --window-size 2016 \
  --compress-every 32 --cache-slack 16 --max-drop 32 \
  --streaming-mode ours --mode streaming \
  --output-file results/ablation/pg19_softlite_full.json
```

**关键发现** (探索日志 §14):
- **slack 单独**: 一般能降低 PPL,但速度不稳定
- **max_drop 单独**: 在 slack=0 时无显著作用
- **slack + max_drop**: 交互效果显著,同时提升速度和质量

---

## 5) 诊断与分析脚本

### 5.1) Token-level NLL 分析

查看修剪边界处的 NLL 尖峰：

```bash
python experiments/diagnose_prune_nll.py \
  --dataset-name pg19 \
  --dataset-config pg19 \
  --max-eval-tokens 20000 \
  --n-sink 4 \
  --window-size 2044 \
  --compress-every 32 \
  --output-dir results/diagnose_prune
```

**输出**:
- 每个 token 的 NLL 时间序列（CSV）
- 修剪边界标注
- 尖峰统计指标（峰值/面积/宽度）

### 5.2) Kernel Profiling

分析算子密度和开销分布：

```bash
python experiments/profile_kernels.py \
  --dataset-name pg19 \
  --max-eval-tokens 5000 \
  --n-sink 4 \
  --window-size 2044 \
  --output-file results/profile_kernels/pg19_profile.json
```

---

## 6) 论文中推荐的固定表述

建议在论文的实验部分使用以下标准化描述：

### 方法对比表格

| Method | Configuration | Description |
|--------|--------------|-------------|
| **Baseline** | PyTorch native + sliding window (window=2044, sink=4) | Full recomputation baseline |
| **MIT StreamingLLM** | `--streaming-mode mit --compress-every 1` | Official MIT implementation |
| **Ours (Standard)** | `--streaming-mode ours --compress-every 1` | Our implementation, vanilla StreamingLLM |
| **Ours (Lazy Prune)** | `--compress-every 32, sink=4` | Lazy pruning optimization |
| **Ours (Best)** | `--compress-every 64, sink=32, slack=16, max_drop=32` | **Lazy Prune + Softlite (Ours)** |

### 创新点正式名称

论文中建议使用以下术语：

1. **Lazy Pruning** (compress_every): 延迟修剪策略,每 N 步执行一次 KV cache 压缩
2. **Softlite Eviction** (cache_slack + max_drop): 平滑驱逐策略,结合缓冲空间和单步驱逐上限
3. **Enhanced Attention Sinks** (sink=32): 增强的 attention sink 配置

**数学形式化** (供 Method 章节使用):

```
Lazy Pruning:
  prune_condition = (t mod R == 0) and (|KV| > S + W)
  where R = compress_every, S = n_sink, W = window_size

Softlite Eviction:
  cache_capacity = S + W + σ  (σ = cache_slack)
  evict_amount = min(|KV| - (S + W), δ)  (δ = max_drop)
  trigger = |KV| > cache_capacity
```

---

## 7) 负面结果记录（供 Discussion 章节参考）

这些方法已尝试但**未取得预期效果**,应在论文 Discussion 章节讨论：

### 7.1) INT8/INT4 量化 ❌

**尝试**: TorchAO INT8 weight-only 量化 MLP 层  
**结果**: 无 TPOT 增益,甚至更慢；质量稳定但速度回退  
**原因**: Batch=1 解码时量化开销抵消收益；A800 FP16 throughput 已接近峰值

```bash
# 不推荐运行,仅供记录
python experiments/eval_streaming_llm.py \
  --quantization int8_weight_only \
  --streaming-mode ours \
  ... # (其他参数)
```

### 7.2) Torch.compile + CUDA Graphs ❌

**尝试**: `torch.compile(mode="reduce-overhead")` 降低 Python 开销  
**结果**: CUDAGraphs 覆写错误,不稳定  
**原因**: RoPE rerotation 路径与 CUDA Graphs 不兼容

### 7.3) Refresh (尾部重新插入) ❌

**尝试**: 从驱逐区域重新插入选定 token 到尾部  
**结果**: PPL 崩溃（+10-30%）  
**原因**: 位置不一致（RoPE 编码错误）,违反了位置编码语义

```bash
# 不推荐运行,仅供记录
python experiments/eval_streaming_llm.py \
  --refresh-budget 64 \  # ❌ 已证实有害
  ... # (其他参数)
```

### 7.4) Auto-cap Overlap ❌

**尝试**: 在固定总长度下重新分配 overlap  
**结果**: 无 PPL 改善,TPOT 变化微小（测量噪声范围）  
**原因**: 总长度不变,只是内部重新分配,无实质性上下文增加

### 7.5) Static Cache ❌

**尝试**: 使用 HuggingFace StaticCache 减少动态重分配  
**结果**: 与 streaming 裁剪不兼容,触发 CUDA 越界错误  
**原因**: StaticCache 假设 KV 长度固定,与 streaming wrapper 的裁剪逻辑冲突

---

## 8) 重要说明与注意事项

### 8.1) 参数约束

- **Total cache size**: `n_sink + window_size (+ overlap) ≤ 2048`（推荐,避免显存溢出）
- **compress_every**: 建议范围 [16, 64],过大（≥128）会导致 PPL 断崖
- **cache_slack**: 建议范围 [0, 32],过大会增加显存压力
- **max_drop**: 建议设为 window_size/8 到 window_size/4

### 8.2) 数据集说明

- **WikiText**: 短段落拼接,拼接边界处上下文不连续,对 cache 裁剪更敏感
- **PG19**: 长篇小说,上下文连续,更适合评估长文本性能

### 8.3) 基线一致性

- 所有实验必须使用**相同的固定基线**（`results/baselines/`）
- 避免重复运行基线导致的比较不公平
- 基线采用 1-3 次运行的平均值,减少测量噪声

### 8.4) 环境要求

- **GPU**: NVIDIA A800 或同等 Ampere 架构 GPU (80GB 显存)
- **PyTorch**: ≥2.0 (CUDA 11.8+)
- **Transformers**: ≥4.35 (支持 StaticCache,虽然我们未使用)
- **Python**: 3.8-3.10

---

## 9) 快速开始（论文复现最小化流程）

如果只想快速复现论文核心结果,按以下顺序执行：

```bash
# 1. 生成固定基线（只需运行一次）
python experiments/run_fixed_baseline.py --n-sink 4 --window-size 2044 --runs 1

# 2. MIT 参考实现
python experiments/eval_streaming_llm.py \
  --dataset-name pg19 --dataset-config pg19 --max-eval-tokens 20000 \
  --n-sink 4 --window-size 2044 --compress-every 1 \
  --streaming-mode mit --mode streaming

# 3. Our Standard 实现
python experiments/eval_streaming_llm.py \
  --dataset-name pg19 --dataset-config pg19 --max-eval-tokens 20000 \
  --n-sink 4 --window-size 2044 --compress-every 1 \
  --streaming-mode ours --mode streaming

# 4. Our Best (Lazy Prune + Softlite)
python experiments/eval_streaming_llm.py \
  --dataset-name pg19 --dataset-config pg19 --max-eval-tokens 20000 \
  --n-sink 32 --window-size 2016 --compress-every 64 \
  --cache-slack 16 --max-drop 32 \
  --streaming-mode ours --mode streaming

# 5. 生成结果汇总表格（可选）
python experiments/summarize_results.py --results-dir results/ --output-file paper_results.md
```

**预计运行时间**: 
- 基线: ~30-40 分钟 (PG19 20k)
- 每个配置: ~15-25 分钟 (PG19 20k)
- WikiText 运行时间约为 PG19 的 1/5

---

## 10) 相关文档

- **[探索日志.md](./探索日志.md)**: 完整的实验探索记录和负面结果分析
- **[ADVANCED_IMPROVEMENT_PLAN.md](./ADVANCED_IMPROVEMENT_PLAN.md)**: 优化计划和理论分析
- **[LAZY_PRUNE_REPORT.md](./LAZY_PRUNE_REPORT.md)**: Lazy Pruning 详细报告
- **[CUDA_KERNEL_FULL_REPORT.md](./CUDA_KERNEL_FULL_REPORT.md)**: CUDA 算子融合实验报告
- **[QUANT_FAILURE_REPORT.md](./QUANT_FAILURE_REPORT.md)**: 量化失败分析报告

---

**最后更新**: 2025-12-29  
**实验基准**: 探索日志 §13-14 (Lazy Prune + Softlite 优化)
