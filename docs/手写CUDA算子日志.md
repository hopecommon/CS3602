# æ‰‹å†™CUDAç®—å­å®ç°æ—¥å¿— (2024-12-23)

## èƒŒæ™¯

æ ¹æ®profilingç»“æœï¼Œåœ¨StreamingLLMä¼˜åŒ–attention/KV cacheä¹‹åï¼Œå‰©ä½™çš„æ€§èƒ½ç“¶é¢ˆä¸»è¦åœ¨ï¼š
1. MLP (addmmæ“ä½œï¼Œå ç”¨æœ€å¤šæ—¶é—´)
2. LayerNorm (é«˜é¢‘è°ƒç”¨ï¼Œ33345æ¬¡)
3. Residual add (é«˜é¢‘è°ƒç”¨ï¼Œ65664æ¬¡)
4. Kernel launch overhead

ä¸ºäº†è¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼Œæˆ‘ä»¬å°è¯•æ‰‹å†™CUDAç®—å­æ¥èåˆLayerNormå’ŒResidual Addæ“ä½œã€‚

## å®æ–½æ­¥éª¤

### Phase 1: Kernelå®ç°ä¸æµ‹è¯• âœ“ (å®Œæˆ)

#### 1.1 è®¾è®¡ä¸å®ç°
- **ç›®æ ‡ç®—å­**: Fused LayerNorm + Residual Add
  ```python
  # åŸå§‹å®ç° (ä¸¤ä¸ªkernel)
  normalized = F.layer_norm(x, normalized_shape, weight, bias, eps)
  output = normalized + residual
  
  # èåˆå®ç° (ä¸€ä¸ªkernel)
  output = fused_layernorm_residual(x, residual, weight, bias, eps)
  ```

- **CUDA Kernelç‰¹æ€§**:
  - æ¯ä¸ªthread blockå¤„ç†ä¸€ä¸ªtoken (hidden_sizeç»´åº¦)
  - Warp-level parallel reductionè®¡ç®—mean/variance
  - å…±äº«å†…å­˜ä¼˜åŒ–è§„çº¦æ“ä½œ
  - æ”¯æŒFP32/FP16

#### 1.2 ç¼–è¯‘å’Œæµ‹è¯•ç»“æœ

**ç¯å¢ƒ**:
- PyTorch 2.3.0 + CUDA 12.1
- NVIDIA A800 (Ampereæ¶æ„)
- JITç¼–è¯‘ (torch.utils.cpp_extension)

**æ­£ç¡®æ€§æµ‹è¯•** âœ“:
```
FP32 - small batch [2, 128, 2560]:
  Max absolute error: 9.536743e-07
  Max relative error:  6.128661e-03
  Status: âœ“ PASS

FP16 - small batch [2, 128, 2560]:
  Max absolute error: 3.906250e-03
  Max relative error:  inf (éƒ¨åˆ†zeroå¯¼è‡´ï¼Œå®é™…å¯æ¥å—)
  Status: âœ“ PASS

FP16 - decode [1, 1, 2560]:
  Max absolute error: 3.906250e-03
  Status: âœ“ PASS

FP16 - large batch [4, 256, 2560]:
  Max absolute error: 3.906250e-03
  Status: âœ“ PASS

FP16 - long sequence [1, 2048, 2560]:
  Max absolute error: 3.906250e-03
  Status: âœ“ PASS
```

**æ€§èƒ½æµ‹è¯•** (åˆæ­¥):
```
FP16 - decode (1, 1, 2560):
  PyTorch time: 0.0305 ms
  Fused time:   0.0240 ms
  Speedup:      1.27x

FP16 - small batch (2, 128, 2560):
  PyTorch time: 0.0302 ms
  Fused time:   0.0242 ms
  Speedup:      1.25x

FP16 - large batch (4, 256, 2560):
  PyTorch time: 0.0304 ms
  Fused time:   0.0244 ms
  Speedup:      1.24x
```

### Phase 2: æ¨¡å‹é›†æˆ (è¿›è¡Œä¸­)

#### 2.1 GPTNeoXæ¶æ„åˆ†æ

æ¯ä¸ªGPTNeoXLayerçš„ç»“æ„:
```python
# Part 1: Attention path
hidden = input_layernorm(hidden_states)              # LN1
attn_output = attention(hidden, ...)                  # Attention
attn_output = post_attention_dropout(attn_output)     # Dropout
attn_output = attn_output + hidden_states             # Residual Add 1

# Part 2: MLP path
hidden = post_attention_layernorm(attn_output)        # LN2
mlp_output = mlp(hidden)                              # MLP
mlp_output = post_mlp_dropout(mlp_output)             # Dropout
output = mlp_output + attn_output                     # Residual Add 2
```

**èåˆç‚¹åˆ†æ**:
- ç›®æ ‡ï¼šèåˆLN1+Residual1, LN2+Residual2
- æŒ‘æˆ˜ï¼šLayerNormåœ¨residual add **ä¹‹å‰**ï¼Œä¸­é—´æœ‰å…¶ä»–æ“ä½œ
- æˆ‘ä»¬çš„kernel: `output = LN(x) + residual`
- å®é™…æµç¨‹: `output = operation(LN(x)) + residual`

#### 2.2 é›†æˆç­–ç•¥

**æ–¹æ¡ˆA**: Monkey-patching (å·²å®ç°æ¡†æ¶)
- æ–‡ä»¶ï¼š`fused_kernels/gptneox_integration.py`
- å®ç°ï¼š`apply_fused_kernels(model, enabled=True/False)`
- çŠ¶æ€ï¼šæ¡†æ¶å®Œæˆï¼Œä½†æœªçœŸæ­£è°ƒç”¨fused kernel
- é—®é¢˜ï¼šéœ€è¦ä¿®æ”¹è®¡ç®—é¡ºåºæˆ–kernelæ¥å£

**æ–¹æ¡ˆB**: è‡ªå®šä¹‰Layer (å¾…å®ç°)
- åˆ›å»º`FusedGPTNeoXLayer`ç»§æ‰¿åŸå§‹Layer
- é‡å†™forwardå‡½æ•°ï¼ŒçœŸæ­£ä½¿ç”¨fused kernel
- æ›´çµæ´»ä½†éœ€è¦æ›´å¤šä»£ç 

**æ–¹æ¡ˆC**: ä¿®æ”¹Kernelæ¥å£ (å¯èƒ½æœ€ä¼˜)
- å®ç°ä¸¤ä¸ªç‹¬ç«‹kernel:
  1. `fused_ln_forward`: è®¡ç®—LNä½†ä¸åŠ residualï¼Œé¿å…å†™å›
  2. `fused_residual_add`: åœ¨æœ€ååŠ residual
- åŒ¹é…å®é™…è®¡ç®—æµç¨‹

## å½“å‰çŠ¶æ€æ€»ç»“

### âœ“ å·²å®Œæˆ
1. **CUDA Kernelå®ç°**: å®Œæ•´çš„Fused LN+Residual kernel
2. **Pythonç»‘å®š**: JITç¼–è¯‘ï¼Œæ˜“ç”¨æ¥å£
3. **æ­£ç¡®æ€§éªŒè¯**: æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œæ•°å€¼ç²¾åº¦ç¬¦åˆé¢„æœŸ
4. **åˆæ­¥æ€§èƒ½æµ‹è¯•**: å•kernelæœ‰1.25xåŠ é€Ÿ

### ğŸ”„ è¿›è¡Œä¸­
5. **æ¨¡å‹é›†æˆ**: æ¡†æ¶æ­å»ºå®Œæˆï¼Œéœ€è¦è§£å†³è®¡ç®—é¡ºåºé—®é¢˜

### ğŸ“‹ å¾…å®Œæˆ
6. **Kernelä¼˜åŒ–**:
   - å‘é‡åŒ–å†…å­˜è®¿é—® (float4)
   - æ›´å¥½çš„çº¿ç¨‹å—é…ç½®
   - Welfordç®—æ³•æå‡æ•°å€¼ç¨³å®šæ€§
   
7. **çœŸæ­£é›†æˆåˆ°forward pass**:
   - ä¿®æ”¹GPTNeoXLayerçš„forward
   - ç¡®ä¿fused kernelè¢«å®é™…è°ƒç”¨
   
8. **ç«¯åˆ°ç«¯è¯„ä¼°**:
   - åœ¨decode-loopä¸­æµ‹è¯•
   - æµ‹é‡å®é™…TPOTæ”¹å–„
   - éªŒè¯PPLä¸å˜æ€§

## æ€§èƒ½é¢„æœŸ

**ç†è®ºåˆ†æ**:
- æ¯å±‚2ä¸ªèåˆç‚¹ Ã— 32å±‚ = 64ä¸ªèåˆæœºä¼š
- æ¯æ¬¡èåˆèŠ‚çœï¼š
  - 1ä¸ªkernel launch
  - 1æ¬¡ä¸­é—´tensorå†™å›global memory
  - 1æ¬¡è¯»å–global memory

**å®é™…æŒ‘æˆ˜**:
- LayerNormåªå æ€»æ—¶é—´~15% (profilingæ•°æ®)
- MLP (addmm) å ä¸»å¯¼åœ°ä½ (~45%)
- é¢„æœŸæ•´ä½“åŠ é€Ÿ: 5-10% (ä¿å®ˆä¼°è®¡)

**è¿›ä¸€æ­¥ä¼˜åŒ–æ–¹å‘**:
- Fused MLP (æ›´é«˜ä»·å€¼ï¼Œä½†æ›´å¤æ‚)
- Fused LN + Linear
- CUDA Graphs (å‡å°‘launch overhead)

## æŠ€æœ¯éš¾ç‚¹

### 1. FP16ç±»å‹è½¬æ¢
**é—®é¢˜**: CUDAä¸­`__half`ç±»å‹ä¸èƒ½ç›´æ¥cast
**è§£å†³**: å®ç°helperå‡½æ•°
```cuda
__device__ __forceinline__ float to_float(__half x) { return __half2float(x); }
__device__ __forceinline__ __half from_float(float x) { return __float2half(x); }
```

### 2. C++ä¸­çš„Kernelè°ƒç”¨è¯­æ³•
**é—®é¢˜**: CUDA kernelè°ƒç”¨è¯­æ³•`<<<>>>`åœ¨.cppæ–‡ä»¶ä¸­æ— æ•ˆ
**è§£å†³**: åˆ›å»ºlauncherå‡½æ•°åœ¨.cuä¸­
```cpp
// .cu file
extern "C" void fused_layernorm_residual_cuda_forward_float(...) {
    fused_layernorm_residual_kernel<float><<<blocks, threads, shared_mem_size>>>(...);
}

// .cpp file
extern "C" void fused_layernorm_residual_cuda_forward_float(...);
```

### 3. è®¡ç®—é¡ºåºä¸åŒ¹é…
**é—®é¢˜**: `LN(x) + residual` vs `op(LN(x)) + residual`
**å¾…è§£å†³**: éœ€è¦é‡æ–°è®¾è®¡kernelæ¥å£æˆ–ä¿®æ”¹æ¨¡å‹forward

## ä»£ç ç»“æ„

```
fused_kernels/
â”œâ”€â”€ __init__.py                 # æ¨¡å—å…¥å£
â”œâ”€â”€ fused_ln_residual.cu        # CUDA kernelå®ç° âœ“
â”œâ”€â”€ fused_ln_residual_cuda.cpp  # C++ç»‘å®š âœ“
â”œâ”€â”€ fused_ln_residual.py        # Pythonæ¥å£ âœ“
â”œâ”€â”€ gptneox_integration.py      # æ¨¡å‹é›†æˆ (æ¡†æ¶å®Œæˆ)
â”œâ”€â”€ test_fused_ln_residual.py   # å•å…ƒæµ‹è¯• âœ“
â”œâ”€â”€ test_integration.py         # é›†æˆæµ‹è¯• (å¾…è¿è¡Œ)
â””â”€â”€ README.md                   # è¯¦ç»†æ–‡æ¡£ âœ“
```

## æµ‹è¯•å‘½ä»¤

```bash
# å•å…ƒæµ‹è¯• (kernelæ­£ç¡®æ€§å’Œæ€§èƒ½)
python fused_kernels/test_fused_ln_residual.py

# é›†æˆæµ‹è¯• (æ¨¡å‹çº§åˆ«)
python fused_kernels/test_integration.py

# æ¸…é™¤ç¼–è¯‘ç¼“å­˜
rm -rf ~/.cache/torch_extensions/py312_cu121/fused_ln_residual
```

## ä¸‹ä¸€æ­¥è®¡åˆ’

### ç«‹å³è¡ŒåŠ¨ (1-2å¤©)
1. è§£å†³è®¡ç®—é¡ºåºé—®é¢˜ï¼Œå®ç°çœŸæ­£çš„æ¨¡å‹é›†æˆ
2. è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•ï¼Œæµ‹é‡å®é™…åŠ é€Ÿæ¯”
3. å¦‚æœåŠ é€Ÿä¸æ˜æ˜¾ï¼Œè€ƒè™‘ï¼š
   - ä¼˜åŒ–ç°æœ‰kernel (å‘é‡åŒ–ç­‰)
   - æˆ–è½¬å‘æ›´é«˜ä»·å€¼ç›®æ ‡ (MLP fusion)

### ä¸­æœŸç›®æ ‡ (3-5å¤©)
4. Kernelä¼˜åŒ–ï¼Œäº‰å–2-3xå•kernelåŠ é€Ÿ
5. å®ç°å¤šä¸ªèåˆç®—å­ (å¦‚æœè¯æ˜æœ‰ä»·å€¼)
6. å®Œæ•´çš„æ¶ˆèå®éªŒ

### è¯„ä¼°æ ‡å‡†
- **æˆåŠŸ**: TPOTé™ä½â‰¥5%, PPLä¸å˜
- **å¯æ¥å—**: TPOTé™ä½2-5%, PPLä¸å˜
- **å¤±è´¥**: TPOTæ— æ”¹å–„æˆ–PPLä¸‹é™

## å‚è€ƒèµ„æ–™

- [PyTorch C++ Extensions Tutorial](https://pytorch.org/tutorials/advanced/cpp_extension.html)
- [CUDA C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [NVIDIA Transformer Engine](https://github.com/NVIDIA/TransformerEngine)
- [FlashAttention Paper](https://arxiv.org/abs/2205.14135)

## ç»“è®º

âœ“ **æˆåŠŸå®ç°äº†ç¬¬ä¸€ä¸ªæ‰‹å†™CUDAç®—å­**
- Kernelæœ¬èº«æ­£ç¡®ä¸”æœ‰1.25xåŠ é€Ÿ
- ç¼–è¯‘å’Œæµ‹è¯•åŸºç¡€è®¾æ–½å®Œå¤‡
- ä¸ºåç»­ä¼˜åŒ–æ‰“ä¸‹åŸºç¡€

âš  **æ¨¡å‹é›†æˆä»éœ€è§£å†³**
- è®¡ç®—é¡ºåºä¸åŒ¹é…
- éœ€è¦ä¿®æ”¹forward passæˆ–é‡æ–°è®¾è®¡kernel

ğŸ“Š **æ€§èƒ½æ”¹å–„é¢„æœŸ**
- ä¿å®ˆä¼°è®¡: 5-10%æ•´ä½“TPOTæ”¹å–„
- å¦‚æœé…åˆå…¶ä»–ä¼˜åŒ–: å¯èƒ½æ›´é«˜

---
**è®°å½•æ—¶é—´**: 2024-12-23  
**çŠ¶æ€**: è¿›è¡Œä¸­
**ä¸‹æ¬¡æ›´æ–°**: å®Œæˆæ¨¡å‹é›†æˆå
