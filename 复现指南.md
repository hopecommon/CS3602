# 复现指南

## 0) 固定基线（只跑一次，之后复用）

我们统一用 PyTorch 原生推理 + sliding window 作为基线。
只需要跑一次（wikitext/pg19 各 1 个），以后不再重复跑。

python experiments/run_fixed_baseline.py \
--n-sink 4 \
--window-size 2044 \
--runs 1

生成：

- results/baselines/wikitext_baseline_avg.json
- results/baselines/pg19_baseline_avg.json

之后所有实验会自动读取这个 baseline。

———

## 1) MIT StreamingLLM（参考实现）

使用 --streaming-mode mit 即可：

python experiments/eval_streaming_llm.py \
--dataset-name pg19 --dataset-config pg19 \
--max-eval-tokens 20000 \
--n-sink 4 --window-size 2044 \
--compress-every 1 \
--streaming-mode mit \
--mode streaming

（wikitext 把 dataset 换成 wikitext-103-v1 即可）

———

## 2) Our StreamingLLM（标准配置）

这是我们自己的 Streaming 实现（非优化版）：

python experiments/eval_streaming_llm.py \
--dataset-name pg19 --dataset-config pg19 \
--max-eval-tokens 20000 \
--n-sink 4 --window-size 2044 \
--compress-every 1 \
--streaming-mode ours \
--mode streaming

———

## 3) Our Best (当前最快配置)

目前我们观察到 lazy prune（compress_every=32）是速度和PPL的平衡点。
如果最终确定 auto-cap + overlap=256 是最高点，可以这样跑：

python experiments/eval_streaming_llm.py \
--dataset-name pg19 --dataset-config pg19 \
--max-eval-tokens 20000 \
--n-sink 16 \
--window-size 1776 \
--overlap 256 \
--refresh-budget 0 \
--compress-every 32 \
--streaming-mode ours \
--mode streaming

说明：

- auto-cap 的逻辑是：window + sink + overlap <= 2048
- 上面这条等效于 auto-cap=2048, sink=16, overlap=256 的配置

如果最终决定“最快结果”是另一组参数，把这些参数替换即可。

———

## 4) 推荐在论文中固定写法

建议在论文里写成：

- Baseline: PyTorch native + sliding window (window=2044, sink=4)
- MIT: --streaming-mode mit
- Ours: --streaming-mode ours
- Best Ours: compress_every=32, no refresh, (auto-cap setting with chosen overlap)

———

## 5) 说明（避免误解）

- auto-cap 下 overlap 只是“窗口分配改变”，不会增加总长度；
- refresh（尾部插入+重赋位置）在我们的实验中是负收益，因此关闭。
