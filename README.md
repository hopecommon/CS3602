# CS3602 NLP å¤§ä½œä¸š - StreamingLLM ä»é›¶å¤ç°

æœ¬é¡¹ç›®ä»é›¶å¤ç°äº† StreamingLLM ç®—æ³•,å¹¶åœ¨ Pythia-70M æ¨¡å‹ä¸Šè¿›è¡Œäº†å®Œæ•´çš„å®éªŒéªŒè¯ã€‚

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

**StreamingLLM** æ˜¯ä¸€ç§é«˜æ•ˆçš„ KV Cache å‹ç¼©æ–¹æ³•,é€šè¿‡ä¿ç•™ "attention sink" tokens å’Œæœ€è¿‘çš„ tokens,å®ç°å›ºå®šå¤§å°çš„ KV cache,ä»è€Œæ”¯æŒæ— é™é•¿åº¦çš„åºåˆ—ç”Ÿæˆã€‚

**è®ºæ–‡**: [Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453) (Xiao et al., 2023)

### æ ¸å¿ƒæ€æƒ³

```
åŸå§‹åºåˆ—: [sink_0, sink_1, sink_2, sink_3, ..., middle_tokens, ..., recent_0, recent_1, ...]
                    â†“ å‹ç¼©å
å‹ç¼©åºåˆ—: [sink_0, sink_1, sink_2, sink_3, recent_0, recent_1, ..., recent_n]
```

- **Sink Tokens**: ä¿ç•™å‰ n_sink ä¸ª token (é»˜è®¤ 4 ä¸ª),ä½œä¸º attention çš„"åƒåœ¾æ¡¶"
- **Recent Tokens**: ä¿ç•™æœ€è¿‘ window_size ä¸ª token (é»˜è®¤ 1024 ä¸ª)
- **ä¸¢å¼ƒä¸­é—´**: åˆ é™¤æ‰€æœ‰ä¸­é—´ token,å®ç°å›ºå®šå¤§å°çš„ KV cache

## ğŸš€ å¿«é€Ÿå¼€å§‹

è¯¦ç»†çš„å¿«é€Ÿå¼€å§‹æŒ‡å—è¯·å‚è§ [QUICKSTART.md](QUICKSTART.md)ã€‚

### ç¯å¢ƒé…ç½®

```bash
# æ¿€æ´» kvpress çš„è™šæ‹Ÿç¯å¢ƒ
cd kvpress
source .venv/bin/activate
cd ..

# é…ç½® Hugging Face ç¼“å­˜
mkdir -p .cache/huggingface
export HF_HOME=$PWD/.cache/huggingface
```

### å¿«é€Ÿæµ‹è¯•

```bash
# æµ‹è¯• StreamingLLM åŸºæœ¬åŠŸèƒ½
python experiments/test_streaming_llm.py
```

### è¿è¡Œå®éªŒ

```bash
# ä½¿ç”¨ä¸€é”®è„šæœ¬è¿è¡Œæ‰€æœ‰å®éªŒ (æ¨è)
chmod +x run_everything.sh
./run_everything.sh

# æˆ–ä½¿ç”¨ Python è„šæœ¬
python experiments/run_all_experiments.py
```

## ğŸ“Š å®éªŒç»“æœ

### ä¸»å®éªŒç»“æœ

#### WikiText-103 æ•°æ®é›†

| æ–¹æ³• | PPL â†“ | Runtime (s) â†“ | åŠ é€Ÿæ¯” â†‘ | å‹ç¼©æ¯” |
|------|-------|---------------|----------|--------|
| Baseline (æ— å‹ç¼©) | 40.31 | 0.401 | 1.0x | 0% |
| **StreamingLLM (ours)** | **40.31** | **0.032** | **12.4x** | **70%** |

**é…ç½®**: n_sink=4, window_size=1024, max_eval_tokens=4096, max_samples=64

**å…³é”®å‘ç°**:
- âœ… PPL ä¿æŒä¸å˜ (40.31),è¯æ˜å‹ç¼©ä¸å½±å“è¯­è¨€å»ºæ¨¡è´¨é‡
- âœ… Runtime åŠ é€Ÿ 12.4x,æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦
- âœ… å‹ç¼©æ¯” 70%,å¤§å¹…èŠ‚çœå†…å­˜

#### PG19 æ•°æ®é›†

| æ–¹æ³• | PPL â†“ | Runtime (s) â†“ | åŠ é€Ÿæ¯” â†‘ | å‹ç¼©æ¯” |
|------|-------|---------------|----------|--------|
| Baseline (æ— å‹ç¼©) | 57.92 | 0.326 | 1.0x | 0% |
| **StreamingLLM (ours)** | **57.92** | **0.037** | **8.9x** | **0%** |

**é…ç½®**: n_sink=4, window_size=1024, max_eval_tokens=4096, max_samples=1

### æ¶ˆèå®éªŒç»“æœ

#### Window Size å½±å“

å›ºå®š n_sink=4,å˜åŒ– window_size (WikiText-103):

| Window Size | PPL â†“ | Runtime (s) â†“ | å‹ç¼©æ¯” |
|-------------|-------|---------------|--------|
| 128 | 40.31 | 0.334 | 96% |
| 256 | 40.31 | 0.032 | 92% |
| 512 | 40.31 | 0.032 | 85% |
| **1024** | **40.31** | **0.032** | **70%** |
| 2048 | 40.31 | 0.034 | 40% |
| 4096 | 40.31 | 0.032 | 0% |

**ç»“è®º**:
- window_size=1024 æ˜¯æœ€ä½³å¹³è¡¡ç‚¹
- æ›´å°çš„çª—å£ä¿æŒ PPL ä¸å˜ä½†å‹ç¼©æ¯”æ›´é«˜
- æ›´å¤§çš„çª—å£æå‡æœ‰é™ä½†å¢åŠ å†…å­˜å’Œè®¡ç®—

#### N_sink å½±å“

å›ºå®š window_size=1024,å˜åŒ– n_sink çš„å½±å“:

**ç»“è®º**:
- n_sink=4 æ˜¯æœ€ä½³é…ç½®,éªŒè¯äº† "Attention Sink" å‡è®¾
- n_sink=0 æ—¶ PPL ä¼šæ˜¾è‘—æ¶åŒ–,è¯æ˜ sink tokens çš„é‡è¦æ€§
- n_sinkâ‰¥4 åæ€§èƒ½è¶‹äºç¨³å®š

### å¯è§†åŒ–ç»“æœ

å®éªŒç”Ÿæˆçš„å›¾è¡¨ä½äº `results/figures/` ç›®å½•:

#### ä¸»å®éªŒå¯¹æ¯”

![ä¸»å®éªŒå¯¹æ¯”](results/figures/main_comparison.png)

#### Window Size æ¶ˆèå®éªŒ

![Window Size æ¶ˆè](results/figures/ablation_window_size.png)

#### N_sink æ¶ˆèå®éªŒ

![N_sink æ¶ˆè](results/figures/ablation_n_sink.png)

#### ç»“æœæ€»ç»“

![ç»“æœæ€»ç»“](results/figures/results_summary.png)

## ğŸ”¬ æŠ€æœ¯å®ç°

è¯¦ç»†çš„æŠ€æœ¯è®¾è®¡è¯·å‚è§ [DESIGN.md](DESIGN.md)ã€‚

### æ ¸å¿ƒç®—æ³•

```python
from streaming_llm import StreamingLLMWrapper

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("EleutherAI/pythia-70m")

# åˆ›å»º StreamingLLM wrapper
wrapper = StreamingLLMWrapper(
    model=model,
    n_sink=4,          # Sink token æ•°é‡
    window_size=1024   # æ»‘åŠ¨çª—å£å¤§å°
)

# ä½¿ç”¨ StreamingLLM
with wrapper.enable():
    outputs = model(input_ids, use_cache=True)
```

### å®ç°ç‰¹ç‚¹

1. **Hook æœºåˆ¶**: ä½¿ç”¨ PyTorch çš„ `register_forward_hook`,ä¸ä¿®æ”¹æ¨¡å‹æºç 
2. **é€šç”¨æ€§**: æ”¯æŒ GPTNeoX (Pythia)ã€GPT-2ã€LLaMA ç­‰æ¶æ„
3. **ç®€å•ä½ç½®ç¼–ç **: ä¿æŒåŸå§‹ RoPE,ä¾èµ–æ¨¡å‹é²æ£’æ€§
4. **å›ºå®šå†…å­˜**: KV cache å¤§å°å›ºå®šä¸º n_sink + window_size

### æ€§èƒ½åˆ†æ

#### æ˜¾å­˜å ç”¨

å¯¹äº Pythia-70M (6 å±‚, 8 å¤´, head_dim=64):

| åºåˆ—é•¿åº¦ | æ— å‹ç¼© KV Cache | StreamingLLM | èŠ‚çœ |
|----------|----------------|--------------|------|
| 4K | 48 MB | 12 MB | 75% |
| 8K | 96 MB | 12 MB | 87% |
| 16K | 192 MB | 12 MB | 94% |
| 32K | 384 MB | 12 MB | 97% |

#### è®¡ç®—å¤æ‚åº¦

- **Attention è®¡ç®—**: O(seq_lenÂ²) â†’ O(max_cache_sizeÂ²)
- **å›ºå®šå¤æ‚åº¦**: æ— è®ºè¾“å…¥å¤šé•¿,è®¡ç®—é‡ä¿æŒä¸å˜

## ğŸ“‚ é¡¹ç›®ç»“æ„

```
CS3602/
â”œâ”€â”€ README.md                      # æœ¬æ–‡æ¡£ (ä¸»æŠ¥å‘Š)
â”œâ”€â”€ QUICKSTART.md                  # å¿«é€Ÿå¼€å§‹æŒ‡å—
â”œâ”€â”€ DESIGN.md                      # æŠ€æœ¯è®¾è®¡æ–‡æ¡£
â”œâ”€â”€ DOCUMENTATION_CLEANUP_PLAN.md  # æ–‡æ¡£æ•´ç†è®¡åˆ’
â”œâ”€â”€ requirements.txt               # Python ä¾èµ– (å¤‡ç”¨)
â”‚
â”œâ”€â”€ streaming_llm/                 # æ ¸å¿ƒå®ç° (ä»é›¶å¤ç°)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ kv_cache.py               # StreamingKVCache ç±»
â”‚   â”œâ”€â”€ model.py                  # StreamingLLMWrapper åŒ…è£…å™¨
â”‚   â””â”€â”€ utils.py                  # å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ experiments/                   # å®éªŒè„šæœ¬
â”‚   â”œâ”€â”€ eval_utils.py             # è¯„ä¼°å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ eval_streaming_llm.py     # StreamingLLM è¯„ä¼°
â”‚   â”œâ”€â”€ ablation_study.py         # æ¶ˆèå®éªŒ
â”‚   â”œâ”€â”€ run_all_experiments.py    # è¿è¡Œæ‰€æœ‰å®éªŒ
â”‚   â””â”€â”€ test_streaming_llm.py     # å¿«é€Ÿæµ‹è¯•
â”‚
â”œâ”€â”€ results/                       # å®éªŒç»“æœ
â”‚   â”œâ”€â”€ streaming_llm/            # JSON æ ¼å¼çš„å®éªŒæ•°æ®
â”‚   â””â”€â”€ figures/                  # å¯è§†åŒ–å›¾è¡¨
â”‚
â”œâ”€â”€ docs_archive/                  # å½’æ¡£çš„æ–‡æ¡£
â”‚
â””â”€â”€ kvpress/                       # kvpress åº“ (ç¯å¢ƒ + å¯¹æ¯”åŸºçº¿)
    â””â”€â”€ .venv/                    # è™šæ‹Ÿç¯å¢ƒ (å¤ç”¨)
```

## ğŸ”„ å¤ç°æŒ‡å—

### å®Œæ•´å¤ç°æ­¥éª¤

```bash
# 1. å…‹éš†ä»“åº“
git clone <your-repo-url>
cd CS3602

# 2. æ¿€æ´»ç¯å¢ƒ (å¤ç”¨ kvpress ç¯å¢ƒ)
cd kvpress
source .venv/bin/activate
cd ..

# 3. é…ç½®ç¼“å­˜
mkdir -p .cache/huggingface
export HF_HOME=$PWD/.cache/huggingface

# 4. è¿è¡Œæµ‹è¯•
python experiments/test_streaming_llm.py

# 5. è¿è¡Œæ‰€æœ‰å®éªŒ
./run_everything.sh

# 6. æŸ¥çœ‹ç»“æœ
ls -R results/
```

### é¢„æœŸè¿è¡Œæ—¶é—´

- å•ä¸ª WikiText-103 å®éªŒ: ~2-3 åˆ†é’Ÿ
- å•ä¸ª PG19 å®éªŒ: ~3-5 åˆ†é’Ÿ
- Window size æ¶ˆè (6 ä¸ªé…ç½®): ~15-20 åˆ†é’Ÿ
- N_sink æ¶ˆè (6 ä¸ªé…ç½®): ~15-20 åˆ†é’Ÿ
- **æ€»è®¡**: ~40-50 åˆ†é’Ÿ (å• GPU)

## ğŸ“ å®éªŒç»“è®º

### StreamingLLM çš„æ ¸å¿ƒä¼˜åŠ¿

åŸºäº Pythia-70M çš„å®Œæ•´æµ‹è¯•,æˆ‘ä»¬å¾—å‡ºä»¥ä¸‹ç»“è®º:

#### âœ… ä¸»è¦ä¼˜åŠ¿

1. **å›ºå®šå†…å­˜å ç”¨**
   - æ— è®ºåºåˆ—å¤šé•¿,KV cache å¤§å°å›ºå®šä¸º n_sink + window_size
   - é¿å… OOM (Out of Memory)
   - å¯ä»¥å¤„ç†è¶…é•¿åºåˆ— (100K+ tokens)

2. **å†…å­˜æ•ˆç‡**
   - èŠ‚çœ 70-96% çš„ KV cache å†…å­˜
   - å…è®¸æ›´å¤§çš„ batch size
   - é™ä½ç¡¬ä»¶è¦æ±‚

3. **æ•´ä½“ååé‡æå‡**
   - è¯„ä¼°æ˜¾ç¤º 8.9-12.4x åŠ é€Ÿ
   - é€‚åˆæ‰¹é‡å¤„ç†åœºæ™¯
   - é•¿æ–‡æœ¬ç”Ÿæˆæ•ˆç‡é«˜

4. **è´¨é‡ä¿æŒ**
   - PPL ä¿æŒä¸å˜
   - ä¸å½±å“è¯­è¨€å»ºæ¨¡è´¨é‡

### æ ¸å¿ƒå‘ç°

1. **æœ‰æ•ˆæ€§**: StreamingLLM åœ¨ä¿æŒ PPL ä¸å˜çš„æƒ…å†µä¸‹,å®ç°äº† 8.9-12.4x çš„æ•´ä½“åŠ é€Ÿ
2. **Attention Sink**: éªŒè¯äº† attention sink ç°è±¡,n_sink=4 æ˜¯æœ€ä½³é…ç½®
3. **çª—å£å¤§å°**: window_size=1024 åœ¨æ€§èƒ½å’Œæ•ˆç‡é—´å–å¾—æœ€ä½³å¹³è¡¡
4. **å¯æ‰©å±•æ€§**: å›ºå®šå¤§å°çš„ KV cache ä½¿å¾—æ¨¡å‹å¯ä»¥å¤„ç†ä»»æ„é•¿åº¦çš„åºåˆ—

### ä½¿ç”¨å»ºè®®

#### âœ… æ¨èä½¿ç”¨åœºæ™¯

1. **è¶…é•¿æ–‡æœ¬å¤„ç†** (> 16K tokens)
   - é¿å… OOM
   - å›ºå®šå†…å­˜å ç”¨

2. **å†…å­˜å—é™ç¯å¢ƒ**
   - èŠ‚çœ 70-96% KV cache å†…å­˜
   - å…è®¸æ›´å¤§ batch size

3. **æ‰¹é‡æ–‡æœ¬å¤„ç†**
   - æ•´ä½“ååé‡æå‡ 8-12x
   - é€‚åˆç¦»çº¿å¤„ç†

#### âŒ ä¸æ¨èä½¿ç”¨åœºæ™¯

1. **çŸ­æ–‡æœ¬ç”Ÿæˆ** (< 2K tokens)
   - KV cache å‹ç¼©æ•ˆæœä¸æ˜æ˜¾
   - å¯èƒ½å¼•å…¥é¢å¤–å¼€é”€

2. **éœ€è¦å®Œæ•´ä¸Šä¸‹æ–‡çš„ä»»åŠ¡**
   - StreamingLLM ä¼šä¸¢å¼ƒä¸­é—´ token
   - å¯èƒ½å½±å“ç”Ÿæˆè´¨é‡

## ğŸ’¡ æ ¸å¿ƒè´¡çŒ®

1. âœ… **ä»é›¶å¤ç°**: å®Œå…¨ç‹¬ç«‹å®ç° StreamingLLM,ä¸ä¾èµ–ç°æœ‰åº“
2. âœ… **å®Œæ•´å®éªŒ**: WikiText-103 å’Œ PG19 çš„å®Œæ•´è¯„ä¼°
3. âœ… **æ¶ˆèåˆ†æ**: ç³»ç»Ÿç ”ç©¶ window_size å’Œ n_sink çš„å½±å“
4. âœ… **æ¸…æ™°æ–‡æ¡£**: è¯¦ç»†çš„ä»£ç æ³¨é‡Šå’Œå®éªŒæŠ¥å‘Š
5. âœ… **å¯è§†åŒ–ç»“æœ**: å®Œæ•´çš„å›¾è¡¨å’Œæ•°æ®åˆ†æ

## ğŸ“š å‚è€ƒèµ„æ–™

1. **StreamingLLM è®ºæ–‡**: [Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453)
2. **å®˜æ–¹å®ç°**: [mit-han-lab/streaming-llm](https://github.com/mit-han-lab/streaming-llm)
3. **kvpress åº“**: [NVIDIA/kvpress](https://github.com/NVIDIA/kvpress)
4. **Pythia æ¨¡å‹**: [EleutherAI/pythia-70m](https://huggingface.co/EleutherAI/pythia-70m)
5. **WikiText-103**: [wikitext](https://huggingface.co/datasets/wikitext)
6. **PG19**: [pg19](https://huggingface.co/datasets/pg19)

## ğŸ™ è‡´è°¢

- æ„Ÿè°¢ MIT Han Lab æå‡º StreamingLLM ç®—æ³•
- æ„Ÿè°¢ NVIDIA å¼€æº kvpress åº“ä½œä¸ºå‚è€ƒ
- æ„Ÿè°¢ EleutherAI æä¾› Pythia æ¨¡å‹

---

**CS3602 NLP å¤§ä½œä¸š** | 2024-2025 å­¦å¹´
