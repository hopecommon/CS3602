\begin{table}[H]
\centering
\caption{Summary of investigated but ineffective optimization routes in our batch-1 streaming setting.}
\small
\begin{tabular}{p{0.34\linewidth}p{0.18\linewidth}p{0.40\linewidth}}
\toprule
Method & Outcome & Notes (see project logs) \\
\midrule
FlashAttention / FlashDecoding & Inconclusive / low ROI & Integration is sensitive to environment and attention implementation; once streaming bounds attention length, speed is often dominated by MLP and launch/framework overhead. \\
Speculative decoding & Negative result & In long-context workloads, draft/target mismatch yields low acceptance rates, making SpecDec slower than normal decoding. \\
Quantization (TorchAO INT8/INT4) & Slower / unstable & INT8 WO v1 produced NaNs; v2 is stable but slower for batch-1 decode in our stack; INT4 backend dependencies were problematic. \\
\texttt{torch.compile} / CUDA Graphs & Unstable & Repeated-run CUDA graph overwrite errors observed in rotary-embedding path; shape/cache semantics hinder capture. \\
HF StaticCache & Incompatible & StaticCache assumes fixed cache updates; pruning can trigger device-side asserts (index out of bounds). \\
CUDA fusion (residual/LN) & Slower & Amdahl's law: residual/LN is a small fraction; custom kernel launch overhead dominated (0.92$\times$, TPOT +8.6\%). \\
\bottomrule
\end{tabular}
\end{table}
