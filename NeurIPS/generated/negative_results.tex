\begin{table}[t]
\centering
\caption{Summary of investigated but ineffective optimization routes in our batch-1 streaming setting.}
\small
\begin{tabular}{p{0.34\linewidth}p{0.18\linewidth}p{0.40\linewidth}}
\toprule
Method & Outcome & Notes (see project logs) \\
\midrule
FlashAttention / FlashDecoding & No gain / hard to integrate & Streaming already bounds attention length; remaining bottlenecks are MLP and launch/framework overhead; RoPE re-alignment complicates clean integration. \\
Speculative decoding & No gain / unreliable & Long-form generation yields low acceptance; cache consistency with pruning is fragile. \\
Quantization (TorchAO INT8/INT4) & Slower / unstable & INT8 WO v1 produced NaNs; v2 is stable but slower for batch-1 decode in our stack; INT4 backend dependencies were problematic. \\
\texttt{torch.compile} / CUDA Graphs & Unstable & Repeated-run CUDA graph overwrite errors observed in rotary-embedding path; shape/cache semantics hinder capture. \\
HF StaticCache & Incompatible & StaticCache assumes fixed cache updates; pruning can trigger device-side asserts (index out of bounds). \\
CUDA fusion (residual/LN) & No gain & Amdahl's law: residual/LN is a small fraction; custom kernel launch overhead dominated, leading to slowdown. \\
\bottomrule
\end{tabular}
\end{table}
