% !TeX program = pdflatex
\documentclass{article}

\usepackage{neurips_2025}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}

\newcommand{\insertdata}[1]{\textcolor{red}{[INSERT #1]}}

\title{Efficient KV Cache Management for Long-Context LLM Inference:\\ Lazy Pruning, Slack, and Staged Eviction}

% Course Assignment Submission - Two Authors + Advisor
\author{%
  学生姓名1 \\
  学号: 学号1 \\
  上海交通大学 计算机科学与工程系 \\
  \texttt{email1@sjtu.edu.cn} \\
  \And
  学生姓名2 \\
  学号: 学号2 \\
  上海交通大学 计算机科学与工程系 \\
  \texttt{email2@sjtu.edu.cn} \\
  \AND
  指导教师: 教师姓名 \\
  上海交通大学 计算机科学与工程系 \\
}

\begin{document}

\maketitle

\begin{abstract}
Long-context LLM inference is bottlenecked by KV-cache growth and per-token runtime overhead. StreamingLLM~\cite{xiao2023streamingllm} bounds KV-cache size by retaining attention sinks and a recent-token window, but practical decoding can suffer from (i) non-trivial KV pruning overhead and (ii) quality degradation caused by periodic hard evictions, especially on long-form generation. We propose three lightweight, training-free KV-cache management mechanisms on top of StreamingLLM: \textbf{Lazy Pruning} (amortize pruning by triggering after $R$ tokens of overflow), \textbf{Slack} (a hard-cap buffer that enables staged evictions under bounded memory), and \textbf{Max\_Drop} (limit the maximum tokens evicted per prune event). We evaluate on Pythia-2.8B using WikiText-103 (sanity) and PG19 (long-context), and report TTFT/TPOT/throughput/memory/perplexity trade-offs. We also document why FlashAttention, speculative decoding, quantization, and CUDA operator fusion did not improve batch-1 streaming decode under our setting.
\end{abstract}

\section{Introduction}
Long-context LLM inference is challenging because both computation and memory scale with the context length. During autoregressive decoding, the KV cache grows linearly with the number of processed tokens, increasing attention cost and memory traffic. StreamingLLM~\cite{xiao2023streamingllm} mitigates this by keeping only (i) the first $S$ ``sink'' tokens and (ii) a sliding window of the most recent $W$ tokens, thereby bounding the effective KV-cache length.

\paragraph{Motivation.}
In our setting (Pythia-2.8B on NVIDIA A800), once attention length is bounded, decode becomes dominated by MLP and framework/launch overhead. Meanwhile, KV pruning itself can incur additional overhead due to slicing/copy and rotary-position re-alignment. More importantly, periodic hard eviction introduces distribution shifts that manifest as token-level NLL spikes and perplexity (PPL) degradation on long-form datasets. These observations motivate KV-management mechanisms that (i) reduce amortized pruning overhead and (ii) stage evictions to stabilize quality, without modifying attention kernels or model weights.

\paragraph{Contributions.}
We propose three training-free, modular mechanisms on top of StreamingLLM:
(i) \textbf{Lazy Pruning}, which triggers pruning only after the cache overflow reaches $R$ tokens;
(ii) \textbf{Slack}, a hard capacity buffer of $\sigma$ tokens that supports staged pruning;
(iii) \textbf{Max\_Drop}, which limits the maximum tokens evicted per prune event to $\delta$.
We provide a reproducible evaluation pipeline and summarize negative results for methods that are effective in other regimes but do not improve batch-1 streaming decode.

\section{Related Work}
\label{sec:related}
\textbf{Long-context inference and KV-cache compression.}
Test-time KV-cache management is a widely used approach to make long-context inference feasible, including retention/eviction heuristics and cache compression~\cite{xiao2023streamingllm,zhang2023h2o,liu2023scissorhands,kwon2023paged}. Our work builds on StreamingLLM's Start+Recent rule and focuses on \emph{training-free} extensions that target the practical costs of pruning and eviction-induced distribution shifts.
\textbf{Kernel- and decoding-level accelerations.}
Attention kernels such as FlashAttention aim to reduce attention IO cost~\cite{dao2022flashattention,dao2023flashattention2}, while speculative decoding accelerates generation by drafting tokens~\cite{leviathan2023speculative,chen2023accelerating}. Post-training quantization reduces compute/memory but its benefits depend on hardware and runtime regime~\cite{dettmers2022llmint8,frantar2023gptq}. We empirically find these do not improve batch-1 streaming decode in our setting and analyze the underlying reasons (Sec.~\ref{sec:negative}).

\section{Method}
\label{sec:method}
We first define Start+Recent streaming, then formalize our three mechanisms.

\subsection{Preliminaries: Start+Recent StreamingLLM}
Let $x_{1:t}$ be the processed prefix at decode step $t$. For each layer $\ell$, let $(\mathbf{K}_t^\ell,\mathbf{V}_t^\ell)$ denote the KV cache with sequence length $L_t$ along the cache dimension. Start+Recent streaming retains (i) the first $S$ sink tokens, and (ii) the most recent $W$ tokens. Define the soft capacity
\begin{equation}
  C_0 \triangleq S + W.
\end{equation}
Our code also supports optional \emph{non-core} retained tokens (e.g., overlap/refresh); we denote their total budget by $B\ge 0$ and define the effective soft capacity
\begin{equation}
  C \triangleq C_0 + B.
\end{equation}
Unless stated otherwise, we use $B=0$ in paper experiments to isolate our three mechanisms.
When pruning is performed, the retained index set is
\begin{equation}
  \mathcal{I}_t \;=\; \{0,1,\ldots,S-1\}\;\cup\;\{L_t-W,\ldots,L_t-1\},
\end{equation}
with the recent segment clamped to avoid overlap with the sink segment.

\paragraph{RoPE consistency.}
For rotary-position-embedding models (e.g., Pythia/GPT-NeoX), pruning changes token positions in the cache; therefore, cached keys must be re-aligned to the new positions. Let $p\in\mathcal{I}_t$ be an old token position and $p' = f_t(p)\in\{0,\ldots,|\mathcal{I}_t|-1\}$ be its new position after compaction. RoPE encodes position via per-dimension frequencies $\omega_i$ (from \texttt{inv\_freq}). Re-alignment is equivalent to applying a \emph{delta-rotation} with $\Delta p = p'-p$:
\begin{equation}
\begin{bmatrix}
k'_{2i}\\k'_{2i+1}
\end{bmatrix}
\;=\;
\begin{bmatrix}
\cos(\omega_i \Delta p) & -\sin(\omega_i \Delta p)\\
\sin(\omega_i \Delta p) & \cos(\omega_i \Delta p)
\end{bmatrix}
\begin{bmatrix}
k_{2i}\\k_{2i+1}
\end{bmatrix}.
\end{equation}
Our implementation exploits that Start+Recent yields a constant $\Delta p$ for the contiguous recent block (a constant shift), and falls back to the general per-token re-rotation when tokens are explicitly relocated (e.g., refresh tokens).

\subsection{Lazy Pruning}
Na\"ive streaming prunes as soon as the cache exceeds $C$, which can add overhead due to slicing/copy and KV relocation. We instead amortize pruning by allowing the cache to exceed $C$ by up to $R{-}1$ tokens before triggering a prune.
Define overflow:
\begin{equation}
  \mathrm{overflow}_t \triangleq L_t - C.
\end{equation}
Let $R$ be the \emph{overflow allowance} hyperparameter (in code: \texttt{compress\_every}). We allow $R=0$ to disable pruning for debugging (unbounded memory), and use $R\ge 1$ in all bounded-memory settings. Lazy pruning triggers only when overflow reaches $R$:
\begin{equation}
  \mathrm{PruneTrigger}(t) \triangleq \mathbb{1}[R>0]\cdot \mathbb{1}\big[\mathrm{overflow}_t \ge R\big].
\end{equation}
This matches the implementation semantics and avoids the ambiguity that would arise from interpreting $R=0$ as ``prune immediately''.

\subsection{Slack (Hard-Cap Buffer)}
Slack introduces a \emph{hard} capacity that bounds temporary cache growth during staged pruning. Specifically, we define
\begin{equation}
  H \triangleq C + \sigma,
\end{equation}
where $\sigma \ge 0$ is a fixed slack budget. In our implementation, $\sigma$ does \emph{not} change the pruning trigger; it caps the post-prune target length when combined with Max\_Drop (Sec.~\ref{subsec:maxdrop}).

\subsection{Max\_Drop (Staged Eviction)}
\label{subsec:maxdrop}
Periodic pruning can cause ``eviction cliffs'' when a large block is removed at once, which we observed as token-level NLL spikes on long-form generation. Max\_Drop limits the maximum number of tokens evicted in a single prune event.

Let $\delta \ge 0$ be the maximum drop size. When pruning is triggered, we set a target retained length
\begin{equation}
  L_t^\star \;=\;
  \begin{cases}
    C, & \delta = 0 \\
    \min\big(\max(L_t-\delta,\; C),\; H\big), & \delta > 0
  \end{cases}
\end{equation}
and prune the cache to keep exactly $L_t^\star$ tokens using the Start+Recent rule. This turns a single large eviction into multiple smaller evictions across steps while remaining within the hard capacity $H$.

\paragraph{Hyperparameter interactions and a toy example.}
When $\delta>0$, staged eviction can leave $L_t^\star > C$, so pruning may trigger again soon; this is intentional. A practical rule-of-thumb to avoid pruning \emph{every} step when the cache saturates at $H$ is to choose $\sigma < R$ (so that after pruning to $H$, the overflow remains $<R$ until enough new tokens arrive).
For example, with $(S,W,R,\sigma,\delta)=(4,2044,32,16,32)$ (so $C=2048$, $H=2064$), if a step reaches $L_t=2090$ (overflow $=42$), we prune to $L_t^\star=\min(\max(2090-32,2048),2064)=2058$ and continue staging further evictions as decoding proceeds.

\subsection{Algorithm Summary}
Algorithm~\ref{alg:kv} summarizes the pruning control logic (RoPE re-alignment and buffer management are omitted for clarity).
\begin{algorithm}[t]
\caption{KV pruning with Lazy Pruning, Slack, and Max\_Drop}
\label{alg:kv}
\begin{algorithmic}[1]
\REQUIRE sink size $S$, window size $W$, extra budget $B$ (default 0), overflow allowance $R$ (0 disables), slack $\sigma$, max\_drop $\delta$
\STATE $C_0 \leftarrow S+W$, $C \leftarrow C_0 + B$, $H \leftarrow C+\sigma$
\STATE observe cache length $L_t$
\IF{$R=0$} \STATE \textbf{return} \ENDIF
\IF{$L_t \le C$} \STATE \textbf{return} \ENDIF
\IF{$L_t - C < R$} \STATE \textbf{return} \ENDIF
\IF{$\delta = 0$}
  \STATE $L_t^\star \leftarrow C$
\ELSE
  \STATE $L_t^\star \leftarrow \min(\max(L_t-\delta,\; C),\; H)$
\ENDIF
\STATE keep first $S$ tokens and most recent $(L_t^\star - S)$ tokens
\STATE prune KV cache and re-align RoPE positions
\end{algorithmic}
\end{algorithm}

\section{Experiments}
\label{sec:exp}

\subsection{Setup}
We evaluate on Pythia-2.8B. We use WikiText-103 as a short-context sanity check and PG19 as the primary long-context benchmark~\cite{pg19}. We report TTFT, TPOT (ms/token), throughput (tok/s), total runtime, peak GPU memory, and perplexity (PPL).
\textbf{Baselines and fairness.} Our long-context evaluation must respect the model's maximum position length. Therefore, we compare methods under the same \emph{effective context cap} $C$ (Sec.~\ref{sec:method}): the non-streaming baseline uses a sliding window of length $C$ and recomputes attention each step (no KV cache), while StreamingLLM variants reuse KV cache and apply pruning/re-alignment to stay within the same cap. All methods share the same model, dataset segment, and evaluation protocol implemented in \texttt{experiments/eval\_streaming\_llm.py}.

\paragraph{Protocol and repeatability.}
To reduce measurement noise, our reproduction script runs a warmup pass followed by $N$ repeated trials for each configuration and reports mean and standard deviation (saved in JSON). Each JSON record includes environment fingerprints (GPU, driver, torch/CUDA) to prevent reusing baselines across incompatible environments.

\subsection{Main Results and Ablations}
We provide a one-click script (\texttt{run\_paper\_experiments.sh}) that runs all configurations and generates LaTeX tables automatically. If the tables are not generated yet, we show placeholders.
\paragraph{Implementation-path fairness check.}
To ensure that improvements are not caused by divergent code paths, we also report an \emph{Ours-framework-only} configuration that uses our wrapper implementation but matches Start+Recent semantics (MIT-style cache backend) with all proposed mechanisms disabled.
\IfFileExists{generated/tables.tex}{
  \input{generated/tables.tex}
}{
  \begin{table}[t]
  \centering
  \caption{Main results on PG19 (long-context).}
  \small
  \begin{tabular}{lcccc}
  \toprule
  Method & TPOT$\downarrow$ & Speedup$\uparrow$ & PPL$\downarrow$ & Peak Mem$\downarrow$ \\
  \midrule
  Baseline (Sliding Window, no KV) & \insertdata{TPOT} & 1.00$\times$ & \insertdata{PPL} & \insertdata{Mem} \\
  StreamingLLM (MIT) & \insertdata{TPOT} & \insertdata{Speedup} & \insertdata{PPL} & \insertdata{Mem} \\
  Ours (Lazy/Slack/Max\_Drop) & \insertdata{TPOT} & \insertdata{Speedup} & \insertdata{PPL} & \insertdata{Mem} \\
  \bottomrule
  \end{tabular}
  \end{table}
}

\paragraph{Ablations.}
\IfFileExists{generated/ablations.tex}{
  \input{generated/ablations.tex}
}{
  \begin{table}[t]
  \centering
  \caption{Ablation placeholders (generated by script).}
  \small
  \begin{tabular}{lccc}
  \toprule
  Setting & TPOT$\downarrow$ & Speedup$\uparrow$ & PPL$\downarrow$ \\
  \midrule
  Ours w/o Slack & \insertdata{TPOT} & \insertdata{Speedup} & \insertdata{PPL} \\
  Ours w/o Max\_Drop & \insertdata{TPOT} & \insertdata{Speedup} & \insertdata{PPL} \\
  Ours (full) & \insertdata{TPOT} & \insertdata{Speedup} & \insertdata{PPL} \\
  \bottomrule
  \end{tabular}
  \end{table}
}

\section{Discussion}
\label{sec:negative}
\paragraph{Why do popular inference accelerations fail in batch-1 streaming?}
In our setting, streaming bounds attention length, shifting the bottleneck toward MLP and framework/launch overhead. Therefore, further optimizing attention kernels can have limited impact. Moreover, methods that assume static shapes or stable cache semantics may be incompatible with pruning and RoPE re-alignment in streaming decode.

\paragraph{Attempts and analysis of ineffective methods.}
We tested several common acceleration routes (see our project log in \texttt{docs/}): (i) FlashAttention integration was non-trivial under RoPE re-alignment and did not improve batch-1 TPOT in our environment; (ii) speculative decoding was unreliable on long-form generation and incompatible with streaming cache management; (iii) quantization via TorchAO exhibited numerical issues (INT8 v1) and remained slower even when stabilized (INT8 v2) in batch-1 streaming decode; (iv) \texttt{torch.compile} with CUDA Graphs was unstable due to overwritten outputs in rotary-embedding paths; (v) HuggingFace StaticCache conflicted with pruning updates and triggered device-side asserts. These results highlight that strong batch-inference accelerations do not directly transfer to batch-1 streaming decode.

\IfFileExists{generated/negative_results.tex}{
  \input{generated/negative_results.tex}
}{}

\section{Conclusion}
We presented three modular KV-cache management mechanisms for StreamingLLM: Lazy Pruning, Slack, and Max\_Drop. Our method is training-free and targets long-context decoding where pruning overhead and eviction-induced quality loss become important. We provide a reproducible pipeline and report negative results to clarify which acceleration techniques do or do not apply in this regime.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
