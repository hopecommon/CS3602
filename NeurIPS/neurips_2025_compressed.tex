% !TeX program = pdflatex
\documentclass{article}

\usepackage{neurips_2025}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}

\newcommand{\insertdata}[1]{\textcolor{red}{[INSERT #1]}}

\title{Efficient KV Cache Management for Long-Context LLM Inference:\\ Lazy Pruning, Slack, and Staged Eviction}

% Course Assignment Submission - Two Authors + Advisor
\author{%
  学生姓名1 \\
  学号: 学号1 \\
  上海交通大学 计算机科学与工程系 \\
  \texttt{email1@sjtu.edu.cn} \\
  \And
  学生姓名2 \\
  学号: 学号2 \\
  上海交通大学 计算机科学与工程系 \\
  \texttt{email2@sjtu.edu.cn} \\
  \AND
  指导教师: 教师姓名 \\
  上海交通大学 计算机科学与工程系 \\
}

\begin{document}

\maketitle

\begin{abstract}
Long-context LLM inference is bottlenecked by KV-cache growth and per-token runtime overhead. StreamingLLM~\cite{xiao2023streamingllm} bounds KV-cache size by retaining attention sinks and a recent-token window, but practical decoding can suffer from (i) non-trivial KV pruning overhead and (ii) quality degradation caused by periodic hard evictions, especially on long-form generation. We propose three lightweight, training-free KV-cache management mechanisms on top of StreamingLLM: \textbf{Lazy Pruning} (amortize pruning by triggering after $R$ tokens of overflow), \textbf{Slack} (a hard-cap buffer that enables staged evictions under bounded memory), and \textbf{Max\_Drop} (limit the maximum tokens evicted per prune event). We evaluate on Pythia-2.8B using WikiText-103 (sanity) and PG19 (long-context), and report TTFT/TPOT/throughput/memory/perplexity trade-offs. We also document why FlashAttention, speculative decoding, quantization, and CUDA operator fusion did not improve batch-1 streaming decode under our setting.
\end{abstract}

\section{Introduction}
Long-context LLM inference is challenging because both computation and memory scale with the context length. During autoregressive decoding, the KV cache grows linearly with the number of processed tokens, increasing attention cost and memory traffic. StreamingLLM~\cite{xiao2023streamingllm} mitigates this by keeping only (i) the first $S$ ``sink'' tokens and (ii) a sliding window of the most recent $W$ tokens, thereby bounding the effective KV-cache length.

\paragraph{Motivation.}
In our setting (Pythia-2.8B on NVIDIA A800), once attention length is bounded, decode becomes dominated by MLP and framework/launch overhead. Meanwhile, KV pruning itself can incur additional overhead due to slicing/copy and rotary-position re-alignment. More importantly, periodic hard eviction introduces distribution shifts that manifest as token-level NLL spikes and perplexity (PPL) degradation on long-form datasets. These observations motivate KV-management mechanisms that (i) reduce amortized pruning overhead and (ii) stage evictions to stabilize quality, without modifying attention kernels or model weights.

\paragraph{Contributions.}
We propose three training-free, modular mechanisms on top of StreamingLLM:
(i) \textbf{Lazy Pruning}, which triggers pruning only after the cache overflow reaches $R$ tokens;
(ii) \textbf{Slack}, a hard capacity buffer of $\sigma$ tokens that supports staged pruning;
(iii) \textbf{Max\_Drop}, which limits the maximum tokens evicted per prune event to $\delta$.
We provide a reproducible evaluation pipeline and summarize negative results for methods that are effective in other regimes but do not improve batch-1 streaming decode.

\section{Method}
\label{sec:method}
We first define Start+Recent streaming, then formalize our three mechanisms.

\subsection{Preliminaries: Start+Recent StreamingLLM}
Let $x_{1:t}$ be the processed prefix at decode step $t$. For each layer $\ell$, let $(\mathbf{K}_t^\ell,\mathbf{V}_t^\ell)$ denote the KV cache with sequence length $L_t$ along the cache dimension. Start+Recent streaming retains (i) the first $S$ sink tokens, and (ii) the most recent $W$ tokens. Define the soft capacity
\begin{equation}
  C \triangleq S + W.
\end{equation}
When pruning is performed, the retained index set is
\begin{equation}
  \mathcal{I}_t \;=\; \{0,1,\ldots,S-1\}\;\cup\;\{L_t-W,\ldots,L_t-1\},
\end{equation}
with the recent segment clamped to avoid overlap with the sink segment.

\paragraph{RoPE consistency.}
For rotary-position-embedding models (e.g., Pythia/GPT-NeoX), pruning changes token positions in the cache; therefore, cached keys must be re-aligned to the new positions. Our implementation applies a constant RoPE shift for the retained recent block and performs re-rotation when tokens are relocated.

\subsection{Lazy Pruning}
Na\"ive streaming prunes as soon as the cache exceeds $C$, which can add overhead due to slicing/copy and KV relocation. We instead amortize pruning using a threshold $R$.
Define overflow:
\begin{equation}
  \mathrm{overflow}_t \triangleq L_t - C.
\end{equation}
Lazy pruning triggers only when overflow reaches $R$:
\begin{equation}
  \mathrm{PruneTrigger}(t) \triangleq \mathbb{1}\big[\mathrm{overflow}_t \ge R\big].
\end{equation}
When $R=0$, pruning is disabled and the cache grows without eviction (not recommended for long sequences).

\subsection{Slack (Hard-Cap Buffer)}
Slack introduces a \emph{hard} capacity that bounds temporary cache growth during staged pruning. Specifically, we define
\begin{equation}
  H \triangleq C + \sigma,
\end{equation}
where $\sigma \ge 0$ is a fixed slack budget. In our implementation, $\sigma$ does \emph{not} change the pruning trigger; it caps the post-prune target length when combined with Max\_Drop (Sec.~\ref{subsec:maxdrop}).

\subsection{Max\_Drop (Staged Eviction)}
\label{subsec:maxdrop}
Periodic pruning can cause ``eviction cliffs'' when a large block is removed at once, which we observed as token-level NLL spikes on long-form generation. Max\_Drop limits the maximum number of tokens evicted in a single prune event.

Let $\delta \ge 0$ be the maximum drop size. When pruning is triggered, we set a target retained length
\begin{equation}
  L_t^\star \;=\;
  \begin{cases}
    C, & \delta = 0 \\
    \min\big(\max(L_t-\delta,\; C),\; H\big), & \delta > 0
  \end{cases}
\end{equation}
and prune the cache to keep exactly $L_t^\star$ tokens using the Start+Recent rule. This turns a single large eviction into multiple smaller evictions across steps while remaining within the hard capacity $H$.

\subsection{Algorithm Summary}
Algorithm~\ref{alg:kv} summarizes the pruning control logic (RoPE re-alignment and buffer management are omitted for clarity).
\begin{algorithm}[t]
\caption{KV pruning with Lazy Pruning, Slack, and Max\_Drop}
\label{alg:kv}
\begin{algorithmic}[1]
\REQUIRE sink size $S$, window size $W$, prune interval $R$, slack $\sigma$, max\_drop $\delta$
\STATE $C \leftarrow S+W$, $H \leftarrow C+\sigma$
\STATE observe cache length $L_t$
\IF{$R=0$} \STATE \textbf{return} \ENDIF
\IF{$L_t \le C$} \STATE \textbf{return} \ENDIF
\IF{$L_t - C < R$} \STATE \textbf{return} \ENDIF
\IF{$\delta = 0$}
  \STATE $L_t^\star \leftarrow C$
\ELSE
  \STATE $L_t^\star \leftarrow \min(\max(L_t-\delta,\; C),\; H)$
\ENDIF
\STATE keep first $S$ tokens and most recent $(L_t^\star - S)$ tokens
\STATE prune KV cache and re-align RoPE positions
\end{algorithmic}
\end{algorithm}

\section{Experiments}
\label{sec:exp}

\subsection{Setup}
We evaluate on Pythia-2.8B. We use WikiText-103 as a short-context sanity check and PG19 as the primary long-context benchmark. We report TTFT, TPOT (ms/token), throughput (tok/s), total runtime, peak GPU memory, and perplexity (PPL). We follow a fixed evaluation protocol and cap the effective context length for fair comparison across configurations.

\subsection{Main Results and Ablations}
We provide a one-click script (\texttt{run\_paper\_experiments.sh}) that runs all configurations and generates LaTeX tables automatically. If the tables are not generated yet, we show placeholders.
\IfFileExists{generated/tables.tex}{
  \input{generated/tables.tex}
}{
  \begin{table}[t]
  \centering
  \caption{Main results on PG19 (long-context).}
  \small
  \begin{tabular}{lcccc}
  \toprule
  Method & TPOT$\downarrow$ & Speedup$\uparrow$ & PPL$\downarrow$ & Peak Mem$\downarrow$ \\
  \midrule
  Baseline (Full KV) & \insertdata{TPOT} & 1.00$\times$ & \insertdata{PPL} & \insertdata{Mem} \\
  StreamingLLM (MIT) & \insertdata{TPOT} & \insertdata{Speedup} & \insertdata{PPL} & \insertdata{Mem} \\
  Ours (Lazy/Slack/Max\_Drop) & \insertdata{TPOT} & \insertdata{Speedup} & \insertdata{PPL} & \insertdata{Mem} \\
  \bottomrule
  \end{tabular}
  \end{table}
}

\paragraph{Ablations.}
\IfFileExists{generated/ablations.tex}{
  \input{generated/ablations.tex}
}{
  \begin{table}[t]
  \centering
  \caption{Ablation placeholders (generated by script).}
  \small
  \begin{tabular}{lccc}
  \toprule
  Setting & TPOT$\downarrow$ & Speedup$\uparrow$ & PPL$\downarrow$ \\
  \midrule
  Ours w/o Slack & \insertdata{TPOT} & \insertdata{Speedup} & \insertdata{PPL} \\
  Ours w/o Max\_Drop & \insertdata{TPOT} & \insertdata{Speedup} & \insertdata{PPL} \\
  Ours (full) & \insertdata{TPOT} & \insertdata{Speedup} & \insertdata{PPL} \\
  \bottomrule
  \end{tabular}
  \end{table}
}

\section{Discussion}
\label{sec:negative}
\paragraph{Why do popular inference accelerations fail in batch-1 streaming?}
In our setting, streaming bounds attention length, shifting the bottleneck toward MLP and framework/launch overhead. Therefore, further optimizing attention kernels can have limited impact. Moreover, methods that assume static shapes or stable cache semantics may be incompatible with pruning and RoPE re-alignment in streaming decode.

\paragraph{Attempts and analysis of ineffective methods.}
We tested several common acceleration routes (see our project log in \texttt{docs/}): (i) FlashAttention integration was non-trivial under RoPE re-alignment and did not improve batch-1 TPOT in our environment; (ii) speculative decoding was unreliable on long-form generation and incompatible with streaming cache management; (iii) quantization via TorchAO exhibited numerical issues (INT8 v1) and remained slower even when stabilized (INT8 v2) in batch-1 streaming decode; (iv) \texttt{torch.compile} with CUDA Graphs was unstable due to overwritten outputs in rotary-embedding paths; (v) HuggingFace StaticCache conflicted with pruning updates and triggered device-side asserts. These results highlight that strong batch-inference accelerations do not directly transfer to batch-1 streaming decode.

\IfFileExists{generated/negative_results.tex}{
  \input{generated/negative_results.tex}
}{}

\section{Conclusion}
We presented three modular KV-cache management mechanisms for StreamingLLM: Lazy Pruning, Slack, and Max\_Drop. Our method is training-free and targets long-context decoding where pruning overhead and eviction-induced quality loss become important. We provide a reproducible pipeline and report negative results to clarify which acceleration techniques do or do not apply in this regime.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
