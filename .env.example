### Environment variables for reproducible runs

# Hugging Face caches
HF_HOME=/path/to/.cache/huggingface
HF_DATASETS_CACHE=$HF_HOME/datasets
HF_HUB_OFFLINE=1
TRANSFORMERS_OFFLINE=1

MODEL_NAME=EleutherAI/pythia-2.8b

# Override Python interpreter (optional)
PYTHON_BIN=kvpress/.venv/bin/python

# Dataset-specific defaults (used by run_kvpress_streaming_decode.sh)
DATASET_NAME=wikitext
DATASET_CONFIG=wikitext-103-v1
SPLIT=test
TEXT_COLUMN=text

# Experiment configs
N_SINK=4
WINDOW_SIZE=2048


# Dataset-specific sampling controls
WIKITEXT_MAX_SAMPLES=128
PG19_MAX_SAMPLES=1

# Dataset-specific token windows used by run_comprehensive_comparisons.sh
WIKITEXT_MAX_TOKENS=4096
PG19_20K_MAX_TOKENS=20000
PG19_MAX_TOKENS=20000

# PG19 sample selector (recommended)
# For stable PPL across scripts/machines, pin to a single long PG19 book and
# control the evaluated length via `PG19_20K_MAX_TOKENS` / `--max-eval-tokens`.
PG19_SAMPLE_LENGTH=50000
PG19_SAMPLE_FILE=data/pg19/long_context_50000.json

# WikiText sample selector
WIKITEXT_SAMPLE_LENGTH=4096
WIKITEXT_SAMPLE_FILE=data/wikitext/long_context_4096.json

# Decode-loop controls (run_everything.sh / experiments rely on these)
MAX_LENGTH=2048
STRIDE=1024
