################################################################################
# StreamingLLM å®Œæ•´å®éªŒ
# å¼€å§‹æ—¶é—´: Thu Dec  4 05:07:55 PM CST 2025
################################################################################

[1;33m=== é˜¶æ®µ 1: Baseline å®éªŒ ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 1: WikiText-103 Baseline[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_streaming_llm.py         --dataset-name wikitext         --dataset-config wikitext-103-v1         --max-samples 64         --max-eval-tokens 4096         --n-sink 0         --window-size 999999         --output results/streaming_llm/wikitext_baseline.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:07:56 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: wikitext:wikitext-103-v1
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
n_sink: 0
window_size: 999999
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ•°æ®é›†å¤§å°: 3407 tokens
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼°åŸºçº¿ (æ— å‹ç¼©)
============================================================
åŸºçº¿ PPL: 40.31
åŸºçº¿ Runtime: 0.401s
åŸºçº¿ Prefill: 0.401s

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
StreamingLLM PPL: 40.31
StreamingLLM Runtime: 0.032s
StreamingLLM Prefill: 0.032s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "wikitext:wikitext-103-v1",
  "split": "test",
  "max_length": 1024,
  "stride": 512,
  "max_samples": 64,
  "max_eval_tokens": 4096,
  "total_tokens": 3407,
  "streaming_llm": {
    "n_sink": 0,
    "window_size": 999999,
    "max_cache_size": 999999
  },
  "baseline": {
    "perplexity": 40.312744140625,
    "runtime_sec": 0.4012642939342186,
    "prefill_sec": 0.40126241801772267
  },
  "streaming": {
    "perplexity": 40.312744140625,
    "runtime_sec": 0.032307264977134764,
    "prefill_sec": 0.03230652795173228
  },
  "metrics": {
    "speedup": 12.420249569816898,
    "compression_ratio": 0.0,
    "ppl_increase_percent": 0.0,
    "peak_memory_mb": 759.7294921875
  },
  "device": "cuda",
  "dtype": "torch.float16"
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/wikitext_baseline.json

============================================================
æ€»ç»“
============================================================
åŠ é€Ÿæ¯”: 12.42x
å‹ç¼©æ¯”: 0.00%
PPL å¢åŠ : 0.00%
å³°å€¼æ˜¾å­˜: 759.7 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 14s)


[0;34m========================================[0m
[0;34må®éªŒ 2: PG19 Baseline[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_streaming_llm.py         --dataset-name pg19         --max-samples 1         --max-eval-tokens 4096         --n-sink 0         --window-size 999999         --output results/streaming_llm/pg19_baseline.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:08:10 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: pg19:wikitext-103-v1
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
n_sink: 0
window_size: 999999
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ£€æµ‹åˆ° PG19 æ•°æ®é›†...
âœ“ ä½¿ç”¨æœ¬åœ°ç¼“å­˜: data/pg19/sample.json
  å·²åŠ è½½ç¼“å­˜æ•°æ® (é•¿åº¦: 318194 å­—ç¬¦)
æ•°æ®é›†å¤§å°: 4096 tokens
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼°åŸºçº¿ (æ— å‹ç¼©)
============================================================
åŸºçº¿ PPL: 57.92
åŸºçº¿ Runtime: 0.326s
åŸºçº¿ Prefill: 0.326s

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
StreamingLLM PPL: 57.92
StreamingLLM Runtime: 0.037s
StreamingLLM Prefill: 0.037s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "pg19:wikitext-103-v1",
  "split": "test",
  "max_length": 1024,
  "stride": 512,
  "max_samples": 1,
  "max_eval_tokens": 4096,
  "total_tokens": 4096,
  "streaming_llm": {
    "n_sink": 0,
    "window_size": 999999,
    "max_cache_size": 999999
  },
  "baseline": {
    "perplexity": 57.91722106933594,
    "runtime_sec": 0.3256916319951415,
    "prefill_sec": 0.3256900170817971
  },
  "streaming": {
    "perplexity": 57.91722106933594,
    "runtime_sec": 0.03673226595856249,
    "prefill_sec": 0.03673176106531173
  },
  "metrics": {
    "speedup": 8.866636007769104,
    "compression_ratio": 0.0,
    "ppl_increase_percent": 0.0,
    "peak_memory_mb": 759.7294921875
  },
  "device": "cuda",
  "dtype": "torch.float16"
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/pg19_baseline.json

============================================================
æ€»ç»“
============================================================
åŠ é€Ÿæ¯”: 8.87x
å‹ç¼©æ¯”: 0.00%
PPL å¢åŠ : 0.00%
å³°å€¼æ˜¾å­˜: 759.7 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 8s)

[1;33m=== é˜¶æ®µ 2: æˆ‘ä»¬çš„ StreamingLLM å®éªŒ ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 3: WikiText-103 StreamingLLM[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_streaming_llm.py         --dataset-name wikitext         --dataset-config wikitext-103-v1         --max-samples 64         --max-eval-tokens 4096         --n-sink 4         --window-size 1024         --output results/streaming_llm/wikitext_result.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:08:18 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: wikitext:wikitext-103-v1
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
n_sink: 4
window_size: 1024
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ•°æ®é›†å¤§å°: 3407 tokens
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼°åŸºçº¿ (æ— å‹ç¼©)
============================================================
åŸºçº¿ PPL: 40.31
åŸºçº¿ Runtime: 0.333s
åŸºçº¿ Prefill: 0.333s

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
StreamingLLM PPL: 40.31
StreamingLLM Runtime: 0.032s
StreamingLLM Prefill: 0.032s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "wikitext:wikitext-103-v1",
  "split": "test",
  "max_length": 1024,
  "stride": 512,
  "max_samples": 64,
  "max_eval_tokens": 4096,
  "total_tokens": 3407,
  "streaming_llm": {
    "n_sink": 4,
    "window_size": 1024,
    "max_cache_size": 1028
  },
  "baseline": {
    "perplexity": 40.312744140625,
    "runtime_sec": 0.33310132392216474,
    "prefill_sec": 0.33309981401544064
  },
  "streaming": {
    "perplexity": 40.312744140625,
    "runtime_sec": 0.032463917043060064,
    "prefill_sec": 0.03246327897068113
  },
  "metrics": {
    "speedup": 10.260663353727152,
    "compression_ratio": 0.6982682712063399,
    "ppl_increase_percent": 0.0,
    "peak_memory_mb": 759.7294921875
  },
  "device": "cuda",
  "dtype": "torch.float16"
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/wikitext_result.json

============================================================
æ€»ç»“
============================================================
åŠ é€Ÿæ¯”: 10.26x
å‹ç¼©æ¯”: 69.83%
PPL å¢åŠ : 0.00%
å³°å€¼æ˜¾å­˜: 759.7 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 14s)


[0;34m========================================[0m
[0;34må®éªŒ 4: PG19 StreamingLLM[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_streaming_llm.py         --dataset-name pg19         --max-samples 1         --max-eval-tokens 4096         --n-sink 4         --window-size 1024         --output results/streaming_llm/pg19_result.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:08:32 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: pg19:wikitext-103-v1
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
n_sink: 4
window_size: 1024
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ£€æµ‹åˆ° PG19 æ•°æ®é›†...
âœ“ ä½¿ç”¨æœ¬åœ°ç¼“å­˜: data/pg19/sample.json
  å·²åŠ è½½ç¼“å­˜æ•°æ® (é•¿åº¦: 318194 å­—ç¬¦)
æ•°æ®é›†å¤§å°: 4096 tokens
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼°åŸºçº¿ (æ— å‹ç¼©)
============================================================
åŸºçº¿ PPL: 57.92
åŸºçº¿ Runtime: 0.856s
åŸºçº¿ Prefill: 0.856s

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
StreamingLLM PPL: 57.92
StreamingLLM Runtime: 0.079s
StreamingLLM Prefill: 0.079s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "pg19:wikitext-103-v1",
  "split": "test",
  "max_length": 1024,
  "stride": 512,
  "max_samples": 1,
  "max_eval_tokens": 4096,
  "total_tokens": 4096,
  "streaming_llm": {
    "n_sink": 4,
    "window_size": 1024,
    "max_cache_size": 1028
  },
  "baseline": {
    "perplexity": 57.91722106933594,
    "runtime_sec": 0.8558688020566478,
    "prefill_sec": 0.8558671630453318
  },
  "streaming": {
    "perplexity": 57.91722106933594,
    "runtime_sec": 0.07943330600392073,
    "prefill_sec": 0.07943258399609476
  },
  "metrics": {
    "speedup": 10.774684387609438,
    "compression_ratio": 0.7490234375,
    "ppl_increase_percent": 0.0,
    "peak_memory_mb": 759.7294921875
  },
  "device": "cuda",
  "dtype": "torch.float16"
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/pg19_result.json

============================================================
æ€»ç»“
============================================================
åŠ é€Ÿæ¯”: 10.77x
å‹ç¼©æ¯”: 74.90%
PPL å¢åŠ : 0.00%
å³°å€¼æ˜¾å­˜: 759.7 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 9s)

[1;33m=== é˜¶æ®µ 3: kvpress å®˜æ–¹åº“å¯¹æ¯”å®éªŒ ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 5: WikiText-103 kvpress StreamingLLM[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_kvpress.py         --dataset-name wikitext         --dataset-config wikitext-103-v1         --max-samples 64         --max-eval-tokens 4096         --n-sink 4         --window-size 1024         --output results/kvpress/wikitext_result.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:08:41 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
KVPress StreamingLLM Evaluation
============================================================
Model: EleutherAI/pythia-70m
Dataset: wikitext:wikitext-103-v1
Device: cuda
Data type: torch.float16
n_sink: 4
window_size: 1024
============================================================

Loading tokenizer...
Loading dataset...
Dataset size: 3407 tokens
Loading model...

============================================================
Evaluating Baseline (no compression)
============================================================
Baseline PPL: 40.31
Baseline Runtime: 0.326s
Baseline Prefill: 0.326s

Calculated compression_ratio: 0.6983
(Keeping 1028 tokens out of 3407)

============================================================
Evaluating kvpress StreamingLLM
============================================================
kvpress StreamingLLM PPL: 40.31
kvpress StreamingLLM Runtime: 0.086s
kvpress StreamingLLM Prefill: 0.086s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "wikitext:wikitext-103-v1",
  "split": "test",
  "max_length": 1024,
  "stride": 512,
  "max_samples": 64,
  "max_eval_tokens": 4096,
  "total_tokens": 3407,
  "streaming_llm": {
    "n_sink": 4,
    "window_size": 1024,
    "max_cache_size": 1028,
    "compression_ratio": 0.6982682712063399
  },
  "baseline": {
    "perplexity": 40.312744140625,
    "runtime_sec": 0.326349479961209,
    "prefill_sec": 0.32634981989394873
  },
  "kvpress": {
    "perplexity": 40.312744140625,
    "runtime_sec": 0.08551559608895332,
    "prefill_sec": 0.0855160120408982
  },
  "metrics": {
    "speedup": 3.8162568570736535,
    "compression_ratio": 0.6982682712063399,
    "ppl_increase_percent": 0.0,
    "peak_memory_mb": 759.7294921875
  },
  "device": "cuda",
  "dtype": "torch.float16"
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/kvpress/wikitext_result.json

============================================================
Summary
============================================================
Speedup: 3.82x
Compression ratio: 69.83%
PPL increase: 0.00%
Peak memory: 759.7 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 14s)


[0;34m========================================[0m
[0;34må®éªŒ 6: PG19 kvpress StreamingLLM[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_kvpress.py         --dataset-name pg19         --max-samples 1         --max-eval-tokens 4096         --n-sink 4         --window-size 1024         --output results/kvpress/pg19_result.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:08:55 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
KVPress StreamingLLM Evaluation
============================================================
Model: EleutherAI/pythia-70m
Dataset: pg19:wikitext-103-v1
Device: cuda
Data type: torch.float16
n_sink: 4
window_size: 1024
============================================================

Loading tokenizer...
Loading dataset...
æ£€æµ‹åˆ° PG19 æ•°æ®é›†...
âœ“ ä½¿ç”¨æœ¬åœ°ç¼“å­˜: data/pg19/sample.json
  å·²åŠ è½½ç¼“å­˜æ•°æ® (é•¿åº¦: 318194 å­—ç¬¦)
Dataset size: 4096 tokens
Loading model...

============================================================
Evaluating Baseline (no compression)
============================================================
Baseline PPL: 57.92
Baseline Runtime: 0.325s
Baseline Prefill: 0.325s

Calculated compression_ratio: 0.7490
(Keeping 1028 tokens out of 4096)

============================================================
Evaluating kvpress StreamingLLM
============================================================
kvpress StreamingLLM PPL: 57.92
kvpress StreamingLLM Runtime: 0.090s
kvpress StreamingLLM Prefill: 0.090s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "pg19:wikitext-103-v1",
  "split": "test",
  "max_length": 1024,
  "stride": 512,
  "max_samples": 1,
  "max_eval_tokens": 4096,
  "total_tokens": 4096,
  "streaming_llm": {
    "n_sink": 4,
    "window_size": 1024,
    "max_cache_size": 1028,
    "compression_ratio": 0.7490234375
  },
  "baseline": {
    "perplexity": 57.91722106933594,
    "runtime_sec": 0.32510856993030757,
    "prefill_sec": 0.3251090870471671
  },
  "kvpress": {
    "perplexity": 57.91722106933594,
    "runtime_sec": 0.08980486402288079,
    "prefill_sec": 0.08980522502679378
  },
  "metrics": {
    "speedup": 3.620166607540047,
    "compression_ratio": 0.7490234375,
    "ppl_increase_percent": 0.0,
    "peak_memory_mb": 759.7294921875
  },
  "device": "cuda",
  "dtype": "torch.float16"
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/kvpress/pg19_result.json

============================================================
Summary
============================================================
Speedup: 3.62x
Compression ratio: 74.90%
PPL increase: 0.00%
Peak memory: 759.7 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 9s)

[1;33m=== é˜¶æ®µ 4: æ¶ˆèå®éªŒ ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 7: Window Size æ¶ˆèå®éªŒ[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/ablation_study.py         --dataset-name wikitext         --dataset-config wikitext-103-v1         --max-samples 64         --max-eval-tokens 4096         --ablation-type window_size         --output results/streaming_llm/ablation_window_size.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:09:04 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM æ¶ˆèå®éªŒ
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: wikitext:wikitext-103-v1
æ¶ˆèç±»å‹: window_size
è®¾å¤‡: cuda
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ•°æ®é›†å¤§å°: 3407 tokens
åŠ è½½æ¨¡å‹...

============================================================
æ¶ˆèå®éªŒ: Window Size (n_sink=4)
============================================================

æµ‹è¯• window_size=128...
  PPL: 40.31, Runtime: 0.334s, Compression: 96.13%
æµ‹è¯• window_size=256...
  PPL: 40.31, Runtime: 0.032s, Compression: 92.37%
æµ‹è¯• window_size=512...
  PPL: 40.31, Runtime: 0.032s, Compression: 84.85%
æµ‹è¯• window_size=1024...
  PPL: 40.31, Runtime: 0.032s, Compression: 69.83%
æµ‹è¯• window_size=2048...
  PPL: 40.31, Runtime: 0.034s, Compression: 39.77%
æµ‹è¯• window_size=4096...
  PPL: 40.31, Runtime: 0.032s, Compression: 0.00%

============================================================
æ¶ˆèå®éªŒå®Œæˆ
============================================================
ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/ablation_window_size.json
============================================================

ç»“æœæ€»ç»“:
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "wikitext:wikitext-103-v1",
  "ablation_type": "window_size",
  "total_tokens": 3407,
  "results": [
    {
      "window_size": 128,
      "n_sink": 4,
      "max_cache_size": 132,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.3339552920078859,
      "prefill_sec": 0.33395401504822075,
      "compression_ratio": 0.9612562371587907
    },
    {
      "window_size": 256,
      "n_sink": 4,
      "max_cache_size": 260,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.03186139604076743,
      "prefill_sec": 0.031860849005170166,
      "compression_ratio": 0.923686527737012
    },
    {
      "window_size": 512,
      "n_sink": 4,
      "max_cache_size": 516,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.03192267497070134,
      "prefill_sec": 0.03192229801788926,
      "compression_ratio": 0.8485471088934546
    },
    {
      "window_size": 1024,
      "n_sink": 4,
      "max_cache_size": 1028,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.03188651497475803,
      "prefill_sec": 0.031886153970845044,
      "compression_ratio": 0.6982682712063399
    },
    {
      "window_size": 2048,
      "n_sink": 4,
      "max_cache_size": 2052,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.034115592017769814,
      "prefill_sec": 0.03411508596036583,
      "compression_ratio": 0.3977105958321103
    },
    {
      "window_size": 4096,
      "n_sink": 4,
      "max_cache_size": 4100,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.032038429053500295,
      "prefill_sec": 0.032038090052083135,
      "compression_ratio": 0.0
    }
  ]
}
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 14s)


[0;34m========================================[0m
[0;34må®éªŒ 8: N_sink æ¶ˆèå®éªŒ[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/ablation_study.py         --dataset-name wikitext         --dataset-config wikitext-103-v1         --max-samples 64         --max-eval-tokens 4096         --ablation-type n_sink         --output results/streaming_llm/ablation_n_sink.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:09:18 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM æ¶ˆèå®éªŒ
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: wikitext:wikitext-103-v1
æ¶ˆèç±»å‹: n_sink
è®¾å¤‡: cuda
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ•°æ®é›†å¤§å°: 3407 tokens
åŠ è½½æ¨¡å‹...

============================================================
æ¶ˆèå®éªŒ: N_sink (window_size=1024)
============================================================

æµ‹è¯• n_sink=0...
  PPL: 40.31, Runtime: 0.337s, Compression: 69.94%
æµ‹è¯• n_sink=1...
  PPL: 40.31, Runtime: 0.033s, Compression: 69.91%
æµ‹è¯• n_sink=2...
  PPL: 40.31, Runtime: 0.032s, Compression: 69.89%
æµ‹è¯• n_sink=4...
  PPL: 40.31, Runtime: 0.033s, Compression: 69.83%
æµ‹è¯• n_sink=8...
  PPL: 40.31, Runtime: 0.034s, Compression: 69.71%
æµ‹è¯• n_sink=16...
  PPL: 40.31, Runtime: 0.033s, Compression: 69.47%

============================================================
æ¶ˆèå®éªŒå®Œæˆ
============================================================
ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/ablation_n_sink.json
============================================================

ç»“æœæ€»ç»“:
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "wikitext:wikitext-103-v1",
  "ablation_type": "n_sink",
  "total_tokens": 3407,
  "results": [
    {
      "n_sink": 0,
      "window_size": 1024,
      "max_cache_size": 1024,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.3371211759513244,
      "prefill_sec": 0.33712005510460585,
      "compression_ratio": 0.6994423246257705
    },
    {
      "n_sink": 1,
      "window_size": 1024,
      "max_cache_size": 1025,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.03283859905786812,
      "prefill_sec": 0.03283797192852944,
      "compression_ratio": 0.6991488112709128
    },
    {
      "n_sink": 2,
      "window_size": 1024,
      "max_cache_size": 1026,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.03249257896095514,
      "prefill_sec": 0.03249213902745396,
      "compression_ratio": 0.6988552979160552
    },
    {
      "n_sink": 4,
      "window_size": 1024,
      "max_cache_size": 1028,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.03320162103045732,
      "prefill_sec": 0.03320104593876749,
      "compression_ratio": 0.6982682712063399
    },
    {
      "n_sink": 8,
      "window_size": 1024,
      "max_cache_size": 1032,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.03404278901871294,
      "prefill_sec": 0.03404233406763524,
      "compression_ratio": 0.6970942177869093
    },
    {
      "n_sink": 16,
      "window_size": 1024,
      "max_cache_size": 1040,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.03320200392045081,
      "prefill_sec": 0.0332015739986673,
      "compression_ratio": 0.6947461109480482
    }
  ]
}
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 14s)

[1;33m=== é˜¶æ®µ 5: ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 9: ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/generate_final_figures.py
å¼€å§‹æ—¶é—´: Thu Dec  4 05:09:32 PM CST 2025
============================================================
StreamingLLM å®éªŒç»“æœå¯è§†åŒ–
============================================================
âœ“ å›¾è¡¨ä¿å­˜ç›®å½•: results/figures

ç”Ÿæˆä¸»å®éªŒå¯¹æ¯”å›¾...
âœ“ å·²ä¿å­˜: results/figures/main_comparison.png

ç”ŸæˆWindow Sizeæ¶ˆèå›¾...
âœ“ å·²ä¿å­˜: results/figures/ablation_window_size.png

ç”ŸæˆN_sinkæ¶ˆèå›¾...
âœ“ å·²ä¿å­˜: results/figures/ablation_n_sink.png

ç”Ÿæˆç»¼åˆç»“æœè¡¨æ ¼...
âœ“ å·²ä¿å­˜: results/figures/results_summary.png

============================================================
âœ“ æ‰€æœ‰å›¾è¡¨ç”Ÿæˆå®Œæˆ!
âœ“ ä¿å­˜ä½ç½®: results/figures
============================================================

ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶:
  - ablation_n_sink.png (0.36 MB)
  - ablation_window_size.png (0.33 MB)
  - main_comparison.png (0.31 MB)
  - results_summary.png (0.23 MB)
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 4s)


################################################################################
# å®éªŒå®ŒæˆæŠ¥å‘Š
################################################################################

ç»“æŸæ—¶é—´: Thu Dec  4 05:09:36 PM CST 2025
æ€»è€—æ—¶: 101s (1åˆ†é’Ÿ)

å®éªŒç»Ÿè®¡:
  æ€»å®éªŒæ•°: 9
  [0;32mæˆåŠŸ: 9[0m
  [0;31må¤±è´¥: 0[0m

è¯¦ç»†ç»“æœ:
----------------------------------------
  [0;32mâœ“[0m WikiText-103 Baseline (14s)
  [0;32mâœ“[0m PG19 Baseline (8s)
  [0;32mâœ“[0m WikiText-103 StreamingLLM (14s)
  [0;32mâœ“[0m PG19 StreamingLLM (9s)
  [0;32mâœ“[0m WikiText-103 kvpress StreamingLLM (14s)
  [0;32mâœ“[0m PG19 kvpress StreamingLLM (9s)
  [0;32mâœ“[0m Window Size æ¶ˆèå®éªŒ (14s)
  [0;32mâœ“[0m N_sink æ¶ˆèå®éªŒ (14s)
  [0;32mâœ“[0m ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ (4s)
----------------------------------------

æ—¥å¿—æ–‡ä»¶: results/experiment_log_20251204_170755.txt
æ€»ç»“æ–‡ä»¶: results/experiment_summary_20251204_170755.txt

[0;32mâœ“ æ‰€æœ‰å®éªŒæˆåŠŸå®Œæˆ![0m
