################################################################################
# Decoding Latency å®éªŒ
# å¼€å§‹æ—¶é—´: Thu Dec  4 05:10:50 PM CST 2025
################################################################################

[1;33m=== Decoding Latency å®éªŒ ===[0m
æµ‹é‡é…ç½®:
  - Prompt Length: 512
  - Num Tokens: 2000
  - Warmup Tokens: 200
  - Num Runs: 3


[0;34m========================================[0m
[0;34må®éªŒ 1: Decoding Latency (Cache=512)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 512         --n-sink 4         --prompt-length 512         --num-tokens 2000         --warmup-tokens 200         --num-runs 3         --output results/decoding_latency_512.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:10:50 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 512
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° Baseline (Full KV Cache)
============================================================

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 3.026 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 2.976 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 3.667 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.223 Â± 1.621 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.985 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 512

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 3.071 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 3.054 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 3.072 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.066 Â± 0.079 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.055 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      3.223 Â± 1.621 ms/token
StreamingLLM:  3.066 Â± 0.079 ms/token
åŠ é€Ÿæ¯”:        1.05x
æ˜¾å­˜å‡å°‘:      0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding_latency_512.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 50s)


[0;34m========================================[0m
[0;34må®éªŒ 2: Decoding Latency (Cache=1024)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 1024         --n-sink 4         --prompt-length 512         --num-tokens 2000         --warmup-tokens 200         --num-runs 3         --output results/decoding_latency_1024.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:11:40 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 1024
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° Baseline (Full KV Cache)
============================================================

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 2.924 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 2.920 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 2.914 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 2.919 Â± 0.079 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.913 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 1024

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 2.959 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 3.858 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 2.965 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.260 Â± 1.820 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.961 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      2.919 Â± 0.079 ms/token
StreamingLLM:  3.260 Â± 1.820 ms/token
åŠ é€Ÿæ¯”:        0.90x
æ˜¾å­˜å‡å°‘:      0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding_latency_1024.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 49s)


[0;34m========================================[0m
[0;34må®éªŒ 3: Decoding Latency (Cache=2048)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 2048         --n-sink 4         --prompt-length 512         --num-tokens 2000         --warmup-tokens 200         --num-runs 3         --output results/decoding_latency_2048.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:12:29 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 2048
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° Baseline (Full KV Cache)
============================================================

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 2.952 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 5.979 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 2.956 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.962 Â± 3.286 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.953 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 2048

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 2.988 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 2.977 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: 2.972 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 2.979 Â± 0.031 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.974 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      3.962 Â± 3.286 ms/token
StreamingLLM:  2.979 Â± 0.031 ms/token
åŠ é€Ÿæ¯”:        1.33x
æ˜¾å­˜å‡å°‘:      0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding_latency_2048.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 48s)


[0;34m========================================[0m
[0;34må®éªŒ 4: Decoding Latency (Cache=4096)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 4096         --n-sink 4         --prompt-length 512         --num-tokens 2000         --warmup-tokens 200         --num-runs 3         --output results/decoding_latency_4096.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:13:17 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!
/data2/jflin/CS3602/kvpress/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/data2/jflin/CS3602/kvpress/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
/data2/jflin/CS3602/kvpress/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:222: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/data2/jflin/CS3602/kvpress/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:180: RuntimeWarning: invalid value encountered in divide
  arrmean = um.true_divide(arrmean, div, out=arrmean,
/data2/jflin/CS3602/kvpress/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:214: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 4096
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° Baseline (Full KV Cache)
============================================================

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: nan ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: nan ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: nan ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: nan Â± nan ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: nan ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 4096

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: nan ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: nan ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  å¹³å‡å»¶è¿Ÿ: nan ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: nan Â± nan ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: nan ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      nan Â± nan ms/token
StreamingLLM:  nan Â± nan ms/token
åŠ é€Ÿæ¯”:        nanx
æ˜¾å­˜å‡å°‘:      0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding_latency_4096.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 49s)

[1;33m=== é•¿åºåˆ— Decoding Latency æµ‹è¯• ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 5: Long Sequence Decoding (5000 tokens, Cache=1024)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 1024         --n-sink 4         --prompt-length 512         --num-tokens 5000         --warmup-tokens 200         --num-runs 3         --output results/decoding_latency_long_5000.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:14:06 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!
