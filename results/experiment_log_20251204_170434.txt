################################################################################
# StreamingLLM å®Œæ•´å®éªŒ
# å¼€å§‹æ—¶é—´: Thu Dec  4 05:04:34 PM CST 2025
################################################################################

[1;33m=== é˜¶æ®µ 1: Baseline å®éªŒ ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 1: WikiText-103 Baseline[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_streaming_llm.py         --dataset-name wikitext         --dataset-config wikitext-103-v1         --max-samples 64         --max-eval-tokens 4096         --n-sink 0         --window-size 999999         --output results/streaming_llm/wikitext_baseline.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:04:34 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: wikitext:wikitext-103-v1
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
n_sink: 0
window_size: 999999
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ•°æ®é›†å¤§å°: 3407 tokens
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼°åŸºçº¿ (æ— å‹ç¼©)
============================================================
åŸºçº¿ PPL: 40.31
åŸºçº¿ Runtime: 0.328s
åŸºçº¿ Prefill: 0.328s

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
StreamingLLM PPL: 40.31
StreamingLLM Runtime: 0.033s
StreamingLLM Prefill: 0.033s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "wikitext:wikitext-103-v1",
  "split": "test",
  "max_length": 1024,
  "stride": 512,
  "max_samples": 64,
  "max_eval_tokens": 4096,
  "total_tokens": 3407,
  "streaming_llm": {
    "n_sink": 0,
    "window_size": 999999,
    "max_cache_size": 999999
  },
  "baseline": {
    "perplexity": 40.312744140625,
    "runtime_sec": 0.32754841807764024,
    "prefill_sec": 0.3275470120133832
  },
  "streaming": {
    "perplexity": 40.312744140625,
    "runtime_sec": 0.03286860603839159,
    "prefill_sec": 0.032867830945178866
  },
  "metrics": {
    "speedup": 9.965388179074377,
    "compression_ratio": 0.0,
    "ppl_increase_percent": 0.0,
    "peak_memory_mb": 759.7294921875
  },
  "device": "cuda",
  "dtype": "torch.float16"
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/wikitext_baseline.json

============================================================
æ€»ç»“
============================================================
åŠ é€Ÿæ¯”: 9.97x
å‹ç¼©æ¯”: 0.00%
PPL å¢åŠ : 0.00%
å³°å€¼æ˜¾å­˜: 759.7 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 15s)


[0;34m========================================[0m
[0;34må®éªŒ 2: PG19 Baseline[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_streaming_llm.py         --dataset-name pg19         --max-samples 1         --max-eval-tokens 4096         --n-sink 0         --window-size 999999         --output results/streaming_llm/pg19_baseline.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:04:49 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: pg19:wikitext-103-v1
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
n_sink: 0
window_size: 999999
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ£€æµ‹åˆ° PG19 æ•°æ®é›†...
æœ¬åœ°ç¼“å­˜ä¸å­˜åœ¨ï¼Œå¼€å§‹æµå¼ä¸‹è½½ PG19 æ•°æ®é›†...
æ³¨æ„: åªä¸‹è½½ä¸€æ¡æ ·æœ¬ä»¥èŠ‚çœç©ºé—´å’Œæ—¶é—´
æ­£åœ¨æµå¼åŠ è½½å‰ 10 æ¡æ ·æœ¬...
  å·²åŠ è½½ 1/10 æ¡  å·²åŠ è½½ 2/10 æ¡  å·²åŠ è½½ 3/10 æ¡  å·²åŠ è½½ 4/10 æ¡  å·²åŠ è½½ 5/10 æ¡  å·²åŠ è½½ 6/10 æ¡  å·²åŠ è½½ 7/10 æ¡  å·²åŠ è½½ 8/10 æ¡  å·²åŠ è½½ 9/10 æ¡  å·²åŠ è½½ 10/10 æ¡
âœ“ å·²ä»å‰ 10 æ¡ä¸­éšæœºé€‰æ‹© 1 æ¡æ ·æœ¬ (ç§å­=42)
  æ ·æœ¬é•¿åº¦: 318194 å­—ç¬¦
âœ“ å·²ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜: data/pg19/sample.json
  åç»­è¿è¡Œå°†ç›´æ¥ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œæ— éœ€é‡æ–°ä¸‹è½½
æ•°æ®é›†å¤§å°: 4096 tokens
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼°åŸºçº¿ (æ— å‹ç¼©)
============================================================
åŸºçº¿ PPL: 57.92
åŸºçº¿ Runtime: 0.315s
åŸºçº¿ Prefill: 0.315s

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
StreamingLLM PPL: 57.92
StreamingLLM Runtime: 0.037s
StreamingLLM Prefill: 0.037s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "pg19:wikitext-103-v1",
  "split": "test",
  "max_length": 1024,
  "stride": 512,
  "max_samples": 1,
  "max_eval_tokens": 4096,
  "total_tokens": 4096,
  "streaming_llm": {
    "n_sink": 0,
    "window_size": 999999,
    "max_cache_size": 999999
  },
  "baseline": {
    "perplexity": 57.91722106933594,
    "runtime_sec": 0.31510235904715955,
    "prefill_sec": 0.31510055204853415
  },
  "streaming": {
    "perplexity": 57.91722106933594,
    "runtime_sec": 0.036574169993400574,
    "prefill_sec": 0.036573481047526
  },
  "metrics": {
    "speedup": 8.61543431071755,
    "compression_ratio": 0.0,
    "ppl_increase_percent": 0.0,
    "peak_memory_mb": 759.7294921875
  },
  "device": "cuda",
  "dtype": "torch.float16"
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/pg19_baseline.json

============================================================
æ€»ç»“
============================================================
åŠ é€Ÿæ¯”: 8.62x
å‹ç¼©æ¯”: 0.00%
PPL å¢åŠ : 0.00%
å³°å€¼æ˜¾å­˜: 759.7 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 47s)

[1;33m=== é˜¶æ®µ 2: æˆ‘ä»¬çš„ StreamingLLM å®éªŒ ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 3: WikiText-103 StreamingLLM[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_streaming_llm.py         --dataset-name wikitext         --dataset-config wikitext-103-v1         --max-samples 64         --max-eval-tokens 4096         --n-sink 4         --window-size 1024         --output results/streaming_llm/wikitext_result.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:05:36 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: wikitext:wikitext-103-v1
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
n_sink: 4
window_size: 1024
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ•°æ®é›†å¤§å°: 3407 tokens
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼°åŸºçº¿ (æ— å‹ç¼©)
============================================================
åŸºçº¿ PPL: 40.31
åŸºçº¿ Runtime: 0.328s
åŸºçº¿ Prefill: 0.328s

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
StreamingLLM PPL: 40.31
StreamingLLM Runtime: 0.032s
StreamingLLM Prefill: 0.032s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "wikitext:wikitext-103-v1",
  "split": "test",
  "max_length": 1024,
  "stride": 512,
  "max_samples": 64,
  "max_eval_tokens": 4096,
  "total_tokens": 3407,
  "streaming_llm": {
    "n_sink": 4,
    "window_size": 1024,
    "max_cache_size": 1028
  },
  "baseline": {
    "perplexity": 40.312744140625,
    "runtime_sec": 0.32750740996561944,
    "prefill_sec": 0.3275061040185392
  },
  "streaming": {
    "perplexity": 40.312744140625,
    "runtime_sec": 0.03157408395782113,
    "prefill_sec": 0.03157354996073991
  },
  "metrics": {
    "speedup": 10.3726654557303,
    "compression_ratio": 0.6982682712063399,
    "ppl_increase_percent": 0.0,
    "peak_memory_mb": 759.7294921875
  },
  "device": "cuda",
  "dtype": "torch.float16"
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/wikitext_result.json

============================================================
æ€»ç»“
============================================================
åŠ é€Ÿæ¯”: 10.37x
å‹ç¼©æ¯”: 69.83%
PPL å¢åŠ : 0.00%
å³°å€¼æ˜¾å­˜: 759.7 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 19s)


[0;34m========================================[0m
[0;34må®éªŒ 4: PG19 StreamingLLM[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_streaming_llm.py         --dataset-name pg19         --max-samples 1         --max-eval-tokens 4096         --n-sink 4         --window-size 1024         --output results/streaming_llm/pg19_result.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:05:55 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: pg19:wikitext-103-v1
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
n_sink: 4
window_size: 1024
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ£€æµ‹åˆ° PG19 æ•°æ®é›†...
âœ“ ä½¿ç”¨æœ¬åœ°ç¼“å­˜: data/pg19/sample.json
  å·²åŠ è½½ç¼“å­˜æ•°æ® (é•¿åº¦: 318194 å­—ç¬¦)
æ•°æ®é›†å¤§å°: 4096 tokens
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼°åŸºçº¿ (æ— å‹ç¼©)
============================================================
åŸºçº¿ PPL: 57.92
åŸºçº¿ Runtime: 0.332s
åŸºçº¿ Prefill: 0.332s

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
StreamingLLM PPL: 57.92
StreamingLLM Runtime: 0.036s
StreamingLLM Prefill: 0.036s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "pg19:wikitext-103-v1",
  "split": "test",
  "max_length": 1024,
  "stride": 512,
  "max_samples": 1,
  "max_eval_tokens": 4096,
  "total_tokens": 4096,
  "streaming_llm": {
    "n_sink": 4,
    "window_size": 1024,
    "max_cache_size": 1028
  },
  "baseline": {
    "perplexity": 57.91722106933594,
    "runtime_sec": 0.3323866790160537,
    "prefill_sec": 0.33238473208621144
  },
  "streaming": {
    "perplexity": 57.91722106933594,
    "runtime_sec": 0.036372217000462115,
    "prefill_sec": 0.03637158893980086
  },
  "metrics": {
    "speedup": 9.138477289185607,
    "compression_ratio": 0.7490234375,
    "ppl_increase_percent": 0.0,
    "peak_memory_mb": 759.7294921875
  },
  "device": "cuda",
  "dtype": "torch.float16"
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/pg19_result.json

============================================================
æ€»ç»“
============================================================
åŠ é€Ÿæ¯”: 9.14x
å‹ç¼©æ¯”: 74.90%
PPL å¢åŠ : 0.00%
å³°å€¼æ˜¾å­˜: 759.7 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 9s)

[1;33m=== é˜¶æ®µ 3: kvpress å®˜æ–¹åº“å¯¹æ¯”å®éªŒ ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 5: WikiText-103 kvpress StreamingLLM[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_kvpress.py         --dataset-name wikitext         --dataset-config wikitext-103-v1         --max-samples 64         --max-eval-tokens 4096         --n-sink 4         --window-size 1024         --output results/kvpress/wikitext_result.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:06:04 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
KVPress StreamingLLM Evaluation
============================================================
Model: EleutherAI/pythia-70m
Dataset: wikitext:wikitext-103-v1
Device: cuda
Data type: torch.float16
n_sink: 4
window_size: 1024
============================================================

Loading tokenizer...
Loading dataset...
Dataset size: 3407 tokens
Loading model...

============================================================
Evaluating Baseline (no compression)
============================================================
Baseline PPL: 40.31
Baseline Runtime: 0.334s
Baseline Prefill: 0.334s

Calculated compression_ratio: 0.6983
(Keeping 1028 tokens out of 3407)

============================================================
Evaluating kvpress StreamingLLM
============================================================
kvpress StreamingLLM PPL: 40.31
kvpress StreamingLLM Runtime: 0.086s
kvpress StreamingLLM Prefill: 0.086s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "wikitext:wikitext-103-v1",
  "split": "test",
  "max_length": 1024,
  "stride": 512,
  "max_samples": 64,
  "max_eval_tokens": 4096,
  "total_tokens": 3407,
  "streaming_llm": {
    "n_sink": 4,
    "window_size": 1024,
    "max_cache_size": 1028,
    "compression_ratio": 0.6982682712063399
  },
  "baseline": {
    "perplexity": 40.312744140625,
    "runtime_sec": 0.3344038249924779,
    "prefill_sec": 0.33440387703012675
  },
  "kvpress": {
    "perplexity": 40.312744140625,
    "runtime_sec": 0.08557298104278743,
    "prefill_sec": 0.08557334099896252
  },
  "metrics": {
    "speedup": 3.9078202128458317,
    "compression_ratio": 0.6982682712063399,
    "ppl_increase_percent": 0.0,
    "peak_memory_mb": 759.7294921875
  },
  "device": "cuda",
  "dtype": "torch.float16"
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/kvpress/wikitext_result.json

============================================================
Summary
============================================================
Speedup: 3.91x
Compression ratio: 69.83%
PPL increase: 0.00%
Peak memory: 759.7 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 15s)


[0;34m========================================[0m
[0;34må®éªŒ 6: PG19 kvpress StreamingLLM[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_kvpress.py         --dataset-name pg19         --max-samples 1         --max-eval-tokens 4096         --n-sink 4         --window-size 1024         --output results/kvpress/pg19_result.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:06:19 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
KVPress StreamingLLM Evaluation
============================================================
Model: EleutherAI/pythia-70m
Dataset: pg19:wikitext-103-v1
Device: cuda
Data type: torch.float16
n_sink: 4
window_size: 1024
============================================================

Loading tokenizer...
Loading dataset...
æ£€æµ‹åˆ° PG19 æ•°æ®é›†...
âœ“ ä½¿ç”¨æœ¬åœ°ç¼“å­˜: data/pg19/sample.json
  å·²åŠ è½½ç¼“å­˜æ•°æ® (é•¿åº¦: 318194 å­—ç¬¦)
Dataset size: 4096 tokens
Loading model...

============================================================
Evaluating Baseline (no compression)
============================================================
Baseline PPL: 57.92
Baseline Runtime: 0.331s
Baseline Prefill: 0.331s

Calculated compression_ratio: 0.7490
(Keeping 1028 tokens out of 4096)

============================================================
Evaluating kvpress StreamingLLM
============================================================
kvpress StreamingLLM PPL: 57.92
kvpress StreamingLLM Runtime: 0.089s
kvpress StreamingLLM Prefill: 0.089s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "pg19:wikitext-103-v1",
  "split": "test",
  "max_length": 1024,
  "stride": 512,
  "max_samples": 1,
  "max_eval_tokens": 4096,
  "total_tokens": 4096,
  "streaming_llm": {
    "n_sink": 4,
    "window_size": 1024,
    "max_cache_size": 1028,
    "compression_ratio": 0.7490234375
  },
  "baseline": {
    "perplexity": 57.91722106933594,
    "runtime_sec": 0.3308378660585731,
    "prefill_sec": 0.33083780109882355
  },
  "kvpress": {
    "perplexity": 57.91722106933594,
    "runtime_sec": 0.08940530102699995,
    "prefill_sec": 0.0894057210534811
  },
  "metrics": {
    "speedup": 3.7004278522440384,
    "compression_ratio": 0.7490234375,
    "ppl_increase_percent": 0.0,
    "peak_memory_mb": 759.7294921875
  },
  "device": "cuda",
  "dtype": "torch.float16"
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/kvpress/pg19_result.json

============================================================
Summary
============================================================
Speedup: 3.70x
Compression ratio: 74.90%
PPL increase: 0.00%
Peak memory: 759.7 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 8s)

[1;33m=== é˜¶æ®µ 4: æ¶ˆèå®éªŒ ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 7: Window Size æ¶ˆèå®éªŒ[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/ablation_study.py         --dataset-name wikitext         --dataset-config wikitext-103-v1         --max-samples 64         --max-eval-tokens 4096         --ablation-type window_size         --output results/streaming_llm/ablation_window_size.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:06:27 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM æ¶ˆèå®éªŒ
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: wikitext:wikitext-103-v1
æ¶ˆèç±»å‹: window_size
è®¾å¤‡: cuda
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ•°æ®é›†å¤§å°: 3407 tokens
åŠ è½½æ¨¡å‹...

============================================================
æ¶ˆèå®éªŒ: Window Size (n_sink=4)
============================================================

æµ‹è¯• window_size=128...
  PPL: 40.31, Runtime: 0.308s, Compression: 96.13%
æµ‹è¯• window_size=256...
  PPL: 40.31, Runtime: 0.032s, Compression: 92.37%
æµ‹è¯• window_size=512...
  PPL: 40.31, Runtime: 0.032s, Compression: 84.85%
æµ‹è¯• window_size=1024...
  PPL: 40.31, Runtime: 0.032s, Compression: 69.83%
æµ‹è¯• window_size=2048...
  PPL: 40.31, Runtime: 0.031s, Compression: 39.77%
æµ‹è¯• window_size=4096...
  PPL: 40.31, Runtime: 0.032s, Compression: 0.00%

============================================================
æ¶ˆèå®éªŒå®Œæˆ
============================================================
ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/ablation_window_size.json
============================================================

ç»“æœæ€»ç»“:
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "wikitext:wikitext-103-v1",
  "ablation_type": "window_size",
  "total_tokens": 3407,
  "results": [
    {
      "window_size": 128,
      "n_sink": 4,
      "max_cache_size": 132,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.3080794629640877,
      "prefill_sec": 0.3080777560826391,
      "compression_ratio": 0.9612562371587907
    },
    {
      "window_size": 256,
      "n_sink": 4,
      "max_cache_size": 260,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.031746126944199204,
      "prefill_sec": 0.031745587941259146,
      "compression_ratio": 0.923686527737012
    },
    {
      "window_size": 512,
      "n_sink": 4,
      "max_cache_size": 516,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.03157143003772944,
      "prefill_sec": 0.03157111699692905,
      "compression_ratio": 0.8485471088934546
    },
    {
      "window_size": 1024,
      "n_sink": 4,
      "max_cache_size": 1028,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.031586754019372165,
      "prefill_sec": 0.0315863840514794,
      "compression_ratio": 0.6982682712063399
    },
    {
      "window_size": 2048,
      "n_sink": 4,
      "max_cache_size": 2052,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.03144336608238518,
      "prefill_sec": 0.03144300100393593,
      "compression_ratio": 0.3977105958321103
    },
    {
      "window_size": 4096,
      "n_sink": 4,
      "max_cache_size": 4100,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.03151278500445187,
      "prefill_sec": 0.03151237196289003,
      "compression_ratio": 0.0
    }
  ]
}
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 20s)


[0;34m========================================[0m
[0;34må®éªŒ 8: N_sink æ¶ˆèå®éªŒ[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/ablation_study.py         --dataset-name wikitext         --dataset-config wikitext-103-v1         --max-samples 64         --max-eval-tokens 4096         --ablation-type n_sink         --output results/streaming_llm/ablation_n_sink.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:06:47 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM æ¶ˆèå®éªŒ
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: wikitext:wikitext-103-v1
æ¶ˆèç±»å‹: n_sink
è®¾å¤‡: cuda
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ•°æ®é›†å¤§å°: 3407 tokens
åŠ è½½æ¨¡å‹...

============================================================
æ¶ˆèå®éªŒ: N_sink (window_size=1024)
============================================================

æµ‹è¯• n_sink=0...
  PPL: 40.31, Runtime: 0.333s, Compression: 69.94%
æµ‹è¯• n_sink=1...
  PPL: 40.31, Runtime: 0.033s, Compression: 69.91%
æµ‹è¯• n_sink=2...
  PPL: 40.31, Runtime: 0.032s, Compression: 69.89%
æµ‹è¯• n_sink=4...
  PPL: 40.31, Runtime: 0.032s, Compression: 69.83%
æµ‹è¯• n_sink=8...
  PPL: 40.31, Runtime: 0.032s, Compression: 69.71%
æµ‹è¯• n_sink=16...
  PPL: 40.31, Runtime: 0.032s, Compression: 69.47%

============================================================
æ¶ˆèå®éªŒå®Œæˆ
============================================================
ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/ablation_n_sink.json
============================================================

ç»“æœæ€»ç»“:
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "wikitext:wikitext-103-v1",
  "ablation_type": "n_sink",
  "total_tokens": 3407,
  "results": [
    {
      "n_sink": 0,
      "window_size": 1024,
      "max_cache_size": 1024,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.33347127900924534,
      "prefill_sec": 0.33346987108234316,
      "compression_ratio": 0.6994423246257705
    },
    {
      "n_sink": 1,
      "window_size": 1024,
      "max_cache_size": 1025,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.03271344897802919,
      "prefill_sec": 0.032712365966290236,
      "compression_ratio": 0.6991488112709128
    },
    {
      "n_sink": 2,
      "window_size": 1024,
      "max_cache_size": 1026,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.03223823802545667,
      "prefill_sec": 0.03223783697467297,
      "compression_ratio": 0.6988552979160552
    },
    {
      "n_sink": 4,
      "window_size": 1024,
      "max_cache_size": 1028,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.03219667903613299,
      "prefill_sec": 0.03219632396940142,
      "compression_ratio": 0.6982682712063399
    },
    {
      "n_sink": 8,
      "window_size": 1024,
      "max_cache_size": 1032,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.03225538495462388,
      "prefill_sec": 0.03225491300690919,
      "compression_ratio": 0.6970942177869093
    },
    {
      "n_sink": 16,
      "window_size": 1024,
      "max_cache_size": 1040,
      "perplexity": 40.312744140625,
      "runtime_sec": 0.032175424974411726,
      "prefill_sec": 0.0321749490685761,
      "compression_ratio": 0.6947461109480482
    }
  ]
}
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 14s)

[1;33m=== é˜¶æ®µ 5: ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 9: ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/generate_final_figures.py
å¼€å§‹æ—¶é—´: Thu Dec  4 05:07:01 PM CST 2025
============================================================
StreamingLLM å®éªŒç»“æœå¯è§†åŒ–
============================================================
âœ“ å›¾è¡¨ä¿å­˜ç›®å½•: results/figures

ç”Ÿæˆä¸»å®éªŒå¯¹æ¯”å›¾...
âœ“ å·²ä¿å­˜: results/figures/main_comparison.png

ç”ŸæˆWindow Sizeæ¶ˆèå›¾...
âœ“ å·²ä¿å­˜: results/figures/ablation_window_size.png

ç”ŸæˆN_sinkæ¶ˆèå›¾...
âœ“ å·²ä¿å­˜: results/figures/ablation_n_sink.png

ç”Ÿæˆç»¼åˆç»“æœè¡¨æ ¼...
âœ“ å·²ä¿å­˜: results/figures/results_summary.png

============================================================
âœ“ æ‰€æœ‰å›¾è¡¨ç”Ÿæˆå®Œæˆ!
âœ“ ä¿å­˜ä½ç½®: results/figures
============================================================

ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶:
  - ablation_n_sink.png (0.36 MB)
  - ablation_window_size.png (0.33 MB)
  - main_comparison.png (0.30 MB)
  - results_summary.png (0.23 MB)
  - results_summary_table.png (0.11 MB)
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 5s)


################################################################################
# å®éªŒå®ŒæˆæŠ¥å‘Š
################################################################################

ç»“æŸæ—¶é—´: Thu Dec  4 05:07:06 PM CST 2025
æ€»è€—æ—¶: 152s (2åˆ†é’Ÿ)

å®éªŒç»Ÿè®¡:
  æ€»å®éªŒæ•°: 9
  [0;32mæˆåŠŸ: 9[0m
  [0;31må¤±è´¥: 0[0m

è¯¦ç»†ç»“æœ:
----------------------------------------
  [0;32mâœ“[0m WikiText-103 Baseline (15s)
  [0;32mâœ“[0m PG19 Baseline (47s)
  [0;32mâœ“[0m WikiText-103 StreamingLLM (19s)
  [0;32mâœ“[0m PG19 StreamingLLM (9s)
  [0;32mâœ“[0m WikiText-103 kvpress StreamingLLM (15s)
  [0;32mâœ“[0m PG19 kvpress StreamingLLM (8s)
  [0;32mâœ“[0m Window Size æ¶ˆèå®éªŒ (20s)
  [0;32mâœ“[0m N_sink æ¶ˆèå®éªŒ (14s)
  [0;32mâœ“[0m ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ (5s)
----------------------------------------

æ—¥å¿—æ–‡ä»¶: results/experiment_log_20251204_170434.txt
æ€»ç»“æ–‡ä»¶: results/experiment_summary_20251204_170434.txt

[0;32mâœ“ æ‰€æœ‰å®éªŒæˆåŠŸå®Œæˆ![0m
