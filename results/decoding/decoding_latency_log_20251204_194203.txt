################################################################################
# Decoding Latency å®éªŒ
# å¼€å§‹æ—¶é—´: Thu Dec  4 07:42:03 PM CST 2025
################################################################################

[1;33m=== Decoding Latency å®éªŒ ===[0m
æµ‹é‡é…ç½® (é»˜è®¤æ®µ):
  - Prompt Length: 512
  - Num Tokens: 2000
  - Warmup Tokens: 200
  - Num Runs: 3


[0;34m========================================[0m
[0;34må®éªŒ 1: Decoding Latency Baseline (full cache)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 4096         --n-sink 4         --prompt-length 512         --num-tokens 2000         --warmup-tokens 200         --num-runs 3         --mode baseline         --output results/decoding/decoding_latency_baseline.json
å¼€å§‹æ—¶é—´: Thu Dec  4 07:42:03 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 4096
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
Mode: baseline
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° Baseline (Full KV Cache)
============================================================

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.768 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.008 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.017 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.264 Â± 1.901 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.007 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
åŸºçº¿è¯„ä¼°å®Œæˆ
============================================================
Baseline: 3.264 Â± 1.901 ms/token
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_baseline.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 27s)


[0;34m========================================[0m
[0;34må®éªŒ 2: Streaming Decoding Latency (Cache=512)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py             --cache-size 512             --n-sink 4             --prompt-length 512             --num-tokens 2000             --warmup-tokens 200             --num-runs 3             --mode streaming             --baseline-results results/decoding/decoding_latency_baseline.json             --output results/decoding/decoding_latency_512.json
å¼€å§‹æ—¶é—´: Thu Dec  4 07:42:30 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 512
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
Mode: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 512

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.371 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.987 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.997 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.119 Â± 1.035 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.989 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      3.264 Â± 1.901 ms/token
StreamingLLM:  3.119 Â± 1.035 ms/token
åŠ é€Ÿæ¯”:        1.05x
æ˜¾å­˜å‡å°‘:      -0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_512.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 35s)


[0;34m========================================[0m
[0;34må®éªŒ 3: Streaming Decoding Latency (Cache=1024)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py             --cache-size 1024             --n-sink 4             --prompt-length 512             --num-tokens 2000             --warmup-tokens 200             --num-runs 3             --mode streaming             --baseline-results results/decoding/decoding_latency_baseline.json             --output results/decoding/decoding_latency_1024.json
å¼€å§‹æ—¶é—´: Thu Dec  4 07:43:05 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 1024
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
Mode: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 1024

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.037 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.999 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.990 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.009 Â± 0.091 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.998 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      3.264 Â± 1.901 ms/token
StreamingLLM:  3.009 Â± 0.091 ms/token
åŠ é€Ÿæ¯”:        1.08x
æ˜¾å­˜å‡å°‘:      -0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_1024.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 32s)


[0;34m========================================[0m
[0;34må®éªŒ 4: Streaming Decoding Latency (Cache=2048)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py             --cache-size 2048             --n-sink 4             --prompt-length 512             --num-tokens 2000             --warmup-tokens 200             --num-runs 3             --mode streaming             --baseline-results results/decoding/decoding_latency_baseline.json             --output results/decoding/decoding_latency_2048.json
å¼€å§‹æ—¶é—´: Thu Dec  4 07:43:37 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 2048
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
Mode: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 2048

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.009 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.995 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.994 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.000 Â± 0.075 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.992 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      3.264 Â± 1.901 ms/token
StreamingLLM:  3.000 Â± 0.075 ms/token
åŠ é€Ÿæ¯”:        1.09x
æ˜¾å­˜å‡å°‘:      -0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_2048.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 28s)


[0;34m========================================[0m
[0;34må®éªŒ 5: Streaming Decoding Latency (Cache=4096)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py             --cache-size 4096             --n-sink 4             --prompt-length 512             --num-tokens 2000             --warmup-tokens 200             --num-runs 3             --mode streaming             --baseline-results results/decoding/decoding_latency_baseline.json             --output results/decoding/decoding_latency_4096.json
å¼€å§‹æ—¶é—´: Thu Dec  4 07:44:05 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 4096
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
Mode: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 4096

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.000 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.990 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.160 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.050 Â± 4.025 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.984 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      3.264 Â± 1.901 ms/token
StreamingLLM:  3.050 Â± 4.025 ms/token
åŠ é€Ÿæ¯”:        1.07x
æ˜¾å­˜å‡å°‘:      -0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_4096.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 28s)

[1;33m=== é•¿åºåˆ— Decoding Latency æµ‹è¯• ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 6: Long Sequence Baseline (5000 tokens)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 4096         --n-sink 4         --prompt-length 512         --num-tokens 5000         --warmup-tokens 200         --num-runs 3         --mode baseline         --output results/decoding/decoding_latency_baseline_long_5000.json
å¼€å§‹æ—¶é—´: Thu Dec  4 07:44:33 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 4096
Prompt Length: 512
Num Tokens: 5000
Warmup Tokens: 200
Num Runs: 3
Mode: baseline
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° Baseline (Full KV Cache)
============================================================

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.298 ms/token
  å³°å€¼æ˜¾å­˜: 215.3 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.264 ms/token
  å³°å€¼æ˜¾å­˜: 215.3 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.032 ms/token
  å³°å€¼æ˜¾å­˜: 215.3 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.198 Â± 1.323 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.022 ms/token
  å¹³å‡æ˜¾å­˜: 215.3 MB

============================================================
åŸºçº¿è¯„ä¼°å®Œæˆ
============================================================
Baseline: 3.198 Â± 1.323 ms/token
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_baseline_long_5000.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 56s)


[0;34m========================================[0m
[0;34må®éªŒ 7: Long Sequence Streaming (5000 tokens, Cache=1024)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 1024         --n-sink 4         --prompt-length 512         --num-tokens 5000         --warmup-tokens 200         --num-runs 3         --mode streaming         --baseline-results results/decoding/decoding_latency_baseline_long_5000.json         --output results/decoding/decoding_latency_long_5000.json
å¼€å§‹æ—¶é—´: Thu Dec  4 07:45:29 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 1024
Prompt Length: 512
Num Tokens: 5000
Warmup Tokens: 200
Num Runs: 3
Mode: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 1024

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.301 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.347 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.342 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.330 Â± 1.384 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.143 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      3.198 Â± 1.323 ms/token
StreamingLLM:  3.330 Â± 1.384 ms/token
åŠ é€Ÿæ¯”:        0.96x
æ˜¾å­˜å‡å°‘:      6.3%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_long_5000.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 76s)

[1;33m=== ä¸åŒ n_sink é…ç½®æµ‹è¯• ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 8: Streaming Decoding (n_sink=0, Cache=1024)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 1024         --n-sink 0         --prompt-length 512         --num-tokens 2000         --warmup-tokens 200         --num-runs 3         --mode streaming         --baseline-results results/decoding/decoding_latency_baseline.json         --output results/decoding/decoding_latency_nsink0.json
å¼€å§‹æ—¶é—´: Thu Dec  4 07:46:45 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 1024
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
Mode: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 0
  window_size: 1024

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.972 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.964 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.598 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.178 Â± 1.383 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.965 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      3.264 Â± 1.901 ms/token
StreamingLLM:  3.178 Â± 1.383 ms/token
åŠ é€Ÿæ¯”:        1.03x
æ˜¾å­˜å‡å°‘:      -0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_nsink0.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 34s)


[0;34m========================================[0m
[0;34må®éªŒ 9: Streaming Decoding (n_sink=8, Cache=1024)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 1024         --n-sink 8         --prompt-length 512         --num-tokens 2000         --warmup-tokens 200         --num-runs 3         --mode streaming         --baseline-results results/decoding/decoding_latency_baseline.json         --output results/decoding/decoding_latency_nsink8.json
å¼€å§‹æ—¶é—´: Thu Dec  4 07:47:19 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 1024
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
Mode: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 8
  window_size: 1024

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.004 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.459 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.003 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.155 Â± 1.205 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.002 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      3.264 Â± 1.901 ms/token
StreamingLLM:  3.155 Â± 1.205 ms/token
åŠ é€Ÿæ¯”:        1.03x
æ˜¾å­˜å‡å°‘:      -0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_nsink8.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 33s)


################################################################################
# Decoding Latency å®éªŒå®ŒæˆæŠ¥å‘Š
################################################################################

ç»“æŸæ—¶é—´: Thu Dec  4 07:47:52 PM CST 2025
æ€»è€—æ—¶: 349s (5åˆ†é’Ÿ)

å®éªŒç»Ÿè®¡:
  æ€»å®éªŒæ•°: 9
  [0;32mæˆåŠŸ: 9[0m
  [0;31må¤±è´¥: 0[0m

è¯¦ç»†ç»“æœ:
----------------------------------------
  [0;32mâœ“[0m Decoding Latency Baseline (full cache) (27s)
  [0;32mâœ“[0m Streaming Decoding Latency (Cache=512) (35s)
  [0;32mâœ“[0m Streaming Decoding Latency (Cache=1024) (32s)
  [0;32mâœ“[0m Streaming Decoding Latency (Cache=2048) (28s)
  [0;32mâœ“[0m Streaming Decoding Latency (Cache=4096) (28s)
  [0;32mâœ“[0m Long Sequence Baseline (5000 tokens) (56s)
  [0;32mâœ“[0m Long Sequence Streaming (5000 tokens, Cache=1024) (76s)
  [0;32mâœ“[0m Streaming Decoding (n_sink=0, Cache=1024) (34s)
  [0;32mâœ“[0m Streaming Decoding (n_sink=8, Cache=1024) (33s)
----------------------------------------

ç»“æœæ–‡ä»¶:
-rw-rw-r-- 1 jflin jflin 300K Dec  4 19:43 results/decoding/decoding_latency_1024.json
-rw-rw-r-- 1 jflin jflin 300K Dec  4 19:44 results/decoding/decoding_latency_2048.json
-rw-rw-r-- 1 jflin jflin 300K Dec  4 19:44 results/decoding/decoding_latency_4096.json
-rw-rw-r-- 1 jflin jflin 300K Dec  4 19:43 results/decoding/decoding_latency_512.json
-rw-rw-r-- 1 jflin jflin 150K Dec  4 19:42 results/decoding/decoding_latency_baseline.json
-rw-rw-r-- 1 jflin jflin 375K Dec  4 19:45 results/decoding/decoding_latency_baseline_long_5000.json
-rw-rw-r-- 1 jflin jflin 748K Dec  4 19:46 results/decoding/decoding_latency_long_5000.json
-rw-rw-r-- 1 jflin jflin 300K Dec  4 19:47 results/decoding/decoding_latency_nsink0.json
-rw-rw-r-- 1 jflin jflin 300K Dec  4 19:47 results/decoding/decoding_latency_nsink8.json

æ—¥å¿—æ–‡ä»¶: results/decoding/decoding_latency_log_20251204_194203.txt
æ€»ç»“æ–‡ä»¶: results/decoding/decoding_latency_summary_20251204_194203.txt

[0;32mâœ“ æ‰€æœ‰ Decoding Latency å®éªŒæˆåŠŸå®Œæˆ![0m
