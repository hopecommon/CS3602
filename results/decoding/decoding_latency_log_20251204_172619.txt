################################################################################
# Decoding Latency å®éªŒ
# å¼€å§‹æ—¶é—´: Thu Dec  4 05:26:19 PM CST 2025
################################################################################

[1;33m=== Decoding Latency å®éªŒ ===[0m
æµ‹é‡é…ç½®:
  - Prompt Length: 512
  - Num Tokens: 2000
  - Warmup Tokens: 200
  - Num Runs: 3


[0;34m========================================[0m
[0;34må®éªŒ 1: Decoding Latency (Cache=512)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 512         --n-sink 4         --prompt-length 512         --num-tokens 2000         --warmup-tokens 200         --num-runs 3         --output results/decoding/decoding_latency_512.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:26:19 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 512
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° Baseline (Full KV Cache)
============================================================

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.054 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.558 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.048 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.220 Â± 1.251 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.036 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 512

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.071 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.077 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.730 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.293 Â± 1.481 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.065 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      3.220 Â± 1.251 ms/token
StreamingLLM:  3.293 Â± 1.481 ms/token
åŠ é€Ÿæ¯”:        0.98x
æ˜¾å­˜å‡å°‘:      0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_512.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 51s)


[0;34m========================================[0m
[0;34må®éªŒ 2: Decoding Latency (Cache=1024)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 1024         --n-sink 4         --prompt-length 512         --num-tokens 2000         --warmup-tokens 200         --num-runs 3         --output results/decoding/decoding_latency_1024.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:27:10 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 1024
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° Baseline (Full KV Cache)
============================================================

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.000 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.985 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.988 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 2.991 Â± 0.075 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.985 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 1024

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.756 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.030 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.970 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.252 Â± 1.567 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.010 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      2.991 Â± 0.075 ms/token
StreamingLLM:  3.252 Â± 1.567 ms/token
åŠ é€Ÿæ¯”:        0.92x
æ˜¾å­˜å‡å°‘:      0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_1024.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 50s)


[0;34m========================================[0m
[0;34må®éªŒ 3: Decoding Latency (Cache=2048)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 2048         --n-sink 4         --prompt-length 512         --num-tokens 2000         --warmup-tokens 200         --num-runs 3         --output results/decoding/decoding_latency_2048.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:28:00 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 2048
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° Baseline (Full KV Cache)
============================================================

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.702 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.958 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.963 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.207 Â± 1.778 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.961 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 2048

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.021 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.337 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.018 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.125 Â± 1.106 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.012 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      3.207 Â± 1.778 ms/token
StreamingLLM:  3.125 Â± 1.106 ms/token
åŠ é€Ÿæ¯”:        1.03x
æ˜¾å­˜å‡å°‘:      0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_2048.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 50s)


[0;34m========================================[0m
[0;34må®éªŒ 4: Decoding Latency (Cache=4096)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 4096         --n-sink 4         --prompt-length 512         --num-tokens 2000         --warmup-tokens 200         --num-runs 3         --output results/decoding/decoding_latency_4096.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:28:50 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 4096
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° Baseline (Full KV Cache)
============================================================

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.936 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.925 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.158 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.007 Â± 0.879 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.924 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 4096

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.963 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.973 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.958 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 2.965 Â± 0.054 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.958 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      3.007 Â± 0.879 ms/token
StreamingLLM:  2.965 Â± 0.054 ms/token
åŠ é€Ÿæ¯”:        1.01x
æ˜¾å­˜å‡å°‘:      0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_4096.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 49s)

[1;33m=== é•¿åºåˆ— Decoding Latency æµ‹è¯• ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 5: Long Sequence Decoding (5000 tokens, Cache=1024)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 1024         --n-sink 4         --prompt-length 512         --num-tokens 5000         --warmup-tokens 200         --num-runs 3         --output results/decoding/decoding_latency_long_5000.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:29:39 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 1024
Prompt Length: 512
Num Tokens: 5000
Warmup Tokens: 200
Num Runs: 3
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° Baseline (Full KV Cache)
============================================================

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.029 ms/token
  å³°å€¼æ˜¾å­˜: 215.3 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.271 ms/token
  å³°å€¼æ˜¾å­˜: 215.3 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.021 ms/token
  å³°å€¼æ˜¾å­˜: 215.3 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.107 Â± 0.896 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.017 ms/token
  å¹³å‡æ˜¾å­˜: 215.3 MB

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 1024

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.327 ms/token
  å³°å€¼æ˜¾å­˜: 215.3 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.317 ms/token
  å³°å€¼æ˜¾å­˜: 215.3 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.050 ms/token
  å³°å€¼æ˜¾å­˜: 215.3 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.231 Â± 1.372 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.048 ms/token
  å¹³å‡æ˜¾å­˜: 215.3 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      3.107 Â± 0.896 ms/token
StreamingLLM:  3.231 Â± 1.372 ms/token
åŠ é€Ÿæ¯”:        0.96x
æ˜¾å­˜å‡å°‘:      0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_long_5000.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 109s)

[1;33m=== ä¸åŒ n_sink é…ç½®æµ‹è¯• ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 6: Decoding Latency (n_sink=0, Cache=1024)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 1024         --n-sink 0         --prompt-length 512         --num-tokens 2000         --warmup-tokens 200         --num-runs 3         --output results/decoding/decoding_latency_nsink0.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:31:28 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 1024
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° Baseline (Full KV Cache)
============================================================

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.667 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.988 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.007 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.221 Â± 1.519 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.991 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 0
  window_size: 1024

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.051 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.733 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.024 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.269 Â± 1.766 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.016 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      3.221 Â± 1.519 ms/token
StreamingLLM:  3.269 Â± 1.766 ms/token
åŠ é€Ÿæ¯”:        0.99x
æ˜¾å­˜å‡å°‘:      0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_nsink0.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 51s)


[0;34m========================================[0m
[0;34må®éªŒ 7: Decoding Latency (n_sink=8, Cache=1024)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 1024         --n-sink 8         --prompt-length 512         --num-tokens 2000         --warmup-tokens 200         --num-runs 3         --output results/decoding/decoding_latency_nsink8.json
å¼€å§‹æ—¶é—´: Thu Dec  4 05:32:19 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 1024
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° Baseline (Full KV Cache)
============================================================

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.085 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.446 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.059 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.197 Â± 1.160 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.064 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 8
  window_size: 1024

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.114 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.112 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.114 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.113 Â± 0.047 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.111 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      3.197 Â± 1.160 ms/token
StreamingLLM:  3.113 Â± 0.047 ms/token
åŠ é€Ÿæ¯”:        1.03x
æ˜¾å­˜å‡å°‘:      0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_nsink8.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 50s)


################################################################################
# Decoding Latency å®éªŒå®ŒæˆæŠ¥å‘Š
################################################################################

ç»“æŸæ—¶é—´: Thu Dec  4 05:33:09 PM CST 2025
æ€»è€—æ—¶: 410s (6åˆ†é’Ÿ)

å®éªŒç»Ÿè®¡:
  æ€»å®éªŒæ•°: 7
  [0;32mæˆåŠŸ: 7[0m
  [0;31må¤±è´¥: 0[0m

è¯¦ç»†ç»“æœ:
----------------------------------------
  [0;32mâœ“[0m Decoding Latency (Cache=512) (51s)
  [0;32mâœ“[0m Decoding Latency (Cache=1024) (50s)
  [0;32mâœ“[0m Decoding Latency (Cache=2048) (50s)
  [0;32mâœ“[0m Decoding Latency (Cache=4096) (49s)
  [0;32mâœ“[0m Long Sequence Decoding (5000 tokens, Cache=1024) (109s)
  [0;32mâœ“[0m Decoding Latency (n_sink=0, Cache=1024) (51s)
  [0;32mâœ“[0m Decoding Latency (n_sink=8, Cache=1024) (50s)
----------------------------------------

ç»“æœæ–‡ä»¶:
-rw-rw-r-- 1 jflin jflin 300K Dec  4 17:27 results/decoding/decoding_latency_1024.json
-rw-rw-r-- 1 jflin jflin 300K Dec  4 17:28 results/decoding/decoding_latency_2048.json
-rw-rw-r-- 1 jflin jflin 300K Dec  4 17:29 results/decoding/decoding_latency_4096.json
-rw-rw-r-- 1 jflin jflin 300K Dec  4 17:27 results/decoding/decoding_latency_512.json
-rw-rw-r-- 1 jflin jflin 748K Dec  4 17:31 results/decoding/decoding_latency_long_5000.json
-rw-rw-r-- 1 jflin jflin 300K Dec  4 17:32 results/decoding/decoding_latency_nsink0.json
-rw-rw-r-- 1 jflin jflin 300K Dec  4 17:33 results/decoding/decoding_latency_nsink8.json

æ—¥å¿—æ–‡ä»¶: results/decoding/decoding_latency_log_20251204_172619.txt
æ€»ç»“æ–‡ä»¶: results/decoding/decoding_latency_summary_20251204_172619.txt

[0;32mâœ“ æ‰€æœ‰ Decoding Latency å®éªŒæˆåŠŸå®Œæˆ![0m
