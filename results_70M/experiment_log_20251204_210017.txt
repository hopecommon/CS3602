################################################################################
# StreamingLLM å®Œæ•´å®éªŒ
# å¼€å§‹æ—¶é—´: Thu Dec  4 09:00:17 PM CST 2025
################################################################################

[1;33m=== é˜¶æ®µ 1: Baseline å®éªŒ ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 1: WikiText-103 Baseline[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_streaming_llm.py         --dataset-name wikitext         --dataset-config wikitext-103-v1         --max-samples 64         --max-eval-tokens 4096         --n-sink 0         --window-size 1024         --max-length 2048         --stride 1024         --mode baseline         --output results/streaming_llm/wikitext_baseline.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:00:17 PM CST 2025
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'wikitext-103-v1' at /data2/jflin/CS3602/.cache/huggingface/datasets/wikitext/wikitext-103-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Tue Dec  2 21:44:00 2025).
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: wikitext:wikitext-103-v1
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
n_sink: 0
window_size: 1024
æ¨¡å¼: baseline
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ•°æ®é›†å¤§å°: 3407 tokens
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼°åŸºçº¿ (æ— å‹ç¼©)
============================================================
åŸºçº¿ PPL: 36.30
åŸºçº¿ Runtime: 17.545s
åŸºçº¿ Prefill: 0.390s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "wikitext:wikitext-103-v1",
  "split": "test",
  "max_length": 2048,
  "stride": 1024,
  "max_samples": 64,
  "max_eval_tokens": 4096,
  "total_tokens": 3407,
  "streaming_llm": {
    "n_sink": 0,
    "window_size": 1024,
    "implementation": "ours",
    "cache_type": "StreamingKVCache",
    "max_cache_size": 1024
  },
  "baseline": {
    "perplexity": 36.30228805541992,
    "runtime_sec": 17.54459418798797,
    "prefill_sec": 0.3902663900516927
  },
  "baseline_source": "computed",
  "device": "cuda",
  "dtype": "torch.float16"
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/wikitext_baseline.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 23s)


[0;34m========================================[0m
[0;34må®éªŒ 2: PG19 Baseline[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_streaming_llm.py         --dataset-name pg19         --dataset-config ""         --max-samples 1         --max-eval-tokens 4096         --n-sink 0         --window-size 1024         --max-length 2048         --stride 1024         --mode baseline         --output results/streaming_llm/pg19_baseline.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:00:40 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: pg19:
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
n_sink: 0
window_size: 1024
æ¨¡å¼: baseline
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ£€æµ‹åˆ° PG19 æ•°æ®é›†...
âœ“ ä½¿ç”¨æœ¬åœ°ç¼“å­˜: data/pg19/sample.json
  å·²åŠ è½½ç¼“å­˜æ•°æ® (é•¿åº¦: 318194 å­—ç¬¦)
æ•°æ®é›†å¤§å°: 4096 tokens
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼°åŸºçº¿ (æ— å‹ç¼©)
============================================================
åŸºçº¿ PPL: 57.46
åŸºçº¿ Runtime: 21.916s
åŸºçº¿ Prefill: 0.346s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "pg19:",
  "split": "test",
  "max_length": 2048,
  "stride": 1024,
  "max_samples": 1,
  "max_eval_tokens": 4096,
  "total_tokens": 4096,
  "streaming_llm": {
    "n_sink": 0,
    "window_size": 1024,
    "implementation": "ours",
    "cache_type": "StreamingKVCache",
    "max_cache_size": 1024
  },
  "baseline": {
    "perplexity": 57.45960235595703,
    "runtime_sec": 21.916492628050037,
    "prefill_sec": 0.34578244597651064
  },
  "baseline_source": "computed",
  "device": "cuda",
  "dtype": "torch.float16"
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/pg19_baseline.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 29s)

[1;33m=== é˜¶æ®µ 2: æˆ‘ä»¬çš„ StreamingLLM å®éªŒ ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 3: WikiText-103 StreamingLLM[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_streaming_llm.py         --dataset-name wikitext         --dataset-config wikitext-103-v1         --max-samples 64         --max-eval-tokens 4096         --n-sink 4         --window-size 1024         --max-length 2048         --stride 1024         --mode streaming         --baseline-results results/streaming_llm/wikitext_baseline.json         --output results/streaming_llm/wikitext_result.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:01:09 PM CST 2025
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'wikitext-103-v1' at /data2/jflin/CS3602/.cache/huggingface/datasets/wikitext/wikitext-103-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Tue Dec  2 21:44:00 2025).
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: wikitext:wikitext-103-v1
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
n_sink: 4
window_size: 1024
æ¨¡å¼: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ•°æ®é›†å¤§å°: 3407 tokens
åŠ è½½æ¨¡å‹...

åŠ è½½å·²æœ‰åŸºçº¿ç»“æœ: results/streaming_llm/wikitext_baseline.json

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
StreamingLLM PPL: 36.79
StreamingLLM Runtime: 11.293s
StreamingLLM Prefill: 0.628s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "wikitext:wikitext-103-v1",
  "split": "test",
  "max_length": 2048,
  "stride": 1024,
  "max_samples": 64,
  "max_eval_tokens": 4096,
  "total_tokens": 3407,
  "streaming_llm": {
    "n_sink": 4,
    "window_size": 1024,
    "implementation": "ours",
    "cache_type": "StreamingKVCache",
    "max_cache_size": 1028
  },
  "device": "cuda",
  "dtype": "torch.float16",
  "baseline_source": "loaded:results/streaming_llm/wikitext_baseline.json",
  "baseline": {
    "perplexity": 36.30228805541992,
    "runtime_sec": 17.54459418798797,
    "prefill_sec": 0.3902663900516927
  },
  "streaming": {
    "perplexity": 36.790061950683594,
    "runtime_sec": 11.293028665008023,
    "prefill_sec": 0.6279541050316766
  },
  "metrics": {
    "compression_ratio": 0.6982682712063399,
    "peak_memory_mb": 356.240234375,
    "speedup": 1.5535774067722605,
    "ppl_increase_percent": 1.3436450466125576
  }
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/wikitext_result.json

============================================================
æ€»ç»“
============================================================
åŠ é€Ÿæ¯”: 1.55x
å‹ç¼©æ¯”: 69.83%
PPL å¢åŠ : 1.34%
å³°å€¼æ˜¾å­˜: 356.2 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 16s)


[0;34m========================================[0m
[0;34må®éªŒ 4: WikiText-103 MIT StreamingLLM[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_streaming_llm.py         --dataset-name wikitext         --dataset-config wikitext-103-v1         --max-samples 64         --max-eval-tokens 4096         --n-sink 4         --window-size 1024         --max-length 2048         --stride 1024         --streaming-mode mit         --mode streaming         --baseline-results results/streaming_llm/wikitext_baseline.json         --output results/streaming_llm/wikitext_mit_result.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:01:26 PM CST 2025
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'wikitext-103-v1' at /data2/jflin/CS3602/.cache/huggingface/datasets/wikitext/wikitext-103-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Tue Dec  2 21:44:00 2025).
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: wikitext:wikitext-103-v1
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
n_sink: 4
window_size: 1024
æ¨¡å¼: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ•°æ®é›†å¤§å°: 3407 tokens
åŠ è½½æ¨¡å‹...

åŠ è½½å·²æœ‰åŸºçº¿ç»“æœ: results/streaming_llm/wikitext_baseline.json

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
StreamingLLM PPL: 36.79
StreamingLLM Runtime: 11.076s
StreamingLLM Prefill: 0.283s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "wikitext:wikitext-103-v1",
  "split": "test",
  "max_length": 2048,
  "stride": 1024,
  "max_samples": 64,
  "max_eval_tokens": 4096,
  "total_tokens": 3407,
  "streaming_llm": {
    "n_sink": 4,
    "window_size": 1024,
    "implementation": "mit",
    "cache_type": "StartRecentKVCache",
    "max_cache_size": 1028
  },
  "device": "cuda",
  "dtype": "torch.float16",
  "baseline_source": "loaded:results/streaming_llm/wikitext_baseline.json",
  "baseline": {
    "perplexity": 36.30228805541992,
    "runtime_sec": 17.54459418798797,
    "prefill_sec": 0.3902663900516927
  },
  "streaming": {
    "perplexity": 36.790061950683594,
    "runtime_sec": 11.075816825963557,
    "prefill_sec": 0.28325181792024523
  },
  "metrics": {
    "compression_ratio": 0.6982682712063399,
    "peak_memory_mb": 356.240234375,
    "speedup": 1.5840451737031729,
    "ppl_increase_percent": 1.3436450466125576
  }
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/wikitext_mit_result.json

============================================================
æ€»ç»“
============================================================
åŠ é€Ÿæ¯”: 1.58x
å‹ç¼©æ¯”: 69.83%
PPL å¢åŠ : 1.34%
å³°å€¼æ˜¾å­˜: 356.2 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 16s)


[0;34m========================================[0m
[0;34må®éªŒ 5: PG19 StreamingLLM[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_streaming_llm.py         --dataset-name pg19         --dataset-config ""         --max-samples 1         --max-eval-tokens 4096         --n-sink 4         --window-size 1024         --max-length 2048         --stride 1024         --mode streaming         --baseline-results results/streaming_llm/pg19_baseline.json         --output results/streaming_llm/pg19_result.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:01:42 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: pg19:
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
n_sink: 4
window_size: 1024
æ¨¡å¼: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ£€æµ‹åˆ° PG19 æ•°æ®é›†...
âœ“ ä½¿ç”¨æœ¬åœ°ç¼“å­˜: data/pg19/sample.json
  å·²åŠ è½½ç¼“å­˜æ•°æ® (é•¿åº¦: 318194 å­—ç¬¦)
æ•°æ®é›†å¤§å°: 4096 tokens
åŠ è½½æ¨¡å‹...

åŠ è½½å·²æœ‰åŸºçº¿ç»“æœ: results/streaming_llm/pg19_baseline.json

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
StreamingLLM PPL: 57.99
StreamingLLM Runtime: 13.948s
StreamingLLM Prefill: 0.282s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "pg19:",
  "split": "test",
  "max_length": 2048,
  "stride": 1024,
  "max_samples": 1,
  "max_eval_tokens": 4096,
  "total_tokens": 4096,
  "streaming_llm": {
    "n_sink": 4,
    "window_size": 1024,
    "implementation": "ours",
    "cache_type": "StreamingKVCache",
    "max_cache_size": 1028
  },
  "device": "cuda",
  "dtype": "torch.float16",
  "baseline_source": "loaded:results/streaming_llm/pg19_baseline.json",
  "baseline": {
    "perplexity": 57.45960235595703,
    "runtime_sec": 21.916492628050037,
    "prefill_sec": 0.34578244597651064
  },
  "streaming": {
    "perplexity": 57.994022369384766,
    "runtime_sec": 13.947909932001494,
    "prefill_sec": 0.2824738050112501
  },
  "metrics": {
    "compression_ratio": 0.7490234375,
    "peak_memory_mb": 356.240234375,
    "speedup": 1.5713101629489135,
    "ppl_increase_percent": 0.930079554183217
  }
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/pg19_result.json

============================================================
æ€»ç»“
============================================================
åŠ é€Ÿæ¯”: 1.57x
å‹ç¼©æ¯”: 74.90%
PPL å¢åŠ : 0.93%
å³°å€¼æ˜¾å­˜: 356.2 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 21s)


[0;34m========================================[0m
[0;34må®éªŒ 6: PG19 MIT StreamingLLM[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_streaming_llm.py         --dataset-name pg19         --dataset-config ""         --max-samples 1         --max-eval-tokens 4096         --n-sink 4         --window-size 1024         --max-length 2048         --stride 1024         --streaming-mode mit         --mode streaming         --baseline-results results/streaming_llm/pg19_baseline.json         --output results/streaming_llm/pg19_mit_result.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:02:03 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: pg19:
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
n_sink: 4
window_size: 1024
æ¨¡å¼: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ£€æµ‹åˆ° PG19 æ•°æ®é›†...
âœ“ ä½¿ç”¨æœ¬åœ°ç¼“å­˜: data/pg19/sample.json
  å·²åŠ è½½ç¼“å­˜æ•°æ® (é•¿åº¦: 318194 å­—ç¬¦)
æ•°æ®é›†å¤§å°: 4096 tokens
åŠ è½½æ¨¡å‹...

åŠ è½½å·²æœ‰åŸºçº¿ç»“æœ: results/streaming_llm/pg19_baseline.json

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
StreamingLLM PPL: 57.99
StreamingLLM Runtime: 15.025s
StreamingLLM Prefill: 0.274s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "pg19:",
  "split": "test",
  "max_length": 2048,
  "stride": 1024,
  "max_samples": 1,
  "max_eval_tokens": 4096,
  "total_tokens": 4096,
  "streaming_llm": {
    "n_sink": 4,
    "window_size": 1024,
    "implementation": "mit",
    "cache_type": "StartRecentKVCache",
    "max_cache_size": 1028
  },
  "device": "cuda",
  "dtype": "torch.float16",
  "baseline_source": "loaded:results/streaming_llm/pg19_baseline.json",
  "baseline": {
    "perplexity": 57.45960235595703,
    "runtime_sec": 21.916492628050037,
    "prefill_sec": 0.34578244597651064
  },
  "streaming": {
    "perplexity": 57.994022369384766,
    "runtime_sec": 15.024628986953758,
    "prefill_sec": 0.2739724980201572
  },
  "metrics": {
    "compression_ratio": 0.7490234375,
    "peak_memory_mb": 356.240234375,
    "speedup": 1.4587044144038863,
    "ppl_increase_percent": 0.930079554183217
  }
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/pg19_mit_result.json

============================================================
æ€»ç»“
============================================================
åŠ é€Ÿæ¯”: 1.46x
å‹ç¼©æ¯”: 74.90%
PPL å¢åŠ : 0.93%
å³°å€¼æ˜¾å­˜: 356.2 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 20s)

[1;33m=== é˜¶æ®µ 3: kvpress å®˜æ–¹åº“å¯¹æ¯”å®éªŒ ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 7: WikiText-103 kvpress StreamingLLM (decode-loop)[0m
[0;34m========================================[0m
å‘½ä»¤: bash experiments/run_kvpress_streaming_decode.sh | tee -a "results/experiment_log_20251204_210017.txt"
å¼€å§‹æ—¶é—´: Thu Dec  4 09:02:23 PM CST 2025
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'wikitext-103-v1' at /data2/jflin/CS3602/.cache/huggingface/datasets/wikitext/wikitext-103-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Tue Dec  2 21:44:00 2025).
`torch_dtype` is deprecated! Use `dtype` instead!
Loading tokenizer...
Loading dataset...
Loading model...
PPL: 39.1404
Total runtime: 8.7880s
Prefill runtime: 0.3462s
Streaming loop runtime: 8.4417s
Loading tokenizer...
Loading dataset...
Loading model...
PPL: 39.1404
Total runtime: 8.7880s
Prefill runtime: 0.3462s
Streaming loop runtime: 8.4417s
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 14s)


[0;34m========================================[0m
[0;34må®éªŒ 8: WikiText-103 kvpress StreamingLLM (official eval)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_kvpress.py         --dataset-name wikitext         --dataset-config wikitext-103-v1         --max-samples 64         --max-eval-tokens 4096         --n-sink 4         --window-size 1024         --max-length 2048         --stride 1024         --output results/kvpress/wikitext_result.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:02:37 PM CST 2025
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'wikitext-103-v1' at /data2/jflin/CS3602/.cache/huggingface/datasets/wikitext/wikitext-103-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Tue Dec  2 21:44:00 2025).
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
KVPress StreamingLLM Evaluation
============================================================
Model: EleutherAI/pythia-70m
Dataset: wikitext:wikitext-103-v1
Device: cuda
Data type: torch.float16
n_sink: 4
window_size: 1024
============================================================

Loading tokenizer...
Loading dataset...
Dataset size: 3407 tokens
Loading model...

============================================================
Evaluating Baseline (no compression)
============================================================
Baseline PPL: 35.98
Baseline Runtime: 0.734s
Baseline Prefill: 0.734s

Target cache size per layer: 1028 tokens
Compression ratio (kvpress): 0.4980 (max_length=2048)

============================================================
Evaluating kvpress StreamingLLM
============================================================
kvpress StreamingLLM PPL: 35.98
kvpress StreamingLLM Runtime: 0.235s
kvpress StreamingLLM Prefill: 0.235s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "wikitext:wikitext-103-v1",
  "split": "test",
  "max_length": 2048,
  "stride": 1024,
  "max_samples": 64,
  "max_eval_tokens": 4096,
  "total_tokens": 3407,
  "streaming_llm": {
    "n_sink": 4,
    "window_size": 1024,
    "max_cache_size": 1028,
    "compression_ratio": 0.498046875
  },
  "baseline": {
    "perplexity": 35.978946685791016,
    "runtime_sec": 0.7335783890448511,
    "prefill_sec": 0.7335836990969256
  },
  "kvpress": {
    "perplexity": 35.978946685791016,
    "runtime_sec": 0.23544482595752925,
    "prefill_sec": 0.2354454630985856
  },
  "metrics": {
    "speedup": 3.115712507427017,
    "compression_ratio": 0.498046875,
    "ppl_increase_percent": 0.0,
    "peak_memory_mb": 1377.2529296875
  },
  "device": "cuda",
  "dtype": "torch.float16"
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/kvpress/wikitext_result.json

============================================================
Summary
============================================================
Speedup: 3.12x
Compression ratio: 49.80%
PPL increase: 0.00%
Peak memory: 1377.3 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 7s)


[0;34m========================================[0m
[0;34må®éªŒ 9: PG19 kvpress StreamingLLM (decode-loop)[0m
[0;34m========================================[0m
å‘½ä»¤: DATASET_NAME=pg19 DATASET_CONFIG="" bash experiments/run_kvpress_streaming_decode.sh | tee -a "results/experiment_log_20251204_210017.txt"
å¼€å§‹æ—¶é—´: Thu Dec  4 09:02:44 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!
Loading tokenizer...
Loading dataset...
æ£€æµ‹åˆ° PG19 æ•°æ®é›†...
âœ“ ä½¿ç”¨æœ¬åœ°ç¼“å­˜: data/pg19/sample.json
  å·²åŠ è½½ç¼“å­˜æ•°æ® (é•¿åº¦: 318194 å­—ç¬¦)
Loading model...
PPL: 69.5108
Total runtime: 11.2576s
Prefill runtime: 0.3515s
Streaming loop runtime: 10.9061s
Loading tokenizer...
Loading dataset...
æ£€æµ‹åˆ° PG19 æ•°æ®é›†...
âœ“ ä½¿ç”¨æœ¬åœ°ç¼“å­˜: data/pg19/sample.json
  å·²åŠ è½½ç¼“å­˜æ•°æ® (é•¿åº¦: 318194 å­—ç¬¦)
Loading model...
PPL: 69.5108
Total runtime: 11.2576s
Prefill runtime: 0.3515s
Streaming loop runtime: 10.9061s
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 17s)


[0;34m========================================[0m
[0;34må®éªŒ 10: PG19 kvpress StreamingLLM (official eval)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_kvpress.py         --dataset-name pg19         --dataset-config ""         --max-samples 1         --max-eval-tokens 4096         --n-sink 4         --window-size 1024         --max-length 2048         --stride 1024         --output results/kvpress/pg19_result.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:03:01 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
KVPress StreamingLLM Evaluation
============================================================
Model: EleutherAI/pythia-70m
Dataset: pg19:
Device: cuda
Data type: torch.float16
n_sink: 4
window_size: 1024
============================================================

Loading tokenizer...
Loading dataset...
æ£€æµ‹åˆ° PG19 æ•°æ®é›†...
âœ“ ä½¿ç”¨æœ¬åœ°ç¼“å­˜: data/pg19/sample.json
  å·²åŠ è½½ç¼“å­˜æ•°æ® (é•¿åº¦: 318194 å­—ç¬¦)
Dataset size: 4096 tokens
Loading model...

============================================================
Evaluating Baseline (no compression)
============================================================
Baseline PPL: 57.90
Baseline Runtime: 0.369s
Baseline Prefill: 0.369s

Target cache size per layer: 1028 tokens
Compression ratio (kvpress): 0.4980 (max_length=2048)

============================================================
Evaluating kvpress StreamingLLM
============================================================
kvpress StreamingLLM PPL: 57.90
kvpress StreamingLLM Runtime: 0.081s
kvpress StreamingLLM Prefill: 0.081s

============================================================
å®éªŒç»“æœ
============================================================
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "pg19:",
  "split": "test",
  "max_length": 2048,
  "stride": 1024,
  "max_samples": 1,
  "max_eval_tokens": 4096,
  "total_tokens": 4096,
  "streaming_llm": {
    "n_sink": 4,
    "window_size": 1024,
    "max_cache_size": 1028,
    "compression_ratio": 0.498046875
  },
  "baseline": {
    "perplexity": 57.90441131591797,
    "runtime_sec": 0.36860421393066645,
    "prefill_sec": 0.36860384105239064
  },
  "kvpress": {
    "perplexity": 57.90441131591797,
    "runtime_sec": 0.08073051099199802,
    "prefill_sec": 0.08073090901598334
  },
  "metrics": {
    "speedup": 4.56586003731851,
    "compression_ratio": 0.498046875,
    "ppl_increase_percent": 0.0,
    "peak_memory_mb": 1378.2529296875
  },
  "device": "cuda",
  "dtype": "torch.float16"
}
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/kvpress/pg19_result.json

============================================================
Summary
============================================================
Speedup: 4.57x
Compression ratio: 49.80%
PPL increase: 0.00%
Peak memory: 1378.3 MB
============================================================

[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 6s)

[1;33m=== é˜¶æ®µ 4: æ¶ˆèå®éªŒ ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 11: Window Size æ¶ˆèå®éªŒ[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/ablation_study.py         --dataset-name wikitext         --dataset-config wikitext-103-v1         --max-samples 64         --max-eval-tokens 4096         --max-length 2048         --stride 1024         --ablation-type window_size         --output results/streaming_llm/ablation_window_size.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:03:07 PM CST 2025
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'wikitext-103-v1' at /data2/jflin/CS3602/.cache/huggingface/datasets/wikitext/wikitext-103-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Tue Dec  2 21:44:00 2025).
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM æ¶ˆèå®éªŒ
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: wikitext:wikitext-103-v1
æ¶ˆèç±»å‹: window_size
è®¾å¤‡: cuda
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ•°æ®é›†å¤§å°: 3407 tokens
åŠ è½½æ¨¡å‹...

============================================================
æ¶ˆèå®éªŒ: Window Size (n_sink=4)
============================================================

æµ‹è¯• window_size=128...
  PPL: 65.69, Runtime: 15.666s, Compression: 96.13%
æµ‹è¯• window_size=256...
  PPL: 56.16, Runtime: 14.355s, Compression: 92.37%
æµ‹è¯• window_size=512...
  PPL: 44.06, Runtime: 12.757s, Compression: 84.85%
æµ‹è¯• window_size=1024...
  PPL: 36.79, Runtime: 10.366s, Compression: 69.83%
æµ‹è¯• window_size=2048...
  PPL: 35.69, Runtime: 6.331s, Compression: 39.77%
æµ‹è¯• window_size=4096...
  PPL: 41.38, Runtime: 0.013s, Compression: 0.00%

============================================================
æ¶ˆèå®éªŒå®Œæˆ
============================================================
ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/ablation_window_size.json
============================================================

ç»“æœæ€»ç»“:
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "wikitext:wikitext-103-v1",
  "ablation_type": "window_size",
  "total_tokens": 3407,
  "results": [
    {
      "window_size": 128,
      "n_sink": 4,
      "max_cache_size": 132,
      "perplexity": 65.69120788574219,
      "runtime_sec": 15.665787777048536,
      "prefill_sec": 0.674108432023786,
      "compression_ratio": 0.9612562371587907
    },
    {
      "window_size": 256,
      "n_sink": 4,
      "max_cache_size": 260,
      "perplexity": 56.15695571899414,
      "runtime_sec": 14.354898457997479,
      "prefill_sec": 0.00787196890451014,
      "compression_ratio": 0.923686527737012
    },
    {
      "window_size": 512,
      "n_sink": 4,
      "max_cache_size": 516,
      "perplexity": 44.0606689453125,
      "runtime_sec": 12.75716402893886,
      "prefill_sec": 0.008398051024414599,
      "compression_ratio": 0.8485471088934546
    },
    {
      "window_size": 1024,
      "n_sink": 4,
      "max_cache_size": 1028,
      "perplexity": 36.790061950683594,
      "runtime_sec": 10.36647664802149,
      "prefill_sec": 0.005614658002741635,
      "compression_ratio": 0.6982682712063399
    },
    {
      "window_size": 2048,
      "n_sink": 4,
      "max_cache_size": 2052,
      "perplexity": 35.686309814453125,
      "runtime_sec": 6.3306690900353715,
      "prefill_sec": 0.005905437981709838,
      "compression_ratio": 0.3977105958321103
    },
    {
      "window_size": 4096,
      "n_sink": 4,
      "max_cache_size": 4100,
      "perplexity": 41.381832122802734,
      "runtime_sec": 0.012942221015691757,
      "prefill_sec": 0.012920539011247456,
      "compression_ratio": 0.0
    }
  ]
}
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 65s)


[0;34m========================================[0m
[0;34må®éªŒ 12: N_sink æ¶ˆèå®éªŒ[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/ablation_study.py         --dataset-name wikitext         --dataset-config wikitext-103-v1         --max-samples 64         --max-eval-tokens 4096         --max-length 2048         --stride 1024         --ablation-type n_sink         --output results/streaming_llm/ablation_n_sink.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:04:12 PM CST 2025
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'wikitext-103-v1' at /data2/jflin/CS3602/.cache/huggingface/datasets/wikitext/wikitext-103-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Tue Dec  2 21:44:00 2025).
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
StreamingLLM æ¶ˆèå®éªŒ
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
æ•°æ®é›†: wikitext:wikitext-103-v1
æ¶ˆèç±»å‹: n_sink
è®¾å¤‡: cuda
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ•°æ®é›†...
æ•°æ®é›†å¤§å°: 3407 tokens
åŠ è½½æ¨¡å‹...

============================================================
æ¶ˆèå®éªŒ: N_sink (window_size=1024)
============================================================

æµ‹è¯• n_sink=0...
  PPL: 37.18, Runtime: 10.790s, Compression: 69.94%
æµ‹è¯• n_sink=1...
  PPL: 36.89, Runtime: 10.336s, Compression: 69.91%
æµ‹è¯• n_sink=2...
  PPL: 36.93, Runtime: 11.562s, Compression: 69.89%
æµ‹è¯• n_sink=4...
  PPL: 36.79, Runtime: 10.141s, Compression: 69.83%
æµ‹è¯• n_sink=8...
  PPL: 36.78, Runtime: 10.122s, Compression: 69.71%
æµ‹è¯• n_sink=16...
  PPL: 36.93, Runtime: 11.345s, Compression: 69.47%

============================================================
æ¶ˆèå®éªŒå®Œæˆ
============================================================
ç»“æœå·²ä¿å­˜åˆ°: results/streaming_llm/ablation_n_sink.json
============================================================

ç»“æœæ€»ç»“:
{
  "model": "EleutherAI/pythia-70m",
  "dataset": "wikitext:wikitext-103-v1",
  "ablation_type": "n_sink",
  "total_tokens": 3407,
  "results": [
    {
      "n_sink": 0,
      "window_size": 1024,
      "max_cache_size": 1024,
      "perplexity": 37.17771530151367,
      "runtime_sec": 10.790202272008173,
      "prefill_sec": 0.3077750849770382,
      "compression_ratio": 0.6994423246257705
    },
    {
      "n_sink": 1,
      "window_size": 1024,
      "max_cache_size": 1025,
      "perplexity": 36.886417388916016,
      "runtime_sec": 10.336448728106916,
      "prefill_sec": 0.0056673489743843675,
      "compression_ratio": 0.6991488112709128
    },
    {
      "n_sink": 2,
      "window_size": 1024,
      "max_cache_size": 1026,
      "perplexity": 36.9336051940918,
      "runtime_sec": 11.562154458020814,
      "prefill_sec": 0.004671892966143787,
      "compression_ratio": 0.6988552979160552
    },
    {
      "n_sink": 4,
      "window_size": 1024,
      "max_cache_size": 1028,
      "perplexity": 36.790061950683594,
      "runtime_sec": 10.14096226496622,
      "prefill_sec": 0.0046576609602198005,
      "compression_ratio": 0.6982682712063399
    },
    {
      "n_sink": 8,
      "window_size": 1024,
      "max_cache_size": 1032,
      "perplexity": 36.779815673828125,
      "runtime_sec": 10.121631952933967,
      "prefill_sec": 0.004635829012840986,
      "compression_ratio": 0.6970942177869093
    },
    {
      "n_sink": 16,
      "window_size": 1024,
      "max_cache_size": 1040,
      "perplexity": 36.9306526184082,
      "runtime_sec": 11.344532257993706,
      "prefill_sec": 0.004765007994137704,
      "compression_ratio": 0.6947461109480482
    }
  ]
}
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 70s)

[1;33m=== é˜¶æ®µ 5: ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 13: ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/generate_final_figures.py
å¼€å§‹æ—¶é—´: Thu Dec  4 09:05:22 PM CST 2025
============================================================
StreamingLLM å®éªŒç»“æœå¯è§†åŒ–
============================================================
âœ“ å›¾è¡¨ä¿å­˜ç›®å½•: results/figures

ç”Ÿæˆä¸»å®éªŒå¯¹æ¯”å›¾...
âœ“ å·²ä¿å­˜: results/figures/main_comparison.png

ç”ŸæˆWindow Sizeæ¶ˆèå›¾...
âœ“ å·²ä¿å­˜: results/figures/ablation_window_size.png

ç”ŸæˆN_sinkæ¶ˆèå›¾...
âœ“ å·²ä¿å­˜: results/figures/ablation_n_sink.png

ç”Ÿæˆç»¼åˆç»“æœè¡¨æ ¼...
âœ“ å·²ä¿å­˜: results/figures/results_summary.png

============================================================
âœ“ æ‰€æœ‰å›¾è¡¨ç”Ÿæˆå®Œæˆ!
âœ“ ä¿å­˜ä½ç½®: results/figures
============================================================

ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶:
  - ablation_n_sink.png (0.48 MB)
  - ablation_window_size.png (0.42 MB)
  - main_comparison.png (0.31 MB)
  - results_summary.png (0.24 MB)
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 4s)


################################################################################
# å®éªŒå®ŒæˆæŠ¥å‘Š
################################################################################

ç»“æŸæ—¶é—´: Thu Dec  4 09:05:26 PM CST 2025
æ€»è€—æ—¶: 309s (5åˆ†é’Ÿ)

å®éªŒç»Ÿè®¡:
  æ€»å®éªŒæ•°: 13
  [0;32mæˆåŠŸ: 13[0m
  [0;31må¤±è´¥: 0[0m

è¯¦ç»†ç»“æœ:
----------------------------------------
  [0;32mâœ“[0m WikiText-103 Baseline (23s)
  [0;32mâœ“[0m PG19 Baseline (29s)
  [0;32mâœ“[0m WikiText-103 StreamingLLM (16s)
  [0;32mâœ“[0m WikiText-103 MIT StreamingLLM (16s)
  [0;32mâœ“[0m PG19 StreamingLLM (21s)
  [0;32mâœ“[0m PG19 MIT StreamingLLM (20s)
  [0;32mâœ“[0m WikiText-103 kvpress StreamingLLM (decode-loop) (14s)
  [0;32mâœ“[0m WikiText-103 kvpress StreamingLLM (official eval) (7s)
  [0;32mâœ“[0m PG19 kvpress StreamingLLM (decode-loop) (17s)
  [0;32mâœ“[0m PG19 kvpress StreamingLLM (official eval) (6s)
  [0;32mâœ“[0m Window Size æ¶ˆèå®éªŒ (65s)
  [0;32mâœ“[0m N_sink æ¶ˆèå®éªŒ (70s)
  [0;32mâœ“[0m ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ (4s)
----------------------------------------

æ—¥å¿—æ–‡ä»¶: results/experiment_log_20251204_210017.txt
æ€»ç»“æ–‡ä»¶: results/experiment_summary_20251204_210017.txt

[0;32mâœ“ æ‰€æœ‰å®éªŒæˆåŠŸå®Œæˆ![0m
