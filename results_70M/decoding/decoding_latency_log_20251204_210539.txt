################################################################################
# Decoding Latency å®éªŒ
# å¼€å§‹æ—¶é—´: Thu Dec  4 09:05:39 PM CST 2025
################################################################################

[1;33m=== Decoding Latency å®éªŒ ===[0m
æµ‹é‡é…ç½® (é»˜è®¤æ®µ):
  - Prompt Length: 512
  - Num Tokens: 2000
  - Warmup Tokens: 200
  - Num Runs: 3


[0;34m========================================[0m
[0;34må®éªŒ 1: Decoding Latency Baseline (full cache)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 4096         --n-sink 4         --prompt-length 512         --num-tokens 2000         --warmup-tokens 200         --num-runs 3         --mode baseline         --output results/decoding/decoding_latency_baseline.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:05:39 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 4096
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
Mode: baseline
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° Baseline (Full KV Cache)
============================================================

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.983 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.979 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.992 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 2.985 Â± 0.100 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.971 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
åŸºçº¿è¯„ä¼°å®Œæˆ
============================================================
Baseline: 2.985 Â± 0.100 ms/token
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_baseline.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 26s)


[0;34m========================================[0m
[0;34må®éªŒ 2: Streaming Decoding Latency (Cache=512)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py             --cache-size 512             --n-sink 4             --prompt-length 512             --num-tokens 2000             --warmup-tokens 200             --num-runs 3             --mode streaming             --baseline-results results/decoding/decoding_latency_baseline.json             --output results/decoding/decoding_latency_512.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:06:05 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 512
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
Mode: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 512

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.550 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.030 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.021 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.200 Â± 1.191 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.036 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      2.985 Â± 0.100 ms/token
StreamingLLM:  3.200 Â± 1.191 ms/token
åŠ é€Ÿæ¯”:        0.93x
æ˜¾å­˜å‡å°‘:      -0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_512.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 35s)


[0;34m========================================[0m
[0;34må®éªŒ 3: Streaming Decoding Latency (Cache=1024)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py             --cache-size 1024             --n-sink 4             --prompt-length 512             --num-tokens 2000             --warmup-tokens 200             --num-runs 3             --mode streaming             --baseline-results results/decoding/decoding_latency_baseline.json             --output results/decoding/decoding_latency_1024.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:06:40 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 1024
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
Mode: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 1024

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.214 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.112 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.135 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.154 Â± 0.451 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.113 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      2.985 Â± 0.100 ms/token
StreamingLLM:  3.154 Â± 0.451 ms/token
åŠ é€Ÿæ¯”:        0.95x
æ˜¾å­˜å‡å°‘:      -0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_1024.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 33s)


[0;34m========================================[0m
[0;34må®éªŒ 4: Streaming Decoding Latency (Cache=2048)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py             --cache-size 2048             --n-sink 4             --prompt-length 512             --num-tokens 2000             --warmup-tokens 200             --num-runs 3             --mode streaming             --baseline-results results/decoding/decoding_latency_baseline.json             --output results/decoding/decoding_latency_2048.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:07:13 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 2048
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
Mode: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 2048

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.009 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.991 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.000 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.000 Â± 0.072 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.994 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      2.985 Â± 0.100 ms/token
StreamingLLM:  3.000 Â± 0.072 ms/token
åŠ é€Ÿæ¯”:        0.99x
æ˜¾å­˜å‡å°‘:      -0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_2048.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 28s)


[0;34m========================================[0m
[0;34må®éªŒ 5: Streaming Decoding Latency (Cache=4096)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py             --cache-size 4096             --n-sink 4             --prompt-length 512             --num-tokens 2000             --warmup-tokens 200             --num-runs 3             --mode streaming             --baseline-results results/decoding/decoding_latency_baseline.json             --output results/decoding/decoding_latency_4096.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:07:41 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 4096
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
Mode: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 4096

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.963 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.965 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.967 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 2.965 Â± 0.063 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.957 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      2.985 Â± 0.100 ms/token
StreamingLLM:  2.965 Â± 0.063 ms/token
åŠ é€Ÿæ¯”:        1.01x
æ˜¾å­˜å‡å°‘:      -0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_4096.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 27s)

[1;33m=== é•¿åºåˆ— Decoding Latency æµ‹è¯• ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 6: Long Sequence Baseline (5000 tokens)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 4096         --n-sink 4         --prompt-length 512         --num-tokens 5000         --warmup-tokens 200         --num-runs 3         --mode baseline         --output results/decoding/decoding_latency_baseline_long_5000.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:08:08 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 4096
Prompt Length: 512
Num Tokens: 5000
Warmup Tokens: 200
Num Runs: 3
Mode: baseline
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° Baseline (Full KV Cache)
============================================================

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.263 ms/token
  å³°å€¼æ˜¾å­˜: 215.3 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 2.991 ms/token
  å³°å€¼æ˜¾å­˜: 215.3 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.029 ms/token
  å³°å€¼æ˜¾å­˜: 215.3 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.094 Â± 0.952 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 2.989 ms/token
  å¹³å‡æ˜¾å­˜: 215.3 MB

============================================================
åŸºçº¿è¯„ä¼°å®Œæˆ
============================================================
Baseline: 3.094 Â± 0.952 ms/token
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_baseline_long_5000.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 55s)


[0;34m========================================[0m
[0;34må®éªŒ 7: Long Sequence Streaming (5000 tokens, Cache=1024)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 1024         --n-sink 4         --prompt-length 512         --num-tokens 5000         --warmup-tokens 200         --num-runs 3         --mode streaming         --baseline-results results/decoding/decoding_latency_baseline_long_5000.json         --output results/decoding/decoding_latency_long_5000.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:09:03 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 1024
Prompt Length: 512
Num Tokens: 5000
Warmup Tokens: 200
Num Runs: 3
Mode: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 4
  window_size: 1024

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.267 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.233 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 5000 tokens...
  æ”¶é›†åˆ° 5000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.099 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.199 Â± 1.124 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.062 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      3.094 Â± 0.952 ms/token
StreamingLLM:  3.199 Â± 1.124 ms/token
åŠ é€Ÿæ¯”:        0.97x
æ˜¾å­˜å‡å°‘:      6.3%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_long_5000.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 74s)

[1;33m=== ä¸åŒ n_sink é…ç½®æµ‹è¯• ===[0m

[0;34m========================================[0m
[0;34må®éªŒ 8: Streaming Decoding (n_sink=0, Cache=1024)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 1024         --n-sink 0         --prompt-length 512         --num-tokens 2000         --warmup-tokens 200         --num-runs 3         --mode streaming         --baseline-results results/decoding/decoding_latency_baseline.json         --output results/decoding/decoding_latency_nsink0.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:10:17 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 1024
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
Mode: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 0
  window_size: 1024

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.257 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.198 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.728 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.394 Â± 1.294 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.194 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      2.985 Â± 0.100 ms/token
StreamingLLM:  3.394 Â± 1.294 ms/token
åŠ é€Ÿæ¯”:        0.88x
æ˜¾å­˜å‡å°‘:      -0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_nsink0.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 35s)


[0;34m========================================[0m
[0;34må®éªŒ 9: Streaming Decoding (n_sink=8, Cache=1024)[0m
[0;34m========================================[0m
å‘½ä»¤: kvpress/.venv/bin/python experiments/eval_decoding_latency.py         --cache-size 1024         --n-sink 8         --prompt-length 512         --num-tokens 2000         --warmup-tokens 200         --num-runs 3         --mode streaming         --baseline-results results/decoding/decoding_latency_baseline.json         --output results/decoding/decoding_latency_nsink8.json
å¼€å§‹æ—¶é—´: Thu Dec  4 09:10:52 PM CST 2025
`torch_dtype` is deprecated! Use `dtype` instead!

============================================================
Per-Token Decoding Latency è¯„ä¼°
============================================================
æ¨¡å‹: EleutherAI/pythia-70m
è®¾å¤‡: cuda
æ•°æ®ç±»å‹: torch.float16
Cache Size: 1024
Prompt Length: 512
Num Tokens: 2000
Warmup Tokens: 200
Num Runs: 3
Mode: streaming
============================================================

åŠ è½½ tokenizer...
åŠ è½½æ¨¡å‹...

============================================================
è¯„ä¼° StreamingLLM (æˆ‘ä»¬çš„å®ç°)
============================================================
  n_sink: 8
  window_size: 1024

Run 1/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.000 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 2/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.018 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

Run 3/3
  Warmup: ç”Ÿæˆ 200 tokens...
  Warmup å®Œæˆ
  æµ‹é‡: ç”Ÿæˆ 2000 tokens...
  æ”¶é›†åˆ° 2000 ä¸ªå»¶è¿Ÿæµ‹é‡
  å¹³å‡å»¶è¿Ÿ: 3.455 ms/token
  å³°å€¼æ˜¾å­˜: 201.7 MB

æ€»ç»“ (3 runs):
  å¹³å‡å»¶è¿Ÿ: 3.157 Â± 1.164 ms/token
  ä¸­ä½æ•°å»¶è¿Ÿ: 3.003 ms/token
  å¹³å‡æ˜¾å­˜: 201.7 MB

============================================================
æœ€ç»ˆå¯¹æ¯”
============================================================
Baseline:      2.985 Â± 0.100 ms/token
StreamingLLM:  3.157 Â± 1.164 ms/token
åŠ é€Ÿæ¯”:        0.95x
æ˜¾å­˜å‡å°‘:      -0.0%
============================================================

ç»“æœå·²ä¿å­˜åˆ°: results/decoding/decoding_latency_nsink8.json
[0;32mâœ“ æˆåŠŸ[0m (è€—æ—¶: 32s)


################################################################################
# Decoding Latency å®éªŒå®ŒæˆæŠ¥å‘Š
################################################################################

ç»“æŸæ—¶é—´: Thu Dec  4 09:11:24 PM CST 2025
æ€»è€—æ—¶: 345s (5åˆ†é’Ÿ)

å®éªŒç»Ÿè®¡:
  æ€»å®éªŒæ•°: 9
  [0;32mæˆåŠŸ: 9[0m
  [0;31må¤±è´¥: 0[0m

è¯¦ç»†ç»“æœ:
----------------------------------------
  [0;32mâœ“[0m Decoding Latency Baseline (full cache) (26s)
  [0;32mâœ“[0m Streaming Decoding Latency (Cache=512) (35s)
  [0;32mâœ“[0m Streaming Decoding Latency (Cache=1024) (33s)
  [0;32mâœ“[0m Streaming Decoding Latency (Cache=2048) (28s)
  [0;32mâœ“[0m Streaming Decoding Latency (Cache=4096) (27s)
  [0;32mâœ“[0m Long Sequence Baseline (5000 tokens) (55s)
  [0;32mâœ“[0m Long Sequence Streaming (5000 tokens, Cache=1024) (74s)
  [0;32mâœ“[0m Streaming Decoding (n_sink=0, Cache=1024) (35s)
  [0;32mâœ“[0m Streaming Decoding (n_sink=8, Cache=1024) (32s)
----------------------------------------

ç»“æœæ–‡ä»¶:
-rw-rw-r-- 1 jflin jflin 300K Dec  4 21:07 results/decoding/decoding_latency_1024.json
-rw-rw-r-- 1 jflin jflin 300K Dec  4 21:07 results/decoding/decoding_latency_2048.json
-rw-rw-r-- 1 jflin jflin 300K Dec  4 21:08 results/decoding/decoding_latency_4096.json
-rw-rw-r-- 1 jflin jflin 300K Dec  4 21:06 results/decoding/decoding_latency_512.json
-rw-rw-r-- 1 jflin jflin 150K Dec  4 21:06 results/decoding/decoding_latency_baseline.json
-rw-rw-r-- 1 jflin jflin 375K Dec  4 21:09 results/decoding/decoding_latency_baseline_long_5000.json
-rw-rw-r-- 1 jflin jflin 749K Dec  4 21:10 results/decoding/decoding_latency_long_5000.json
-rw-rw-r-- 1 jflin jflin 300K Dec  4 21:10 results/decoding/decoding_latency_nsink0.json
-rw-rw-r-- 1 jflin jflin 300K Dec  4 21:11 results/decoding/decoding_latency_nsink8.json

æ—¥å¿—æ–‡ä»¶: results/decoding/decoding_latency_log_20251204_210539.txt
æ€»ç»“æ–‡ä»¶: results/decoding/decoding_latency_summary_20251204_210539.txt

[0;32mâœ“ æ‰€æœ‰ Decoding Latency å®éªŒæˆåŠŸå®Œæˆ![0m
